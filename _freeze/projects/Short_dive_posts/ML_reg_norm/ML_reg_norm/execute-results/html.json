{
  "hash": "739b8f5bef49f196a606594e7824eea3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A Brief Guide to Vector Norms in Machine Learning\"\nsubtitle: \"Understanding the basic application of norms in machine learning with Python examples\"\nauthor: \"Nigel Gebodh\"\ncategories: [ML, Deep Learning, Linear Algebra, Math]\ndate: \"2024-07-30\"\n# reference-location: margin\ntwitter-card:\n  card-style: summary_large_image\n  image: \"landscape.png\"\n  title: \"Norms in Machine Learning\"  # less than 55 chars\n  description: \"Exploring the hierarchy with different types of neuromodulation\"  # less than 125 chars\nopen-graph:   \n  image: \"landscape.png\"\n  title: \"Norms in Machine Learning\"  # less than 55 chars\n  description: \"Understanding the application of norms in machine learning\"  # less than 125 chars\nback-to-top-navigation: false\nformat: \n  html:\n    page-layout: full\nexecute:\n  freeze: true\n# jupyter: python3\n\ninclude-in-header: citation_metadata.html \nfilters:\n  - cite_as.lua\n---\n\n<style> \n      /* The . with the boxed represents that it is a class */\n      .boxed {\n        background: #e6fffe; /* C5F7F8 E5FAFA E1FCFC  #ecfffe #E5FAFA*/\n        color: #0b3357; /* gray navy*/\n        border: 3px solid #2980B9;\n        margin: 10% auto;\n        width: 95%;\n        height: 95%;\n        padding: 20px;\n        border-radius: 20px;\n      }\n    </style>\n\n\n\n\n<style> \n      /* The . with the boxed represents that it is a class */\n      .boxed_gray {\n        background: \t#f3f6f4; /* #eff8f8 C5F7F8 E5FAFA E1FCFC a6a6a6 #eeeeee #e3e3e3*/\n        color: #444444; /*gray */\n        border: 3px solid #a6a6a6;\n        margin: 10% auto;\n        width: 95%;\n        height: 95%;\n        padding: 10px;\n        border-radius: 20px;\n      }\n    </style>\n\n\n<style> \n      /* The . with the boxed represents that it is a class */\n      .boxed_equation {\n        background: #f0fff1; /* C5F7F8 E5FAFA E1FCFC a6a6a6  #94DEA5 #c1fba4 #c2f8cb #f0fff1*/\n        color: gray;\n        border: 3px solid #f0fff1; /* dark blue- #023D54 width: 70%; */\n        margin: 10% auto;\n        height: 95%;\n        padding: 10px;\n        border-radius: 20px;\n      }\n    </style>\n\n\n\n<img src=\"vector_norms.png\" alt=\"Image by DALL-E 3\" width=\"100%\" height=\"auto\" style=\"border: none;\"/>\n\n<figcaption>Image by DALL-E 3</figcaption>\n\n# What exactly is a norm?\n\nNorms are a fundamental concept in linear algebra and machine learning. Here we'll look at some common norms used in machine learning and how they're applied.\n\n::: boxed\n\n***Norms*** are a class of mathematical operations used to quantify or measure the length or size of vector or matrix or distance between vectors or matrices. </mark>\n\n-   Since norms measure lengths, distances, or magnitudes they are typically ‚â• 0.\n\n-   This is useful in machine learning in cases where you want to build models where you minimize errors/distances between predictions and ground truth.\n\n:::\n\n\n\n# How are norms applied in machine learning?\n\n::: boxed_gray\n\nIn machine learning norms are applied in:\n\n-   **Distance metrics**:\n    -   Since norms measure lengths, distances, magnitudes they can be applied to measure the length of a vector or distances between vectors ($d(u, v)$). They are used in various algorithms that rely on datapoint distance calculations like [k-Nearest Neighbor](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) or [k-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering).\n-   **Loss functions**:\n    -   Since norms help in calculating distances and are widely adapted in error or loss calculations like the Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n-   **Regularization**:\n    -   Techniques like LASSO and Ridge regularization/regression apply the $L_1$ and $L_2$ norms to prevent model overfitting by inducing sparsity and reducing model weights.\n-   **Feature selection**:\n    -   Norms like the $L_1$ can help with feature selection by zeroing out unimportant features or creating a sparse solution (some elements in the solution are zero). [Check out Dr. Steve Brunton‚Äôs explanation](https://www.youtube.com/watch?v=76B5cMEZA4Y).\n\n\n:::\n\n# Common Norms in Machine Learning\n\n-   ***L‚ÇÅ*** **Norm** (Manhattan Norm or Taxicab norm)\n\n-   ***L‚ÇÇ*** **Norm** (Euclidean Norm or Magnitude)\n\n-   **ùêø‚àû Norm** (Max Norm or Chebyshev Norm)\n\n-   ***L‚Çö*** **(*0****‚â§**p\\<1*****) ‚ÄúNorm‚Äù (**Not as common, but can show up)\n\n## Norm Notation\n\nNorms are typically denoted with double vertical lines around a transformation being applied (here a $\\cdot$ is used as a space holder) and the norm type as a subscript on the outside ($p$, where $p$ can be $p \\geq 0, 1, 2, 3, ...,\\infty$). If $p$ is not explicitly given, the norm is assumed to be $p=2$ since it is the most commonly used norm. Norms can also be denoted with either a capital letter $L$ or a lower case $\\ell$ with the letter ùëù as either a superscript ($L^p$) or subscript ($L_p$) .\\\n$$\nL_p = \\ell_p = \\Vert \\cdot \\Vert_p\n$$\n\nTypical transformations can be:\n\n-   Absolute value¬†‚Ää‚Äî‚Ää¬† $|\\mathbf{x}|$\n-   Squaring¬†‚Ää‚Äî‚Ää¬† $|\\mathbf{x}|^2$\n-   Min/Max operations¬†‚Ää‚Äî‚Ää¬† $min(|\\mathbf{x}|)$ or $max(|\\mathbf{x}|)$\n\n## $L_1$ Norm (Manhattan Norm)\n\nThe $L_1$ norm or $\\ell_1$, also known as Manhattan norm or Taxicab norm, is defined as the sum of the absolute values of the vector components. It's referred to as the Manhattan norm because distance is calculated in a gridwise format similar to the gridded blocks in Manhattan.\n\n::: boxed_equation\n$$\n\\textcolor{teal}{L_1 := ||\\mathbf{x}||_{p=1} := \\sum_{i=1}^n  |x_i|}\n$$\n:::\n\n$$\nor\n$$ $$\n\\begin{align*}\n||\\mathbf{x}||_{p=1} & :=  \\left[ \\sum_{i=1}^n  |x_i|^1 \\right]^\\frac{1}{1}\n\\end{align*}\n$$\n\n$$\nor\n$$\n\n$$\n||\\mathbf{x}||_{p=1} := |x_1|^1 + |x_2|^1 + |x_3|^1 + ... + |x_N|^1\n$$\n\n$$\nor\n$$\n\n$$\n||\\mathbf{x}||_{p=1} := |x_1| + |x_2| + |x_3| + ... + |x_N|\n$$\n\n$\\textcolor{darkorange}{Note}$: A number to the power of 1 is just that number returned (i.e. $3^1 = 3$, $x^1 = x$). Here the powers are added for demonstration and later generalization.\n\n\n<img src=\"L1_norm.gif\" alt=\"$L_1$ Norm visualized as the distance from 0.\" width=\"100%\" height=\"auto\" style=\"border: none;\"/>\n\n### $\\cdot$ Example: $L_1$ Norm\n\n<details>\n\n<summary>Example</summary>\n\n#### **1.** $L_1$ Example\n\nGiven a vector $\\mathbf{x} = [3,4]$, compute the $\\ell_\\textcolor{red}1$ norm.\n\n$$\n||\\mathbf{x}||_{1} := \\sum_{i=1}^n  |x_i| := \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}1 \\right]^\\frac{1}{\\textcolor{red}1}\n$$ $$\n\\begin{align*}\n||\\mathbf{x}||_{1} & = \\left[ \\sum_{i=1}^{n=2}  |x_i|^\\textcolor{red}1 \\right]^\\frac{1}{\\textcolor{red}1} \\\\\n& = (|3|^\\textcolor{red}1 + |4|^\\textcolor{red}1)^\\frac{1}{\\textcolor{red}1}\\\\\n& = ( 3 + 4)^\\frac{1}{\\textcolor{red}1}\\\\\n& = ( 7)^1 \\\\\n& = 7\n\\end{align*}\n$$\n\n$$\nor \n$$ $$\n||\\mathbf{x}||_{1} = |3| + |4| =  3 + 4 = 7\n$$\n\n::: {#792b4746 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.linalg import norm\n\nnorm_type = 1\nx = np.array([3,4])\n\nnorm_l1 = np.sum(np.abs(x))\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 7\n\nnorm_l1 = norm(x, 1)\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 7\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{1}-norm: 7$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{1}-norm: 7.0$\n:::\n:::\n\n\n#### **2. Distance Example**\n\nGiven two vectors compute their Manhattan distance.<br> $\\mathbf{u} = [1, 2, 3]$ $\\mathbf{v} = [4, 5, 6]$\n\nSteps:\n\n-   Compute the error\n-   Find the absolute value\n-   Sum all elements\n\n$$\n\\begin{align*}\n||\\mathbf{x}||_{1} & = |1-4| + |2-5| + |3-6| \\\\\n & = |-3| + |-3| + |-3| \\\\\n & = 3 + 3 + 3 \\\\\n & = 9\\\\\n\\end{align*}\n$$\n\n::: {#dee45880 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.linalg import norm\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nnorm_type = 1\nx = np.subtract(u,v)\n\nnorm_l1 = np.sum(np.abs(x))\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 9\n\nnorm_l1 = norm(x, 1)\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 9\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{1}-norm: 9$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{1}-norm: 9.0$\n:::\n:::\n\n\n</details>\n\n## $L_2$ Norm (Euclidean Norm)\n\nThe $L_2$ norm or $\\ell_2$, also known as the Euclidean norm is defined as the square root of the sum of squared vector components.\n\n::: boxed_equation\n$$\n\\textcolor{teal}{L_2 := ||\\mathbf{x}||_{p=2} := \\sqrt[2]{\\sum_{i=1}^n  |x_i|^2 }  }\n$$\n:::\n\n$$\nor\n$$\n\n$$\n||\\mathbf{x}||_{p=2} :=  \\left[ \\sum_{i=1}^n  |x_i|^2 \\right]^\\frac{1}{2}\n$$\n\n$$\nor\n$$\n\n$$\n||\\mathbf{x}||_{p=2} := (|x_1|^2 + |x_2|^2 + |x_3|^2 + ... + |x_N|^2)^\\frac{1}{2}\n$$\n\n$\\textcolor{darkorange}{Note}$: Values raised to the power of a fraction are equivalent to the $n^{th}$ root (i.e. $x^\\frac{1}{n} = \\sqrt[n]{x}$ or $25^\\frac{1}{2} = \\sqrt[2]{25} = 5$). Here the powers are added for demonstration and later generalization.\n\n\n<img src=\"L2_norm.gif\" alt=\"$L_2$ Norm visualized as the distance from 0.\" width=\"100%\" height=\"auto\" style=\"border: none;\"/>\n\n### $\\cdot$ Example: $L_2$ Norm\n\n<details>\n\n<summary>Example</summary>\n\n#### **1.** $L_2$ Example\n\nGiven a vector $\\mathbf{x} = [3,4]$, compute the $\\ell_\\textcolor{red}2$ norm\n\n$$\n||\\mathbf{x}||_{p=\\textcolor{red}2} = \\sqrt[\\textcolor{red}2]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}2 } : =  \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}2 \\right]^\\frac{1}{\\textcolor{red}2}\n$$ $$\n\\begin{align*}\n||\\mathbf{x}||_{\\textcolor{red}{2}} & = \\sqrt[\\textcolor{red}{2}]{ |3|^\\textcolor{red}{2} + |4|^\\textcolor{red}{2} }\\\\\n& = \\sqrt[\\textcolor{red}{2}]{ 9 + 16 }\\\\\n& = \\sqrt[\\textcolor{red}{2}]{25} \\\\\n& = 5\n\\end{align*}\n$$\n\n::: {#890c2b98 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.linalg import norm\n\nnorm_type = 2\nx = np.array([3,4])\n\nnorm_l2 = np.sqrt(np.sum(x**2))\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5\n\nnorm_l2 = norm(x, 2)\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5 \n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{2}-norm: 5.0$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{2}-norm: 5.0$\n:::\n:::\n\n\n#### **2. Distance Example**\n\nGiven two vectors compute their Euclidian distance.<br> $\\mathbf{u} = [1, 2, 3]$ $\\mathbf{v} = [4, 5, 6]$\n\nSteps:\n\n-   Compute the error\n-   Find the absolute value, squared\n-   Sum all elements\n-   Compute the square root\n\n$$\n\\begin{align*}\n||\\mathbf{x}||_{\\textcolor{red}{2}} & = (|1-4|^\\textcolor{red}{2} + |2-5|^\\textcolor{red}{2} + |3-6|^\\textcolor{red}{2})^{\\frac{1}{\\textcolor{red}{2}}} \\\\\n & = (|-3|^\\textcolor{red}{2} + |-3|^\\textcolor{red}{2} + |-3|^\\textcolor{red}{2})^{\\frac{1}{\\textcolor{red}{2}}} \\\\\n & = (9 + 9 + 9)^{\\frac{1}{\\textcolor{red}{2}}} \\\\\n & = (27)^{\\frac{1}{\\textcolor{red}{2}}}\\\\\n & = 5.196\\\\\n\\end{align*}\n$$\n\n::: {#e2e15e45 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.linalg import norm\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nnorm_type = 2\nx = np.subtract(u,v)\n\nnorm_l2 = np.sqrt(np.sum(x**2))\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5.196\n\nnorm_l2 = norm(x, 2)\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5.196\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{2}-norm: 5.196152422706632$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{2}-norm: 5.196152422706632$\n:::\n:::\n\n\n</details>\n\n## $L_\\infty$ Norm (Max Norm)\n\nThe $L_\\infty$ norm or $\\ell_\\infty$, also known as the Max norm is defined as the maximum value of a vector. It is sometimes referred to as [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance) or Chebyshev norm. It is useful when the largest deviation is of interest like in outlier or anomaly detection.\n\n::: boxed_equation\n$$\n\\textcolor{teal}{ \\\\\n||\\mathbf{x}||_{p=\\infty} := {max_{1 \\leq i \\leq N}} |x_i| \\\\\n}\n$$\n:::\n\n\n<img src=\"L_infinity_norm.gif\" alt=\"$L_\\infty$ Norm visualized as the distance from 0.\" width=\"100%\" height=\"auto\" style=\"border: none;\"/>\n\n### $\\cdot$Example: $L_\\infty$ Norm\n\n<details>\n\n<summary>Example</summary>\n\n#### **1.** $L_\\infty$ Example\n\nGiven a vector $\\mathbf{x} = [3,4]$, compute the $\\ell_\\infty$ norm\n\n$$\n||\\mathbf{x}||_{p=\\infty} = {max_{1 \\leq i \\leq N}} |x_i|\n$$ $$\n||\\mathbf{x}||_{\\infty} := max [|3|, |4|] := 4\n$$\n\n::: {#6e733b9d .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.linalg import norm\n\nnorm_type = '\\infty'\nx = np.array([3,4])\n\nnorm_linf = np.max(np.abs(x))\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 4\n\nnorm_linf = norm(x, np.inf)\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 4\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{\\infty}-norm: 4$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{\\infty}-norm: 4.0$\n:::\n:::\n\n\n#### **2. Distance Example**\n\nGiven two vectors compute their $L_\\infty$ distance.<br> $\\mathbf{u} = [1, 2, 3]$ $\\mathbf{v} = [4, 5, 6]$\n\nSteps:\n\n-   Compute the error\n-   Find the absolute value\n-   Find the maximum value of all elements\n\n$$\n\\begin{align*}\n||\\mathbf{x}||_{1} & = max [|1-4| , |2-5| , |3-6|] \\\\\n & = max [|-3| , |-3| , |-3|] \\\\\n & = max [3 , 3 , 3] \\\\\n & = 3\n\\end{align*}\n$$\n\n::: {#8bc4d260 .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.linalg import norm\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nnorm_type = '\\infty'\nx = np.subtract(u,v)\n\nnorm_linf = np.max(np.abs(x))\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 3\n\nnorm_linf = norm(x, np.inf)\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 3\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{\\infty}-norm: 3$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{\\infty}-norm: 3.0$\n:::\n:::\n\n\n</details>\n\n## $L_p$ Norm (Generalized Norm)\n\nNorms can be generalized to the $L_p$ or $\\ell_p$ form. This generalized form allows for the computation of any $p$ norms.\n\n::: boxed_equation\n$$\n\\textcolor{teal}{ \\\\\nL_p := ||\\mathbf{x}||_{p} := \\sqrt[p]{\\sum_{i=1}^n  |x_i|^p} := \\left[ \\sum_{i=1}^n  |x_i|^p   \\right]^\\frac{1}{p} \\\\\n}\n$$\n:::\n\n::: boxed_gray\nRules of norms:\n\n-   **Non-Negativity**: Norms are non-negative values. It makes sense since they are computed to get the length or size of a vector or matrix. $$\n    ||\\mathbf{u}||_{p} \\geq 0\n    $$\n\n-   **Definiteness**: Norms are only 0 if the vector being measured is a zero vector (it has a length of 0).\n\n$$\n||\\mathbf{u}||_{p} = 0   \\quad only \\quad if \\quad \\mathbf{u} = 0 \n$$\n\n-   **Triangle inequality**: The norm of the sum of two vectors $\\mathbf{u}$ and $\\mathbf{v}$ (\\|\\|ùêÆ+ùêØ\\|\\|) is not more than the sum of their norms (\\|\\|ùêÆ\\|\\| + \\|\\|ùêØ\\|\\|). This is similar to the *Pythagorean Theorem* where $||\\mathbf{u}||$ and $||\\mathbf{v}||$ are the sides of a triangle and $||\\mathbf{u} + \\mathbf{v} ||$ is the hypotenuse of the triangle. It is sometimes referred to as [Minkowski‚Äôs Inequality](https://en.wikipedia.org/wiki/Triangle_inequality). Mathematically this is: $$\n    ||\\mathbf{u} + \\mathbf{v} ||_p \\leq ||\\mathbf{u} ||_p + ||\\mathbf{v}||_p\n    $$\n\n-   **Homogeneity**: If you multiply a vector $\\mathbf{u}$ by a scalar $k$ and compute the norm its equivalent to multiplying the norm of the vector $\\mathbf{u}$ by the absolute value of that scalar $k$:\n\n$$\n||k\\mathbf{u} || = |k| ||\\mathbf{u} || \n$$\n\n\n:::\n\n### $\\cdot$ Example: $L_p$ Norms\n\n<details>\n\n<summary>Example</summary>\n\n#### **1.** $L_3$ Norm\n\nGiven a vector $\\mathbf{x} = [3,4]$, compute the $\\ell_\\textcolor{red}3$ norm $$\n||\\mathbf{x}||_{p=\\textcolor{red}3} = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}3 \\right]^\\frac{1}{\\textcolor{red}3}  :=  \\sqrt[\\textcolor{red}3]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}3}\n$$ $$\n\\begin{align*}\n||\\mathbf{x}||_{3} & = \\sqrt[3]{ |3|^3 + |4|^3 } \\\\\n& = \\sqrt[3]{ 27 + 64 }\\\\ \n& = \\sqrt[3]{91}\\\\ \n& = {91}^\\frac{1}{3}\\\\ \n& = 4.498\n\\end{align*}\n$$\n\n::: {#1d405890 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([3,4])\n\np=3\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{3}-norm: 4.497941445275415$\n:::\n:::\n\n\n#### **2.** $L_4$ Norm\n\nGiven a vector $\\mathbf{x} = [3,4]$, compute the $\\ell_\\textcolor{red}4$ norm $$\n||\\mathbf{x}||_{p=\\textcolor{red}4} = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}4 \\right]^\\frac{1}{\\textcolor{red}4}  :=  \\sqrt[\\textcolor{red}4]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}4}\n$$ $$\n\\begin{align*}\n||\\mathbf{x}||_{4} & = \\sqrt[4]{ |3|^4 + |4|^4 }\\\\\n& = \\sqrt[4]{ 81 + 256 }\\\\\n& = \\sqrt[4]{337}\\\\\n& = {337}^\\frac{1}{4}\\\\\n& =  4.285\n\\end{align*}\n$$\n\n::: {#2b862abd .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([3,4])\n\np=4\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{4}-norm: 4.284572294953817$\n:::\n:::\n\n\n</details>\n\n## Other $L_p$ \"Norms\"\n\n**What happens to norms when *0 ‚â§ p\\<1*?** \\\nWell, we get a calculated length back, however these cases are not strictly norms (hence ‚Äúnorms‚Äù) since we get a violation of the triangle inequality (*0\\<p\\<1*) or homogeneity (*p=0*).\n\n### $L_p$ for $0<p<1$ Norm\n\nWhat happens when $p$ is between $0$ and $1$ ($0<p<1$)?\n\nIn this case ($0<p<1$) the triangle inequality is violated. The means that, strictly speaking, $L_{0<p<1}$ is not actually a norm but it still returns a measurement of size of a vector. The same generalized formula can be used:\n\n::: boxed_equation\n$$\n\\textcolor{teal}{ \\\\\nL_{0 \\lt p \\lt 1} := ||\\mathbf{x}||_{0 \\lt p \\lt 1} := \\sqrt[p]{\\sum_{i=1}^n  |x_i|^p} := \\left[ \\sum_{i=1}^n  |x_i|^p   \\right]^\\frac{1}{p} \\\\\n}\n$$\n:::\n\n<details>\n\n<summary>Example</summary>\n\n#### **1.** $L_{0.5}$ Example\n\nGiven a vector $\\mathbf{x} = [3,4]$, compute the $\\ell_{\\textcolor{red}{0.5}}$ norm $$\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} = \\left[ \\sum_{i=1}^n  |x_i|^{\\textcolor{red}{0.5}}\\right]^\\frac{1}{\\textcolor{red}{0.5}}  :=  \\sqrt[\\textcolor{red}{0.5}]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}{0.5}}\n$$\n\n$$\nSimplifying:\n$$\n\n$$\n\\begin{align*}\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & = \\left[ \\sum_{i=1}^n  |x_i|^{\\textcolor{red}{0.5}}\\right]^\\frac{1}{\\textcolor{red}{0.5}}  \\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}{\\frac{1}{2}}\\right]^\\frac{1}{\\textcolor{red}{\\frac{1}{2}}}\\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}{\\frac{1}{2}}\\right]^{\\textcolor{red}{2}}\\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & =\n        (|x_1|^{\\textcolor{red}{\\frac{1}{2}}} +\n         |x_2|^{\\textcolor{red}{\\frac{1}{2}}} +\n         |x_3|^{\\textcolor{red}{\\frac{1}{2}}} + ... +\n         |x_N|^{\\textcolor{red}{\\frac{1}{2}}}\n         )^{\\textcolor{red}{2}}\\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & =  (\\sqrt[\\textcolor{red}{2}]{|x_1|} + \\sqrt[\\textcolor{red}{2}]{|x_2|} + \\sqrt[\\textcolor{red}{2}]{|x_3|} + ... + \\sqrt[\\textcolor{red}{2}]{|x_N|})^{\\textcolor{red}{2}}\\\\\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n||\\mathbf{x}||_{0.5} & = ( \\sqrt[2]{|3|} + \\sqrt[2]{|4|} )^2 \\\\\n                     & = (1.73 + 2)^2  \\\\\n                     & = (3.73)^2 \\\\\n                     & =  13.93 \\\\\n\\end{align*}\n$$\n\n::: {#6ad4d833 .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([3,4])\n\np=0.5\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{0.5}-norm: 13.928203230275509$\n:::\n:::\n\n\n#### **2. Distance Example**\n\nGiven two vectors compute their $L_{0.5}$ distance.<br> $\\mathbf{u} = [1, 2, 3]$ $\\mathbf{v} = [4, 5, 6]$\n\nSteps:\n\n-   Compute the error\n-   Find the absolute value\n-   Compute the square root of all elements\n-   Sum all elements and square the result\n\n$$\n\\begin{align*}\n||\\mathbf{x}||_{0.5} & = (\\sqrt[2]{|1-4|} + \\sqrt[2]{|2-5|} + \\sqrt[2]{|3-6|})^2 \\\\\n & = (\\sqrt[2]{|-3|} + \\sqrt[2]{|-3|} + \\sqrt[2]{|-3|})^2\\\\\n & = (\\sqrt[2]{3} + \\sqrt[2]{3} + \\sqrt[2]{3})^2 \\\\\n & = (1.732 + 1.732 + 1.732)^2 \\\\\n & = (5.196)^2 \\\\\n & = 27\\\\\n\\end{align*}\n$$\n\n::: {#0fea74a0 .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\n\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nx = np.subtract(u,v)\n\np=0.5\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{0.5}-norm: 27.0$\n:::\n:::\n\n\n#### **3.** $L_{0.5}$ Violates Triangle Inequality Example\n\nGiven two vectors show that $L_{0.5}$ norm violates the triangle inequality. <br> $\\mathbf{u} = [1, 2, 3]$ $\\mathbf{v} = [4, 5, 6]$\n\nRemember that the triangle inequality states: \\* The $p$ norm of the sum of two vectors is less than or equal to the sum of the $p$ of each vector\n\n$$\n||\\mathbf{u} + \\mathbf{v} ||_p \\leq ||\\mathbf{u} ||_p + ||\\mathbf{v}||_p\n$$\n\nSteps:\n\n-   Compute the sum of $\\mathbf{u}$ and $\\mathbf{v}$\n-   Find the $p$ norm of the sum\n-   Compute the $p$ norm of $\\mathbf{u}$ then $\\mathbf{v}$\n-   Sum the individual norms\n-   Compare both sides of the inequality\n\n$$\n\\begin{align*}\n        ||\\mathbf{u} + \\mathbf{v} ||_p     & \\leq ||\\mathbf{u} ||_p + ||\\mathbf{v}||_p\\\\\n        ||\\mathbf{u} + \\mathbf{v} ||_{0.5} & \\leq ||\\mathbf{u} ||_{0.5} + ||\\mathbf{v}||_{0.5}\\\\\n\\quad\\\\        \n        (\\sqrt[2]{|1+4|} + \\sqrt[2]{|2+5|} + \\sqrt[2]{|3+6|})^2 & \\leq \n             (\\sqrt[2]{|1|} + \\sqrt[2]{|2|} + \\sqrt[2]{|3|})^2 +\n             (\\sqrt[2]{|4|} + \\sqrt[2]{|5|} + \\sqrt[2]{|6|})^2\\\\\n\\quad\\\\\n        (\\sqrt[2]{|5|} + \\sqrt[2]{|7|} + \\sqrt[2]{|9|})^2 & \\leq \n            (\\sqrt[2]{|1|} + \\sqrt[2]{|2|} + \\sqrt[2]{|3|})^2 +\n            (\\sqrt[2]{|4|} + \\sqrt[2]{|5|} + \\sqrt[2]{|6|})^2\\\\     \n\\quad\\\\\n        (2.236 + 2.646 + 3)^2 & \\leq \n            (1 + 1.414 + 1.732)^2 +\n            (2 + 2.236 + 2.449)^2\\\\ \n\\quad\\\\\n        ( 7.882)^2 & \\leq \n            (4.146)^2 +\n            (6.686)^2\\\\ \n\\quad\\\\\n        62.123 & \\leq \n            17.192 +\n            44.697\\\\ \n\\quad\\\\\n        62.123 & \\leq 61.888  (\\textcolor{red}{Violation!})   \\\\     \n\\end{align*}\n$$\n\n::: {#baff1017 .cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\n\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nx = np.add(u,v)\n\np=0.5\n\nnorm_p_uv = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in='||u+v||_{0.5}', val=norm_p_uv )\n\n\nx = u\np=0.5\nnorm_p_u = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in='||u||_{0.5}', val=norm_p_u  )\n\nx = v\np=0.5\nnorm_p_v = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in='||v||_{0.5}', val=norm_p_v )\n\nnorm_p_v_u = norm_p_u + norm_p_v\nprint_result(type_in='||u||_{0.5} + ||v||_{0.5}', val=norm_p_v_u)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{||u+v||_{0.5}}-norm: 62.123075297585515$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{||u||_{0.5}}-norm: 17.191508225450303$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{||v||_{0.5}}-norm: 44.69668203123519$\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{||u||_{0.5} + ||v||_{0.5}}-norm: 61.88819025668549$\n:::\n:::\n\n\n</details>\n\n### $L_0$ \"Norm\"\n\nThe $L_0$ \"norm\" is not actually a norm but sometimes mentioned in the context of norms. It violates the homogeneity rule above. When applied, the $L_0$ \"norm\" counts the number of non-zero elements in a vector. This operation can be used to apply regularization, signal compression etc.\n\n::: boxed_equation\n$$\n\\textcolor{teal}{ \\\\\nL_0 := ||\\mathbf{x}||_{p=0} := Count(x_i \\neq 0)  \n}\n$$\n:::\n\n$$\nor\n$$\n\n$$\n||\\mathbf{x}||_{p=0} :=  \\sum_{i=1}^n  I(x_i \\neq 0)    \\\\\n$$\n\n$$\nWhere: \\\\\n\\quad \\\\\nI(\\cdot)=\n\\left\\{ \n  \\begin{array}{ c l }\n    1 & \\quad \\textrm{if } x_i \\neq 0 \\\\\n    0                 & \\quad \\textrm{otherwise}\n  \\end{array}\n\\right.\n$$\n\n<details>\n\n<summary>Example</summary>\n\nGiven a vector $\\mathbf{x} = [3,4,0]$ compute the $\\ell_0$ \"norm\"\n\n$$\n\\begin{align*}\n||\\mathbf{x}||_{p=0} & := Count(x_i \\neq 0) \\\\ \n||\\mathbf{x}||_{p=0} & :=  \\sum_{i=1}^n  I(x_i \\neq 0)  \\\\\n\\quad\n||\\mathbf{x}||_{p=0} & =  I(3 \\neq 0) +I(4 \\neq 0) + I(0 \\neq 0)  \\\\\n||\\mathbf{x}||_{p=0} & =  1 + 1 + 0  = 2\\\\\n\\end{align*}\n$$\n\nThis means there are $2$ non-zero elements in the vector $\\mathbf{x}$\n\n::: {#60846966 .cell execution_count=13}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([3,4,0])\nl0_norm = np.sum(x != 0)\nprint_result(type_in=0, val=l0_norm)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle L_{0}-norm: 2$\n:::\n:::\n\n\n</details>\n\n# Norms and Loss Functions\n\n**Loss functions** are functions developed to help **minimize the error between actual and predicted outcomes**. Both the commonly used $L_1$ and $L_2$ norms are connected to some widely used loss functions or cost functions. Namely the computation of the Mean Absolute Error (MAE) and Mean Squared Error (MSE).\n\nWhen norms are adapted as loss functions, one aspect that is important is that they are **convex**. This aids in convex optimization techniques like gradient descent. It allows these functions to converge to a global minimum. This means that:\n\n-   *Typically*, norms adapted for loss functions are $L_{p \\geq 1}$ and are convex for all $p \\geq 1$.\n-   Norms where $0 \\lt p \\lt 1$ can be non-convex, making it unsuitable for convex optimization.\n\n<details>\n\n<summary>Comparing Convexity between $L_{0.5}$, $L_1$, and $L_2$</summary>\n\nTo compare the convexity between transformations within the $L_{0.5}$, $L_1$, and $L_2$ norms, we can compute the derivatives of the functions used within each norm.\n\nIt's important to note that all norms for $p \\geq 1$ are convex.\n\n| Norm        | Function          | First Derivative                                            | Second Derivative                        | Convex/Non-Convex |\n|-------------|-------------------|-------------------------------------------------------------|------------------------------------------|-------------------|\n| $L_{p=1}$   | $|x|$             | $1$ for $x \\gt 0$, $-1$ for $x \\lt 0$, undefined at $x = 0$ | $0$ for $x \\neq 0$, undefined at $x = 0$ | Convex            |\n| $L_{p=2}$   | $x^2$             | $2x$                                                        | $2$                                      | Convex            |\n| $L_{p=0.5}$ | $x^{\\frac{1}{2}}$ | $\\frac{1}{2}x^{\\frac{-1}{2}}$                               | $\\frac{-1}{4}x^{\\frac{-3}{2}}$           | Non-Convex        |\n\nWhat we see is:\n\n-   The derivative of $|x|$ is not defined at $x = 0$, and its second derivative is $0$ for all $x \\neq 0$. Regardless of this the function satisfies the triangle inequality and is considered **convex**.\n-   The second derivative of $x^2$ is always $2$, which is non-negative, so $x^2$ is **convex**.\n-   The second derivative of $x^{\\frac{1}{2}}$ is negative for all $x \\geq 0$, so $x^{\\frac{1}{2}}$ is **non-convex**.\n\n</details>\n\n### $L_1$ and $MAE$ Loss Function\n\nIn terms of loss functions, the Mean Absolute Error (MAE) is the averaged form of the $L_1$ norm. Basically, to get from the $L_1$($||\\mathbf{x}||_{p=1}$) norm to the $MAE$, you sum all the absolute values of the entries ($\\textcolor{purple}n$) of a vector then divide by the number of entries ($\\textcolor{purple}{\\frac{1}{n}}$). In this case the vector $\\mathbf{x}$ is assumed to contain errors (i.e. the difference between $y_{true}$ and $y_{predicted}$ := $\\hat{y}$ := y-hat)\n\n::: boxed_equation\n$$\n\\textcolor{teal}{ \nMAE := \\frac{1}{n} \\sum_{i=1}^n {|y - \\hat{y}|}\n}\n$$\n:::\n\n<details>\n\n<summary>Comparing $L_1$ and MAE</summary>\n\n$$\nx_i :=  y_{i, true} - \\hat{y}_{i, predicted} := Error(y_{i, true}, \\hat{y}_{i, predicted})\n$$\n\n$$\n\\begin{align*}\nL_1 & :=  \\textcolor{steelblue}{\\sum_{i=1}^n  |x_i| } \\\\\nL_1 & :=  \\textcolor{steelblue}{\\left[ \\sum_{i=1}^n  |x_i|^1 \\right]^\\frac{1}{1}} \\\\\nL_1 & :=  \\textcolor{steelblue}{|x_1| + |x_2| + |x_3| + ... + |x_N|} \\\\\n\\quad\\\\\n\\quad\\\\\nL_1 & := \\textcolor{steelblue}{||\\mathbf{x}||_{1}} \\\\\nMAE & := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{||\\mathbf{x}||_{1}^{\\textcolor{purple}{1}}} \\\\\n\\quad\\\\\n\\quad\\\\\nMAE & := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{\\sum_{i=1}^n  |x_i| } \\\\\nMAE & := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{\\left[ \\sum_{i=1}^n  |x_i|^1 \\right]^\\frac{1}{1}} \\\\\nMAE & := \\textcolor{purple}{\\frac{1}{n}}(\\textcolor{steelblue}{|x_1| + |x_2| + |x_3| + ... + |x_N|})\\\\\n\\end{align*}\n$$\n\n</details>\n\n### $\\cdot$ Example: $MAE$\n\n<details>\n\n<summary>Example</summary>\n\nGiven a vector $\\mathbf{x} = [3,4]$ of errors compute the **Mean Absolute Error (MAE)**. What we know:\n\n-   $\\textcolor{purple}{n=2}$ $\\therefore$ $\\textcolor{purple}{\\frac{1}{n}} = \\textcolor{purple}{\\frac{1}{2}}$\n-   $\\textcolor{red}{p=1}$\n\n$$\nMAE := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{\\sum_{i=1}^n  |x_i|} := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{\\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}1 \\right]^\\frac{1}{\\textcolor{red}1}}\n$$\n\n$$\nMAE = \\textcolor{purple}{\\frac{1}{2}}\\left[ \\sum_{i=1}^{n=\\textcolor{purple}2}  |x_i|^\\textcolor{red}1 \\right]^\\frac{1}{\\textcolor{red}1} =  \\textcolor{purple}{\\frac{1}{2}}(|3|^\\textcolor{red}1 + |4|^\\textcolor{red}1)^\\frac{1}{\\textcolor{red}1} =  \\textcolor{purple}{\\frac{1}{2}}( 3 + 4)^\\frac{1}{\\textcolor{red}1} =  \\textcolor{purple}{\\frac{1}{2}}( 7)^1 = 3.5\n$$\n\n$$\nor \n$$ $$\nMAE = \\textcolor{purple}{\\frac{1}{2}}(\\textcolor{steelblue}{|3| + |4|}) = \\textcolor{purple}{\\frac{1}{2}}(\\textcolor{steelblue}{ 3 + 4}) = \\textcolor{purple}{\\frac{1}{2}}( \\textcolor{steelblue}{7}) = 3.5\n$$\n\n::: {#8e18f030 .cell execution_count=14}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([3,4])\nMAE = (1/len(x))*np.sum(np.abs(x))\nprint_result(type_in=\"MAE\", val=MAE)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle MAE: 3.5$\n:::\n:::\n\n\n</details>\n\n### $L_2$ and $MSE$ Loss Function\n\nIn terms of loss functions, the Mean Squared Error (MSE) is the averaged form of the $L_2$ norm, squared ($\\frac{1}{n}||\\mathbf{x}||_{p=2}^{2}$). Basically, to get from the $L_2$($||\\mathbf{x}||_{p=2}$) norm to the $MSE$, you square all the absolute values of the entries ($\\textcolor{purple}n$) of a vector then sum them all up then divide by the number of entries ($\\textcolor{purple}{\\frac{1}{n}}$). In this case the vector $\\mathbf{x}$ is assumed to contain errors (i.e. the difference between $y_{true}$ and $y_{predicted}$ := $\\hat{y}$ := y-hat)\n\n::: boxed_equation\n$$\n\\textcolor{teal}{ \nMSE := \\frac{1}{n} \\sum_{i=1}^n {(y - \\hat{y})}^2\n}\n$$\n:::\n\n<details>\n\n<summary>Comparing $L_2$ and MSE</summary>\n\n$$\nx_i :=  y_{i, true} - \\hat{y}_{i, predicted} := Error(y_{i, true}, \\hat{y}_{i, predicted})\n$$\n\n$$\n\\begin{align*}\nL_2 & :=  \\textcolor{steelblue}{\\sqrt[2]{\\sum_{i=1}^n  |x_i|^2 } } \\\\\nL_2 & :=  \\textcolor{steelblue}{\\left[ \\sum_{i=1}^n  |x_i|^2 \\right]^\\frac{1}{2}} \\\\\nL_2 & :=  \\textcolor{steelblue}{|x_1|^2 + |x_2|^2 + |x_3|^2 + ... + |x_N|^2} \\\\\n\\quad\\\\\n\\quad\\\\\nL_2 & :=  \\textcolor{steelblue}{||\\mathbf{x}||_{2}} \\\\\nMSE & :=  \\textcolor{purple}{\\frac{1}{n}} \\textcolor{steelblue}{||\\mathbf{x}||_{2}^\\textcolor{purple}{2}} \\\\\n\\quad\\\\\n\\quad\\\\\nMSE & := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{\\sqrt[2]{\\sum_{i=1}^n  |x_i|^2 } }^\\textcolor{purple}{2} \\\\\nMSE & := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{\\left[ \\sum_{i=1}^n  |x_i|^2 \\right]^{\\frac{1}{2} \\textcolor{purple}{2}}}\\\\\nMSE & := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{ \\sum_{i=1}^n  x_i^2} \\\\\nMSE & := \\textcolor{purple}{\\frac{1}{n}}(\\textcolor{steelblue}{x_1^2 + x_2^2 + x_3^2 + ... + x_N^2})^{\\textcolor{steelblue}{\\frac{1}{2}}\\textcolor{purple}{2}} \\\\\nMSE & := \\textcolor{purple}{\\frac{1}{n}}(\\textcolor{steelblue}{x_1^2 + x_2^2 + x_3^2 + ... + x_N^2})\\\\\n\\end{align*}\n$$\n\n$\\textcolor{darkorange}{Note}$: The powers of $[...]^\\frac{1}{2}$ and $[...]^2$ cancel each other out ($[...]^{\\frac{1}{2} * 2}$). Values raised to the power of a fraction are equivalent to the $n^{th}$ root (i.e. $x^\\frac{1}{n} = \\sqrt[n]{x}$ or $25^\\frac{1}{2} = \\sqrt[2]{25} = 5$). Here the powers are added for demonstration and later generalization.\n\n</details>\n\n### $\\cdot$ Example: $MSE$\n\n<details>\n\n<summary>Example</summary>\n\nGiven a vector $\\mathbf{x} = [3,4]$ of errors compute the **Mean Squared Error (MSE)**. What we know:\n\n-   $\\textcolor{purple}{n=2}$ $\\therefore$ $\\textcolor{purple}{\\frac{1}{n}} = \\textcolor{purple}{\\frac{1}{2}}$\n-   $\\textcolor{red}{p=2}$\n\n$$\nMSE :=  \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{ \\sum_{i=1}^n  x_i^2} := \\textcolor{purple}{\\frac{1}{n}}\\textcolor{steelblue}{\\left[ \\sum_{i=1}^n  x_i^\\textcolor{red}2 \\right]^{\\frac{1}{\\textcolor{red}2}  \\textcolor{purple}{2}}}\n$$\n\n$$\nMSE = \\textcolor{purple}{\\frac{1}{2}}\\left[ \\sum_{i=1}^{n=\\textcolor{purple}2}  x_i^\\textcolor{red}2 \\right]^{\\frac{1}{\\textcolor{red}2} \\textcolor{purple}{2}} =  \\textcolor{purple}{\\frac{1}{2}}(3^\\textcolor{red}2 + 4^\\textcolor{red}2)^{\\frac{1}{\\textcolor{red}2} \\textcolor{purple}{2}} =  \\textcolor{purple}{\\frac{1}{2}}( 9 + 16 )^{\\frac{1}{\\textcolor{red}2} \\textcolor{purple}{2}} =  \\textcolor{purple}{\\frac{1}{2}}(25)^1 = 12.5\n$$\n\n$$\nor \n$$ $$\nMSE = \\textcolor{purple}{\\frac{1}{2}}(\\textcolor{steelblue}{3^2 + 4^2}) = \\textcolor{purple}{\\frac{1}{2}}(\\textcolor{steelblue}{ 9 + 16}) = \\textcolor{purple}{\\frac{1}{2}}( \\textcolor{steelblue}{25}) = 12.5\n$$\n\n::: {#28119bcd .cell execution_count=15}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([3,4])\np=2\n\nMSE = (1/len(x))*np.sum(np.abs(x)**p)\nprint_result(type_in=\"MSE\", val=MSE)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n$\\displaystyle MSE: 12.5$\n:::\n:::\n\n\n</details>\n\n# Norms and Regularization\n\nIn machine learning and statistics norms can be applied to penalize weights within models to prevent them from overfitting and to better generalize. These penalizations utilize norms including the $L_1$ and $L_2$ norms. Three of these commonly used regularization techniques are:\n\n-   $L_1$ Regularization or [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)#:~:text=In%20statistics%20and%20machine%20learning,of%20the%20resulting%20statistical%20model.) regression\n-   $L_2$ Regularization or [Ridge](https://en.wikipedia.org/wiki/Ridge_regression) regression\n-   $L_1$ + $L_2$ Regularization or [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n\nThe penalization works essentially by appending the penalization term (of the norm) to the end of the loss function.\n\n$$\nModified \\quad Loss := \\textcolor{steelblue}{Loss\\quad Function} + \\textcolor{red}{Regularization\\quad Term}\n$$\n\n<details>\n\n<summary>Let's dig deeper!</summary>\n\nIf we assume our loss function is the $\\textcolor{steelblue}{MSE}$ and apply a parameter ($\\textcolor{purple}{\\lambda}$) to tune the degree of regularization we want on our model's parameters/weights ($w$), we get the following:\n\n-   $L_1$ Regularization or LASSO regression: <br>$L_1$ regularization forces weights within the model toward $0$ and in some cases forces weights to be equal to $0$. This can eliminate some features since their weights approach $0$, creating weight based feature selection. $$\n      \\begin{align*}\n      Loss & := \\textcolor{steelblue}{MSE} + \\textcolor{purple}\\lambda \\textcolor{red}{||\\mathbf{w}||_{1}}\\\\\n      Loss & := \\textcolor{steelblue}{MSE} + \\textcolor{purple}\\lambda \\textcolor{red}{\\sum_{j=1}^m  |w_j|}\\\\\n      Loss & := \\textcolor{steelblue}{\\frac{1}{n} \\sum_{i=1}^n {(y - \\hat{y})}^2} +\n                \\textcolor{purple}\\lambda \n                \\textcolor{red}{\\sum_{j=1}^m  |w_j|}\\\\\n      \\end{align*}\n      $$\n\n-   $L_2$ Regularization or Ridge regression: <br> $L_2$ regularization similarly reduces the weights toward $0$. In doing so the weights become scaled down and more evenly distributed. Note that we apply the squared $L_2$ norm or $||x||_{2}$ ¬†‚ü∂ $||x||_{2}^{2}$ eliminating the square root in the $L_2$ norm. $$\n      \\begin{align*}\n      Loss & := \\textcolor{steelblue}{MSE} + \\textcolor{purple}\\lambda \\textcolor{red}{||\\mathbf{w}||_{2}^{2}}\\\\\n      Loss & := \\textcolor{steelblue}{MSE} + \\textcolor{purple}\\lambda \\textcolor{red}{\\sum_{j=1}^m  |w_j|^2}\\\\\n      Loss & := \\textcolor{steelblue}{\\frac{1}{n} \\sum_{i=1}^n {(y - \\hat{y})}^2} +\n                \\textcolor{purple}\\lambda \n                \\textcolor{red}{\\sum_{j=1}^m  |w_j|^2}\\\\\n      \\end{align*}\n      $$\n\n-   $L_1$ + $L_2$ Regularization or Elastic Net: <br> Here we add the term $\\textcolor{PineGreen}{\\alpha}$ and $\\textcolor{PineGreen}{(1 -\\alpha)}$ to weight the application of either $L_1$ or $L_2$ penalization. The $\\textcolor{PineGreen}{\\alpha}$ value is typically $0 \\lt \\textcolor{PineGreen}{\\alpha} \\lt 1$. $$\n      \\begin{align*}\n      Loss & := \\textcolor{steelblue}{MSE} + \n          \\textcolor{purple}\\lambda \n          ( \n          \\textcolor{PineGreen}{\\alpha}     \\textcolor{red}{||\\mathbf{w}||_{1}} + \n          \\textcolor{PineGreen}{(1- \\alpha)}\\textcolor{red}{||\\mathbf{w}||_{2}^{2}}\n          )\\\\\n      \\quad\n      Loss & := \\textcolor{steelblue}{MSE} + \n          \\textcolor{purple}\\lambda \n          ( \n          \\textcolor{PineGreen}{\\alpha}     \\textcolor{red}{\\sum_{j=1}^m  |w_j|} + \n          \\textcolor{PineGreen}{(1- \\alpha)}\\textcolor{red}{\\sum_{j=1}^m  |w_j|^2}\n          )\\\\\n      \\quad\n      Loss & := \\textcolor{steelblue}{\\frac{1}{n} \\sum_{i=1}^n {(y - \\hat{y})}^2} +\n          \\textcolor{purple}\\lambda \n          ( \n          \\textcolor{PineGreen}{\\alpha}     \\textcolor{red}{\\sum_{j=1}^m  |w_j|} + \n          \\textcolor{PineGreen}{(1- \\alpha)}\\textcolor{red}{\\sum_{j=1}^m  |w_j|^2}\n          )\\\\\n      \\end{align*}\n      $$\n\n</details>\n\n# Take Away\n\nNorms play an important role in machine learning from distance calculations to regularization and error calculation. Understanding their properties and applications is essential for developing effective machine learning models and algorithms.\n\n---\n\n###### **Cite as** \n<div id=\"cite-as\">\n</div>\n\n",
    "supporting": [
      "ML_reg_norm_files"
    ],
    "filters": [],
    "includes": {}
  }
}