[
  {
    "objectID": "media/media.html",
    "href": "media/media.html",
    "title": "Talks & Media",
    "section": "",
    "text": "NYC Neuromodulation & Neuroergonomics Conference 2022, Deep learning framework for non-invasive closed-loop neuromodulation for attention, Invited Talk, New York NY, Jul. 2022\nNeuroergonomics Conference 2021,Introduction to practical methods in low-intensity transcranial Electrical Stimulation, Workshop, Virtual, Sep. 2021\nNYC Neuromodulation 2020, M/EEG with noninvasive brain stimulation (NIBS): Artifacts, Modeling, and Removal, Invited Talk, Virtual, Apr. 2020\nNeuromodulation: The Science, Applying HD-tES: A Practical Guide, Workshop, Napa CA, Oct. 2019\nANT neuro, Live application of tDCS/EEG: common problems and solutions, Invited Talk, EEG-tDCS & EEG-TMS methodology in research and clinical research settings, New York NY Jul. 2017\n\nThe City College of New York, Electrophysiology: theory, practical application, and artifacts, Invited Talk, Biomedical Instrumentation, New York NY, Mar. 2017\nNYC Neuromodulation 2017, A practical guide to combining HD-tDCS and EEG, Invited Talk, New York NY, Jan. 2017\nAdvanced Science Research Center, CUNY, Outdoor EEG and Concurrent Environmental Monitoring, Invited Talk, New York NY, Oct. 2016\nThe City College of New York, Biostatistics and research methods, Graduate Teaching, New York NY, 2015\nThe City College of New York, An Introduction to MATLAB and Data Visualization, Graduate Teaching, New York NY, 2015\nChegg, Math for Engineers, Data Analysis, and Programming, Tutoring, Virtual, 2013\nThe City College of New York, A practical guide to 3D printing, Zahn Center NYC, Invited Talk, New York NY, Jun. 2013"
  },
  {
    "objectID": "media/media.html#talks-teaching",
    "href": "media/media.html#talks-teaching",
    "title": "Talks & Media",
    "section": "",
    "text": "NYC Neuromodulation & Neuroergonomics Conference 2022, Deep learning framework for non-invasive closed-loop neuromodulation for attention, Invited Talk, New York NY, Jul. 2022\nNeuroergonomics Conference 2021,Introduction to practical methods in low-intensity transcranial Electrical Stimulation, Workshop, Virtual, Sep. 2021\nNYC Neuromodulation 2020, M/EEG with noninvasive brain stimulation (NIBS): Artifacts, Modeling, and Removal, Invited Talk, Virtual, Apr. 2020\nNeuromodulation: The Science, Applying HD-tES: A Practical Guide, Workshop, Napa CA, Oct. 2019\nANT neuro, Live application of tDCS/EEG: common problems and solutions, Invited Talk, EEG-tDCS & EEG-TMS methodology in research and clinical research settings, New York NY Jul. 2017\n\nThe City College of New York, Electrophysiology: theory, practical application, and artifacts, Invited Talk, Biomedical Instrumentation, New York NY, Mar. 2017\nNYC Neuromodulation 2017, A practical guide to combining HD-tDCS and EEG, Invited Talk, New York NY, Jan. 2017\nAdvanced Science Research Center, CUNY, Outdoor EEG and Concurrent Environmental Monitoring, Invited Talk, New York NY, Oct. 2016\nThe City College of New York, Biostatistics and research methods, Graduate Teaching, New York NY, 2015\nThe City College of New York, An Introduction to MATLAB and Data Visualization, Graduate Teaching, New York NY, 2015\nChegg, Math for Engineers, Data Analysis, and Programming, Tutoring, Virtual, 2013\nThe City College of New York, A practical guide to 3D printing, Zahn Center NYC, Invited Talk, New York NY, Jun. 2013"
  },
  {
    "objectID": "media/media.html#media",
    "href": "media/media.html#media",
    "title": "Talks & Media",
    "section": "Media",
    "text": "Media\n\nNIH G-RISE Research Highlight, CCNY Division of Science, 2023\nTechnology Feature in Quartz: The Objects that Power the Global Economy, pg. 64-65. 2017\nHBO Vice News, March 30, 2017\nGrove School of Engineering, City College of New York - Access to Excellence, 2014\nM/EEG with noninvasive brain stimulation (NIBS): Artifacts, Modeling, and Removal,NYC Neuromodulation, 2020\nNeuromodec’s Top Impactful Neuromodulation articles of 2021, Neuromodec, 2021"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nigel Gebodh, PhD",
    "section": "",
    "text": "I currently work on building, understanding, and integrating machine learning/artificial intelligent systems.\nI’m interested in the intersection of machine learning, intelligent agents, digital health, wearables, signal processing, and understanding brain and body connections.\nI completed my PhD work in Neural and Biomedical Engineering with fellowships awarded from the NIH and Grove School of Engineering."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Nigel Gebodh, PhD",
    "section": "",
    "text": "I currently work on building, understanding, and integrating machine learning/artificial intelligent systems.\nI’m interested in the intersection of machine learning, intelligent agents, digital health, wearables, signal processing, and understanding brain and body connections.\nI completed my PhD work in Neural and Biomedical Engineering with fellowships awarded from the NIH and Grove School of Engineering."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Nigel Gebodh, PhD",
    "section": "Experience",
    "text": "Experience\n\nMeta - Reality Labs\nPhilips - Healthcare\nSoterix Medical Inc./ Google X (Collaboration)\nHarvard Medical School - Spaulding Rehab Hospital"
  },
  {
    "objectID": "index.html#select-publications",
    "href": "index.html#select-publications",
    "title": "Nigel Gebodh, PhD",
    "section": "Select Publications",
    "text": "Select Publications\n\nA Scalable Framework for Closed-Loop Neuromodulation with Deep Learning. Nigel Gebodh, Vladimir Miskovic, Sarah Laszlo, Abhishek Datta, Marom Bikson. bioRxiv 2023.01.18.524615; doi: https://doi.org/10.1101/2023.01.18.524615\nDataset of concurrent EEG, ECG, and behavior with multiple doses of transcranial electrical stimulation. Nigel Gebodh, Zeinab Esmaeilpour, Abhishek Datta, Marom Bikson. Nature Sci Data 8, 274 (2021); doi: https://doi.org/10.1038/s41597-021-01046-y\nInherent physiological artifacts in EEG during tDCS. Nigel Gebodh, Zeinab Esmaeilpour, Devin Adair, Kenneth Chelette, Jacek Dmochowski, Adam J Woods, Emily S Kappenman, Lucas C Parra, Marom Bikson. Neuroimage 185, (2019/1/15); doi: https://doi.org/10.1016/j.neuroimage.2018.10.025\n\n\nSee all publications here."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nigel Gebodh, PhD",
    "section": "Education",
    "text": "Education\nThe Grove School of Engineering,  The City College of New York, CUNY  NIH-GRISE and Grove School of Engineering Fellow\n\nPhD & MPhil\nBuilding improved wearables, and assessing neural (sleep/attention/vigilance) and physiological function (EEG, ECG, EMG etc.) under brain stimulation with computational modeling, and machine learning.\n\n\nMSci\nUnderstanding the human visual system by mapping the early visual cortex. Applying machine learning, signal detection, and signal processing techniques.\n\n\nBE\nMedical device design, device manufacturing, and rapid prototyping."
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#project-overview",
    "href": "projects/Past_projects/ImageRecog_MLP.html#project-overview",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Project Overview",
    "text": "Project Overview\nThis notebook demonstrates the development of a neural network classifier using Keras to recognize handwritten digits from the MNIST dataset. The MNIST dataset is a widely used benchmark in machine learning, consisting of 70,000 grayscale images of handwritten digits (0-9). We will preprocess the images, build a multi-layer perceptron (MLP) model, train it, and evaluate its performance.\nThe process involves:\n\nData Loading: Importing the MNIST dataset.\nData Exploration: Understanding the structure and format of the image data.\nData Preprocessing: Reshaping, normalizing, and one-hot encoding the data.\nModel Building: Constructing a neural network architecture.\nModel Compilation: Configuring the learning process.\nModel Training: Fitting the model to the training data.\nModel Evaluation: Assessing the model’s accuracy on the test data.\nVisualization: Plotting training and validation accuracy and loss."
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#import-libraries",
    "href": "projects/Past_projects/ImageRecog_MLP.html#import-libraries",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Import Libraries",
    "text": "Import Libraries\nWe begin by importing the necessary libraries:\n\nNumPy: For numerical operations.\nKeras (TensorFlow): For building and training the neural network.\nMatplotlib: For data visualization.\n\n\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import load_img, array_to_img\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nUsing TensorFlow backend."
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#load-the-data",
    "href": "projects/Past_projects/ImageRecog_MLP.html#load-the-data",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Load the Data",
    "text": "Load the Data\nWe load the MNIST dataset, which is conveniently provided by Keras.\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nDownloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 1s 0us/step"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#data-exploration",
    "href": "projects/Past_projects/ImageRecog_MLP.html#data-exploration",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Data Exploration",
    "text": "Data Exploration\nLet’s examine the shape of the loaded data.\n\nprint(type(X_train))\nprint(X_train.shape)\nprint(y_train.shape) #60k is the answers\nprint(X_test.shape)  #10K entries\nprint(y_test.shape)\n\n\n&lt;class 'numpy.ndarray'&gt;\n(60000, 28, 28)\n(60000,)\n(10000, 28, 28)\n(10000,)\n\n\nThis shows that we have 60,000 training images and 10,000 test images, each of size 28x28 pixels.\nLet’s visualize a sample image and its corresponding label.\n\n#Lets look at the data to see what it looks like\nprint(X_train[0].shape) #Look at the size of the 1st entry\n\n#Plot it to see what it looks like\n\nplt.imshow(X_train[0])\n\n#Print the answer\nprint(\"The answer is {}\".format(y_train[0]))\n\n(28, 28)\nThe answer is 5"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#data-preprocessing",
    "href": "projects/Past_projects/ImageRecog_MLP.html#data-preprocessing",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nWe need to preprocess the image data before feeding it into the neural network.\n\nReshape: Flatten the 28x28 images into 784-dimensional vectors.\nNormalize: Scale the pixel values to the range [0, 1].\nOne-Hot Encode: Convert the labels into a categorical format.\n\n\nimage_height, image_width =28, 28\n\n\n#Lets reshape each image to be a single vector rather than a matrix\n\n#Have to flatten to plug into neural net\n\nX_train  =X_train.reshape(60000,image_height*image_width)\n\nX_test   =X_test.reshape(10000,image_height*image_width)\n\n\nprint(X_train.shape) #28X28 =784\nprint(X_test.shape)\n\n(60000, 784)\n(10000, 784)\n\n\n\n#Check to see if image is between 0-255\nprint(min(X_train[0]), max(X_train[0])) #it is! so we need to normalize\n\n#We will convert data to float (insead of int) to scale the data betwn 0-1 (not 0-255)\n\nX_train = X_train.astype('float32') #Convert to float\nX_test  = X_test.astype('float32') #Convert to float\n\n0 255\n\n\n\n#Normalize the data\nX_train /= 255.0\nX_test  /= 255.0\nprint(min(X_train[0]), max(X_train[0])) #Normalized\n\n0.0 1.0\n\n\n\n# We want the output to be in one of 9 bins to rep each of the 0-9 numbers\n#In order to do this we can convert the answers to a categorical value\n#We do this using the 'to_categorical' method\n\ny_train =to_categorical(y_train, 10)\ny_test  =to_categorical(y_test, 10)\nprint(y_train.shape)\nprint(y_test.shape)\n\n(60000, 10)\n(10000, 10)\n\n\n\nprint(y_test[0])\nplt.imshow(X_test[0].reshape(image_height, image_width))\n\n[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#build-the-model",
    "href": "projects/Past_projects/ImageRecog_MLP.html#build-the-model",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Build the Model",
    "text": "Build the Model\nWe construct a sequential neural network model with three dense layers.\n\n#Assign the model type\nmodel = Sequential()\n\nWARNING: Logging before flag parsing goes to stderr.\nW0820 07:17:37.393794 140395328268160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n\n\n\n#Add layers to the model\n\nmodel.add(Dense(512, activation='relu',input_shape=(784,)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n\nW0820 07:17:37.444703 140395328268160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nW0820 07:17:37.461894 140395328268160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead."
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#compile-the-model",
    "href": "projects/Past_projects/ImageRecog_MLP.html#compile-the-model",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Compile the Model",
    "text": "Compile the Model\nWe compile the model with the Adam optimizer, categorical cross-entropy loss, and accuracy metric.\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nW0820 07:17:37.521929 140395328268160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nW0820 07:17:37.561933 140395328268160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n\n\n\n\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               262656    \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 669,706\nTrainable params: 669,706\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#calculating-the-number-of-parameters-for-each-layer",
    "href": "projects/Past_projects/ImageRecog_MLP.html#calculating-the-number-of-parameters-for-each-layer",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Calculating the number of parameters for each layer",
    "text": "Calculating the number of parameters for each layer\n\nLayer 1\n\nAfter flattening each image we get:\n\n28 X 28=784\n\nWe then pass the 784 into 512 nodes in the model plus a bias layer 512 (zeros)\nThis gives:\n\n784(pixels) X 512(neurons) X 512(bias)=401920\n\n\n\n\nLayer 2\n\nWe have 512 (output from previous), going into another 512 nodes (in new layer), plus another 512\nThis gives:\n\n512 (input) X 512 (this layer) X 512 =262656\n\n\n\n\nLayer 3\n\nWe have 512 (incoming from last layer), going into 10 nodes (in this layer), 10 bias units\nThis gives:\n\n512 (last layer) X 10 (nodes in this layer) + 10 (bias) =5130"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#train-the-model",
    "href": "projects/Past_projects/ImageRecog_MLP.html#train-the-model",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Train the model",
    "text": "Train the model\nNow we can train our model. To do this we have to pass: * Training data * Number of epochs (the number of times that model passes through the training data) * Validation data (testing data)\n\nhistory =model.fit(X_train, y_train, epochs =20, validation_data=(X_test, y_test))\n\nW0820 07:17:37.736055 140395328268160 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nW0820 07:17:37.796303 140395328268160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\n\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/20\n60000/60000 [==============================] - 24s 394us/step - loss: 0.1828 - acc: 0.9440 - val_loss: 0.0929 - val_acc: 0.9705\nEpoch 2/20\n60000/60000 [==============================] - 23s 386us/step - loss: 0.0808 - acc: 0.9757 - val_loss: 0.0827 - val_acc: 0.9743\nEpoch 3/20\n60000/60000 [==============================] - 23s 389us/step - loss: 0.0565 - acc: 0.9828 - val_loss: 0.0695 - val_acc: 0.9786\nEpoch 4/20\n60000/60000 [==============================] - 23s 381us/step - loss: 0.0429 - acc: 0.9864 - val_loss: 0.0832 - val_acc: 0.9774\nEpoch 5/20\n60000/60000 [==============================] - 23s 386us/step - loss: 0.0353 - acc: 0.9887 - val_loss: 0.0921 - val_acc: 0.9745\nEpoch 6/20\n60000/60000 [==============================] - 23s 387us/step - loss: 0.0287 - acc: 0.9910 - val_loss: 0.0819 - val_acc: 0.9782\nEpoch 7/20\n60000/60000 [==============================] - 23s 387us/step - loss: 0.0272 - acc: 0.9914 - val_loss: 0.0807 - val_acc: 0.9802\nEpoch 8/20\n60000/60000 [==============================] - 24s 395us/step - loss: 0.0237 - acc: 0.9924 - val_loss: 0.1136 - val_acc: 0.9771\nEpoch 9/20\n60000/60000 [==============================] - 23s 390us/step - loss: 0.0201 - acc: 0.9938 - val_loss: 0.1083 - val_acc: 0.9800\nEpoch 10/20\n60000/60000 [==============================] - 23s 381us/step - loss: 0.0202 - acc: 0.9939 - val_loss: 0.1016 - val_acc: 0.9798\nEpoch 11/20\n60000/60000 [==============================] - 23s 390us/step - loss: 0.0170 - acc: 0.9951 - val_loss: 0.1167 - val_acc: 0.9783\nEpoch 12/20\n60000/60000 [==============================] - 23s 387us/step - loss: 0.0175 - acc: 0.9948 - val_loss: 0.1026 - val_acc: 0.9805\nEpoch 13/20\n60000/60000 [==============================] - 23s 381us/step - loss: 0.0179 - acc: 0.9950 - val_loss: 0.1039 - val_acc: 0.9811\nEpoch 14/20\n60000/60000 [==============================] - 23s 377us/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.1173 - val_acc: 0.9809\nEpoch 15/20\n60000/60000 [==============================] - 22s 374us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.1135 - val_acc: 0.9801\nEpoch 16/20\n60000/60000 [==============================] - 25s 415us/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.1391 - val_acc: 0.9792\nEpoch 17/20\n60000/60000 [==============================] - 24s 397us/step - loss: 0.0151 - acc: 0.9964 - val_loss: 0.1211 - val_acc: 0.9819\nEpoch 18/20\n60000/60000 [==============================] - 24s 400us/step - loss: 0.0159 - acc: 0.9962 - val_loss: 0.1208 - val_acc: 0.9800\nEpoch 19/20\n60000/60000 [==============================] - 24s 403us/step - loss: 0.0166 - acc: 0.9960 - val_loss: 0.1309 - val_acc: 0.9808\nEpoch 20/20\n60000/60000 [==============================] - 24s 397us/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.1310 - val_acc: 0.9813\n\n\n\nTraining Accuracy Visualization\nTo understand how well our model learned during the training phase, we can visualize the training accuracy over each epoch. The history object, returned by the model.fit() method, stores the training metrics. We’ll plot the ‘acc’ key from this dictionary, which represents the training accuracy, against the epoch number.\nThis graph will show us how the model’s accuracy improved as it was exposed to more training data. Ideally, we should see a steady increase in accuracy over epochs.\n\n\nPlot the accuracy of the training model\n\n#Look at the attributes in the history object to find the accuracy\nhistory.__dict__\n\n{'epoch': [0,\n  1,\n  2,\n  3,\n  4,\n  5,\n  6,\n  7,\n  8,\n  9,\n  10,\n  11,\n  12,\n  13,\n  14,\n  15,\n  16,\n  17,\n  18,\n  19],\n 'history': {'acc': [0.9439666666666666,\n   0.9757333333333333,\n   0.9827833333333333,\n   0.9864166666666667,\n   0.9886666666666667,\n   0.9910333333333333,\n   0.9914,\n   0.99245,\n   0.9938166666666667,\n   0.9939,\n   0.99515,\n   0.9948,\n   0.9950166666666667,\n   0.9956333333333334,\n   0.9947333333333334,\n   0.9965166666666667,\n   0.9963833333333333,\n   0.9962166666666666,\n   0.99595,\n   0.99645],\n  'loss': [0.1827596818920225,\n   0.08079896697839722,\n   0.05645396511411139,\n   0.04291815567353721,\n   0.03526910000597515,\n   0.02873079521368248,\n   0.02715607473684601,\n   0.023650182965393438,\n   0.020055528101623546,\n   0.02019607128013062,\n   0.016955279541049723,\n   0.017472221146037314,\n   0.017864977817751575,\n   0.015457480643335983,\n   0.017869793417473495,\n   0.012631182595215281,\n   0.015135916414613901,\n   0.015882995463786898,\n   0.016569432756344288,\n   0.013335457366452594],\n  'val_acc': [0.9705,\n   0.9743,\n   0.9786,\n   0.9774,\n   0.9745,\n   0.9782,\n   0.9802,\n   0.9771,\n   0.98,\n   0.9798,\n   0.9783,\n   0.9805,\n   0.9811,\n   0.9809,\n   0.9801,\n   0.9792,\n   0.9819,\n   0.98,\n   0.9808,\n   0.9813],\n  'val_loss': [0.09286645495379343,\n   0.08266143489209934,\n   0.069480553943431,\n   0.08320518101718044,\n   0.09206652115154429,\n   0.08190068501315655,\n   0.08067529291427782,\n   0.11358439496830543,\n   0.10833151409866154,\n   0.10160923933375093,\n   0.11671308373045626,\n   0.10255490619101375,\n   0.10387474813488247,\n   0.11728941089477675,\n   0.11347036018394005,\n   0.13906407877868832,\n   0.12108565404413693,\n   0.120797497302599,\n   0.1309188434239974,\n   0.13095201672244552]},\n 'model': &lt;keras.engine.sequential.Sequential at 0x7fb0320f3400&gt;,\n 'params': {'batch_size': 32,\n  'do_validation': True,\n  'epochs': 20,\n  'metrics': ['loss', 'acc', 'val_loss', 'val_acc'],\n  'samples': 60000,\n  'steps': None,\n  'verbose': 1},\n 'validation_data': [array([[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([[0., 0., 0., ..., 1., 0., 0.],\n         [0., 0., 1., ..., 0., 0., 0.],\n         [0., 1., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)]}\n\n\n\n#Plot the accuracy\nplt.plot(history.history['acc'],label='train')\nplt.xlabel('Epoch Number')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy Over Epoch')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nTraining vs. Validation Accuracy\nTo assess if our model is generalizing well to unseen data, we’ll compare the training accuracy with the validation accuracy. The validation accuracy is calculated on the test dataset during training, providing insights into how the model performs on data it hasn’t been explicitly trained on.\nBy plotting both training and validation accuracies, we can identify potential overfitting. If the training accuracy is significantly higher than the validation accuracy, it might indicate that the model is memorizing the training data rather than learning general patterns.\n\n#Plot the accuracy of training data and validation data\nplt.plot(history.history['acc'],label='train')\nplt.plot(history.history['val_acc'],label='val')\nplt.xlabel('Epoch Number')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy Over Epoch')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nTraining vs. Validation: Accuracy and Loss\nIn addition to accuracy, the loss function provides valuable information about the model’s performance. The loss represents the error between the model’s predictions and the actual labels.\n\n#Plot the accuracy of training data and validation data AND loss\nplt.plot(history.history['acc'],label='train')\nplt.plot(history.history['val_acc'],label='val')\nplt.plot(history.history['loss'],label='loss')\nplt.xlabel('Epoch Number')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy Over Epoch')\nplt.legend()\n# plt.yscale('log')\n\n\n\n\n\n\n\n\n\n\nModel Evaluation on Test Data\nAfter training our model, we need to evaluate its performance on unseen data to assess its generalization ability.\n\nscore=model.evaluate(X_test, y_test)\n\n10000/10000 [==============================] - 1s 84us/step\n\n\n\n#We get score as a list\n#The second item in score gives us the accuracy of or model\nscore\n\n[0.13095201672244552, 0.9813]"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_MLP.html#archived",
    "href": "projects/Past_projects/ImageRecog_MLP.html#archived",
    "title": "Digit Recognition with MLP from Scratch",
    "section": "Archived",
    "text": "Archived\nProject Archive Note:\nThis project is archived.  Please note that library and framework versions may be outdated. Last updated:\n\nApril 2025"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_ResNet50.html#about-this-project",
    "href": "projects/Past_projects/ImageRecog_ResNet50.html#about-this-project",
    "title": "Image Recognition & Classification with ResNet50",
    "section": "About this project",
    "text": "About this project\nWe’re going to use a pretrained model included in Keras to do image recognition. Keras comes with several image recognition models including\n\nVGG (Visual Geometry Group at the University of Oxford)\nResNet50 (MS research)\nInception-v3 (Google)\nXception (Keras)\n\nWe’ll use the ResNet50 in this project. This paper describes ResNets. Check out this link for details on the model and ResNets in general.\nWe have a few images to pick from to see how well the image classifier performs"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_ResNet50.html#import-and-set-up-model",
    "href": "projects/Past_projects/ImageRecog_ResNet50.html#import-and-set-up-model",
    "title": "Image Recognition & Classification with ResNet50",
    "section": "Import and set up model",
    "text": "Import and set up model\n\nimport numpy as np\n# from tensorflow.keras import backend\nfrom keras.preprocessing import image\nfrom tensorflow.keras.applications import resnet50 # &lt;model\n\n\n#Image visualization\n%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\nUsing TensorFlow backend.\n\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\n#Now we load the imported model \nmodel =resnet50.ResNet50() #Create a new instance of the model \n\n\nDetails of the model layers\nAfter we load the model we can use summary() to look at the model’s layers and data flow structure.\n\nmodel.summary()\n\nModel: \"resnet50\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nconv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n__________________________________________________________________________________________________\nbn_conv1 (BatchNormalizationV1) (None, 112, 112, 64) 256         conv1[0][0]                      \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n__________________________________________________________________________________________________\npool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n__________________________________________________________________________________________________\nres2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nbn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_1[0][0]               \n__________________________________________________________________________________________________\nbn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_2[0][0]               \n__________________________________________________________________________________________________\nres2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nbn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd (Add)                       (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n                                                                 bn2a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 56, 56, 256)  0           add[0][0]                        \n__________________________________________________________________________________________________\nres2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_3[0][0]               \n__________________________________________________________________________________________________\nbn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_4[0][0]               \n__________________________________________________________________________________________________\nbn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_5[0][0]               \n__________________________________________________________________________________________________\nbn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n                                                                 activation_3[0][0]               \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n__________________________________________________________________________________________________\nres2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_6[0][0]               \n__________________________________________________________________________________________________\nbn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_7[0][0]               \n__________________________________________________________________________________________________\nbn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_8[0][0]               \n__________________________________________________________________________________________________\nbn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n                                                                 activation_6[0][0]               \n__________________________________________________________________________________________________\nactivation_9 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n__________________________________________________________________________________________________\nres3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_9[0][0]               \n__________________________________________________________________________________________________\nbn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_10 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_10[0][0]              \n__________________________________________________________________________________________________\nbn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_11 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_11[0][0]              \n__________________________________________________________________________________________________\nres3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_9[0][0]               \n__________________________________________________________________________________________________\nbn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n                                                                 bn3a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_12 (Activation)      (None, 28, 28, 512)  0           add_3[0][0]                      \n__________________________________________________________________________________________________\nres3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_12[0][0]              \n__________________________________________________________________________________________________\nbn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_13 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_13[0][0]              \n__________________________________________________________________________________________________\nbn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_14 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_14[0][0]              \n__________________________________________________________________________________________________\nbn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n                                                                 activation_12[0][0]              \n__________________________________________________________________________________________________\nactivation_15 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n__________________________________________________________________________________________________\nres3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_15[0][0]              \n__________________________________________________________________________________________________\nbn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_16 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_16[0][0]              \n__________________________________________________________________________________________________\nbn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_17 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_17[0][0]              \n__________________________________________________________________________________________________\nbn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n                                                                 activation_15[0][0]              \n__________________________________________________________________________________________________\nactivation_18 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n__________________________________________________________________________________________________\nres3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_18[0][0]              \n__________________________________________________________________________________________________\nbn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_19 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_19[0][0]              \n__________________________________________________________________________________________________\nbn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_20 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_20[0][0]              \n__________________________________________________________________________________________________\nbn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n                                                                 activation_18[0][0]              \n__________________________________________________________________________________________________\nactivation_21 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n__________________________________________________________________________________________________\nres4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_21[0][0]              \n__________________________________________________________________________________________________\nbn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_22 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_22[0][0]              \n__________________________________________________________________________________________________\nbn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_23[0][0]              \n__________________________________________________________________________________________________\nres4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_21[0][0]              \n__________________________________________________________________________________________________\nbn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd_7 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n                                                                 bn4a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_24 (Activation)      (None, 14, 14, 1024) 0           add_7[0][0]                      \n__________________________________________________________________________________________________\nres4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_24[0][0]              \n__________________________________________________________________________________________________\nbn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_25 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_25[0][0]              \n__________________________________________________________________________________________________\nbn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_26[0][0]              \n__________________________________________________________________________________________________\nbn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_8 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n                                                                 activation_24[0][0]              \n__________________________________________________________________________________________________\nactivation_27 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n__________________________________________________________________________________________________\nres4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_27[0][0]              \n__________________________________________________________________________________________________\nbn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_28 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_28[0][0]              \n__________________________________________________________________________________________________\nbn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_29[0][0]              \n__________________________________________________________________________________________________\nbn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_9 (Add)                     (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n                                                                 activation_27[0][0]              \n__________________________________________________________________________________________________\nactivation_30 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n__________________________________________________________________________________________________\nres4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_30[0][0]              \n__________________________________________________________________________________________________\nbn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_31 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_31[0][0]              \n__________________________________________________________________________________________________\nbn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_32[0][0]              \n__________________________________________________________________________________________________\nbn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_10 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n                                                                 activation_30[0][0]              \n__________________________________________________________________________________________________\nactivation_33 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n__________________________________________________________________________________________________\nres4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_33[0][0]              \n__________________________________________________________________________________________________\nbn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_34 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_34[0][0]              \n__________________________________________________________________________________________________\nbn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_35[0][0]              \n__________________________________________________________________________________________________\nbn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_11 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n                                                                 activation_33[0][0]              \n__________________________________________________________________________________________________\nactivation_36 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n__________________________________________________________________________________________________\nres4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_36[0][0]              \n__________________________________________________________________________________________________\nbn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_37 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_37[0][0]              \n__________________________________________________________________________________________________\nbn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_38[0][0]              \n__________________________________________________________________________________________________\nbn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_12 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n                                                                 activation_36[0][0]              \n__________________________________________________________________________________________________\nactivation_39 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n__________________________________________________________________________________________________\nres5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_39[0][0]              \n__________________________________________________________________________________________________\nbn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_40 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_40[0][0]              \n__________________________________________________________________________________________________\nbn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_41[0][0]              \n__________________________________________________________________________________________________\nres5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_39[0][0]              \n__________________________________________________________________________________________________\nbn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd_13 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n                                                                 bn5a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_42 (Activation)      (None, 7, 7, 2048)   0           add_13[0][0]                     \n__________________________________________________________________________________________________\nres5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_42[0][0]              \n__________________________________________________________________________________________________\nbn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_43 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_43[0][0]              \n__________________________________________________________________________________________________\nbn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_44[0][0]              \n__________________________________________________________________________________________________\nbn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_14 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n                                                                 activation_42[0][0]              \n__________________________________________________________________________________________________\nactivation_45 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n__________________________________________________________________________________________________\nres5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_45[0][0]              \n__________________________________________________________________________________________________\nbn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_46 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_46[0][0]              \n__________________________________________________________________________________________________\nbn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_47[0][0]              \n__________________________________________________________________________________________________\nbn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_15 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n                                                                 activation_45[0][0]              \n__________________________________________________________________________________________________\nactivation_48 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n__________________________________________________________________________________________________\navg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_48[0][0]              \n__________________________________________________________________________________________________\nfc1000 (Dense)                  (None, 1000)         2049000     avg_pool[0][0]                   \n==================================================================================================\nTotal params: 25,636,712\nTrainable params: 25,583,592\nNon-trainable params: 53,120\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_ResNet50.html#load-image-to-predict",
    "href": "projects/Past_projects/ImageRecog_ResNet50.html#load-image-to-predict",
    "title": "Image Recognition & Classification with ResNet50",
    "section": "Load image to predict",
    "text": "Load image to predict\n\nDownsample the image\n\n#Load in the sample image we want to predict\nimgloc='shark.jpg'\n\nimg =image.load_img(imgloc) #Load in the image\n\nprint(shape(img))#Size of image\n\n#Look at loaded image\nplt.figure()\nplt.imshow(img) \nplt.title('Original')\nplt.show()  # display it\n\n\n\n#Because the image is too big we need to downsample it in order to \n#pass it to Keras. Keras needs imagesize to be matched to \n#the number of input nodes in the model in this case 224x224 pxls \n\nimgName = imgloc#'bay.jpg'\n#Re- Load in the sample image we want to predict\nimg =image.load_img(imgName, target_size=(224,224)) #Load in the image and resize\n\n\nprint(shape(img))#Size of image\n\n#Look at loaded image\nplt.figure()\nplt.imshow(img) \nplt.title('Downsampled')\nplt.show()  # display it\n\n(351, 624, 3)\n(224, 224, 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore Preprocessing steps\n\n#Convert image to array (flatten)\nx=image.img_to_array(img)\n\n#The neural network is actually expecting (a list) more than 1 image so we will trick it\n#Add a 4th dimension to the array\n\nx= np.expand_dims(x,axis=0)\n\n#Normalize data to 0-1 instead of 0-255\nx=resnet50.preprocess_input(x)\n\n\n\nRun neural network and make prediction\n\n#Now we run the normalized data through the network and predict\npredictions =model.predict(x)\n\n#We will get back a predictions object with 1000 element array. \n#Each element reps a probability that the input matches each of the 1000 objects \n#that the network was trained on.\n\n#Here we use a function to tell us the names of the objects that \n#the network predicted. We only want to top 10 so we ask for ony 10\n\nprediction_classes =resnet50.decode_predictions(predictions, top=10)\n\n#Print out all the predictions\nfor imagenet_id, name, likelihood in prediction_classes[0]:\n    print(\"-{}: {:2f} % likelihood\".format(name, likelihood*100))\n    \n#Look at loaded image\nplt.figure()\nplt.imshow(img) \nplt.title('Downsampled')\nplt.show()  # display it\n\n-hammerhead: 65.850562 % likelihood\n-great_white_shark: 21.884859 % likelihood\n-tiger_shark: 12.248133 % likelihood\n-electric_ray: 0.006338 % likelihood\n-albatross: 0.002689 % likelihood\n-sturgeon: 0.002292 % likelihood\n-stingray: 0.001961 % likelihood\n-killer_whale: 0.001225 % likelihood\n-grey_whale: 0.000721 % likelihood\n-loggerhead: 0.000335 % likelihood"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_ResNet50.html#put-it-all-into-a-function",
    "href": "projects/Past_projects/ImageRecog_ResNet50.html#put-it-all-into-a-function",
    "title": "Image Recognition & Classification with ResNet50",
    "section": "Put it all into a function!",
    "text": "Put it all into a function!\n\ndef imgClasser(locc):\n    imgName = locc\n    #Re- Load in the sample image we want to predict\n    img =image.load_img(imgName, target_size=(224,224)) #Load in the image and resize\n    \n    #Convert image to array (flatten)\n    x=image.img_to_array(img)\n\n    #The neural network is actually expecting (a list) more than 1 image so we will trick it\n    #Add a 4th dimension to the array\n\n    x= np.expand_dims(x,axis=0)\n\n    #Normalize data to 0-1 instead of 0-255\n    x=resnet50.preprocess_input(x)\n    \n    \n    #Now we run the normalized data through the network and predict\n    predictions =model.predict(x)\n\n    #We will get back a predictions object with 1000 element array. \n    #Each element reps a probability that the input matches each of the 1000 objects \n    #that the network was trained on.\n\n    #Here we use a function to tell us the names of the objects that \n    #the network predicted. We only want to top 10 so we ask for ony 10\n\n    prediction_classes =resnet50.decode_predictions(predictions, top=10)\n\n    #Print out all the predictions\n    for imagenet_id, name, likelihood in prediction_classes[0]:\n        print(\"-{}: {:2f} % likelihood\".format(name, likelihood*100))\n    #Look at loaded image\n    plt.figure()\n    plt.imshow(img) \n    plt.title('Downsampled')\n    plt.show()  # display it"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_ResNet50.html#archived",
    "href": "projects/Past_projects/ImageRecog_ResNet50.html#archived",
    "title": "Image Recognition & Classification with ResNet50",
    "section": "Archived",
    "text": "Archived\nProject Archive Note:\nThis project is archived.  Please note that library and framework versions may be outdated. Last updated:\n\nApril 2025"
  },
  {
    "objectID": "projects/2024-03-05/index_demo.html",
    "href": "projects/2024-03-05/index_demo.html",
    "title": "LLM chatbot with models from Meta, Google, and Mistral AI",
    "section": "",
    "text": "Photo by and machines on Unsplash"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBuilding LLM Clinical Data Analytics Pipelines with AWS Bedrock\n\n\nLeveraging LLMs like Claude and Gemma for clinical data analysis\n\n\n\n\n\n35 min\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Healthcare Data Analysis\n\n\nInteractive LLM Clinical Data Analysis\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Heart Beats to Algorithms With ECG Data\n\n\nLooking at cardiac data processing, feature extraction, and analysis in Python\n\n\n\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Does My LLM Have A Temperature?\n\n\nUnderstanding the temperature parameter in LLMs\n\n\n\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Brief Guide to Vector Norms in Machine Learning\n\n\nUnderstanding the basic application of norms in machine learning with Python examples\n\n\n\n\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Neuromodulation Landscape\n\n\nExploring the hierarchy with different types of neuromodulation\n\n\n\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nLLM chatbot with models from Meta, Google, and Mistral AI\n\n\nInteractive LLM chatbot with multiple models like Llama, Gemma, Mistral, and Zephyr\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models - Chatting with AI Chatbots from Google, Mistral AI, and Hugging Face\n\n\nUsing Streamlit and Hugging Face to chat with Gemma, Mistral, and Zephyr\n\n\n\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nFacial Recognition with Principal Component Analysis\n\n\nUsing PCA to identify famous faces (Eigen Face)\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nImage Recognition & Classification with ResNet50\n\n\nUsing the ResNet50 model to classify images\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nImage Recognition & Classification with VGG16\n\n\nUsing the VGG16 model to classify images\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nDigit Recognition with MLP from Scratch\n\n\nUsing a MLP to classify digits\n\n\n\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html",
    "href": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html",
    "title": "From Heart Beats to Algorithms With ECG Data",
    "section": "",
    "text": "Here we look at an overview of the Electrocardiogram (ECG), its history, and analyzing and extracting features from ECG data."
  },
  {
    "objectID": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#what-exactly-is-an-ecg-and-some-brief-history",
    "href": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#what-exactly-is-an-ecg-and-some-brief-history",
    "title": "From Heart Beats to Algorithms With ECG Data",
    "section": "What exactly is an ECG and some brief history?",
    "text": "What exactly is an ECG and some brief history?\n\n\nECG and History\n\n\nCardiac data can mean a lot of things. Generally it refers to data from or relating to the heart or cardiovascular system. The term cardiac comes from the Greek word kardia, and roughly translating to heart.\nHere we’ll focus on looking at the electrocardiogram or ECG (or German elektrokardiogram- EKG) which measures the electrical activity of the heart over time. This non-invasive measurement provides critical insights into heart rhythm, function, and overall cardiovascular/ mental health.\nThe term electrocardiogram roughly breaks into:\n\nelectro - electricity\ncardio - heart\ngram - writing\n\n\n\nBrief History\nThe typical ECG signal we know today is credited to work by Willem Einthoven between 1893-1895 (Einthoven, 1895). Einthoven, innovated on the work of Augustus Desire Waller (who introduced the term “electrocardiogram” ) and others by developing more sensitive and smaller detection instrumentation, which he called the string galvanometer (Einthoven, 1912).\nWith more sensitive equipment, Einthoven was able to discern more distinct cardiac peaks, which he labeled as the PQRST peaks. The naming convention is debated but it’s believed to be inspired by Einthoven’s study of work by Descartes (Hurst, 1998). Some of the earliest ECG machines were huge, required multiple operators, and patients had to have their arms and legs submerged in electrolyte (salty water) solutions. Einthoven was even approached by Charles Darwin’s son, Horace Darwin, to help sell and distribute the ECG machines (Burnett, 1985).\n\n\n\n\nA. An early form of the ECG monitoring device. B-C. Einthoven’s correction and demarcation of points on the ECG waveform.\n\n\n\n\n\nECG Waveform\nThe ECG waveform reflects different phases of the heart’s contraction. Each of the PQRST points on the ECG waveform correspond to:\n\nP wave - Atrial depolarization\nQRS complex - Ventricular depolarization\nT wave - Ventricular repolarization\n\n\n\n\n\nThe cardiac cycle and the origin of each part of the ECG waveform. Image source.\n\n\n\n\n\nECG in the Modern Era\nSince its inception ECG acquisition has become digitized and miniaturized to the point where we can fit acquisition tools into a smart watch. Apple’s and other companies’ inclusion of such monitoring and diagnostic tools helped to democratize and increase access to cardiac health monitoring. Studies like the Apple Heart Study have demonstrated that wearable technology can accurately detect irregular heart rhythms, leading to earlier identification of conditions like atrial fibrillation and empowering users to take proactive steps in managing their heart health.\n\n\nECG in Context\nECG is a part of cardiac monitoring which is a broad term that encompasses a variety of methods for monitoring and assessing cardiac health. Taking a step back, we see ECG falls under EXG techniques, which is a term used to describe a group of non-invasive/minimally invasive methods that measure electrical activity in the body.\nFor example, some EXG techniques include:\n\nElectromyography (EMG) - detection of electrical activity in muscles\nElectroencephalography (EEG) - detection of electrical activity in the brain\nElectrogastrography (EGG) - detection of electrical activity in the gut\nElectrooculography (EOG) - detection of electrical activity in the eyes\nElectroretinography (ERG) - detection of electrical activity in the retina\n\nNote: While the heart is classified as a muscle, its muscle tissue differs significantly from skeletal muscle. The heart muscle cells, known as cardiomyocytes, and tissue have distinct anatomical features that set it apart from other muscles. Unlike skeletal muscle, which requires conscious control for contraction, cardiomyocytes possess the unique ability to contract and relax autonomously, without voluntary input. This fundamental difference is one of the key factors that distinguishes electrocardiography (ECG) from electromyography (EMG).\nOther methods that monitor cardiac activity include:\n\nPhotoplethysmography - light based detection of blood volume changes in the skin\nPhonocardiography - sound based detection of heart sounds\nBallistocardiography - detection of the force of the heart on the body\nImpedance cardiography - detection of changes in the impedance of the chest"
  },
  {
    "objectID": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#lets-get-started",
    "href": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#lets-get-started",
    "title": "From Heart Beats to Algorithms With ECG Data",
    "section": "Let’s get Started",
    "text": "Let’s get Started\n\nAbout the data\n\n\nDetails\n\nThe data that we’ll be working with here comes from one of my experiments looking at EEG, ECG, and vigilant behavior concurrently over time (Gebodh et al., 2021). We collected EEG and lead I ECG data over a 70-minute period while participants played a ball moving game and at times were stimulated with low intensity electrical stimulation (Check out my chapters on brain stimulation here (Moreno-Duarte et al., 2014) and here (Gebodh et al., 2019) ). We’ll focus on the first 20 minutes of data since its free from artifacts of electrical stimulation.\n\n\n\n\nSnippet of the GX dataset. The data contains EEG, ECG, EOG, and behavioral data.\n\n\n\n\n\n\nProcessing Pipeline\n\n\nDetails\n\nWhen working with signals it’s good practice to have a processing pipeline and adjust as needed. Here our processing pipeline will be to:\n\nImport the data\nPreprocess the data:\n\nRemove baseline wandering. These low frequency drifts can happen because of sweat or changes in skin impedances. We can attenuate this activity by high-pass filtering the signal.\nRemove high frequency noise. This can happen when muscles under the electrodes like the pectoral muscles are active causing the ECG waveform to become distorted and contaminated with EMG signals. We can attenuate this with low-pass filtering.\nDownsample the signal. Since the signal is sampled at a high frequency (2k Hz) it contains more than enough samples to properly reconstruct the ECG signals so for faster processing we can downsample the signal to a lower frequency.\n\nExtract ECG features\n\n\n\n\nDownloading The Data\nWe’ll start off by downloading and importing the data.\nThe data are uploaded to the OpenNeuro database, which supports neuroscience data sharing, reproducibility, and analysis. Our data can be viewed here. The data are divided into different folders, each corresponding to a different participant (e.g. sub-01 corresponds to participant 1). Subfolders corresponding to different sessions (e.g. ses-02 corresponds to session 2).\nTo download the data, we can use curl, which is a command line tool for transferring data.\n\n\nLong Explanation\n\nFor example in a Command Line Interface (CLI), like command prompt etc., we can run the following command to download the data to a specific folder:\ncurl &lt;DATA URL&gt; -o &lt;OUTPUT FILE PATH & NAME&gt;\nThe data files are broken into different files, each containing a different type of data. The main data files are:\n\n.set and .fdt - contains the recorded timerseries data and metadata\n.json - contains metadata for the experiment\n.tsv - contains the events data (triggers) for the timeseries and EEG electrode locations\n\nThe URL for our data can be broken down as follows for participant 22, session 1:\n\nBase URL: https://s3.amazonaws.com/openneuro.org/\nDataset ID: ds003670/\nParticipant folder: sub-022/\nSession folder: ses-01/\nData type(all are the same): eeg/\nFile name: sub-022_ses-01_task-GXtESCTT_eeg.json\n\nSo, the full DATA URL for the file we want to download is: \nhttps://s3.amazonaws.com/openneuro.org/ds003670/sub-022/ses-01/eeg/sub-022_ses-01_task-GXtESCTT_eeg.json\nWe can then download the data to a specific output folder with specific file name by using the -o flag:\n\nOutput file path: ./temp_data/\nOutput file name (same as downloaded): sub-022_ses-01_task-GXtESCTT_eeg.json\n\nSo, the full OUTPUT FILE PATH & NAME for the data is: \n./temp_data/sub-022_ses-01_task-GXtESCTT_eeg.json\nWe can then use the curl command to download the data to the output file path and name:\n\nimport os\nimport subprocess\n\n# Define the URL and output file path\nfile_name = \"sub-022_ses-01_task-GXtESCTT_eeg.json\"\nurl = \"https://s3.amazonaws.com/openneuro.org/ds003670/sub-022/ses-01/eeg/\" + file_name\noutput_file_path = os.path.join(\"./temp_data/\", file_name )\n\n# Download the file using curl\nsubprocess.run([\"curl\", url, \"-o\", output_file_path])\n\n# OR\n\nos.system(f\"curl {url} -o {output_file_path}\") #Works but not recommended\nOnce the data are downloaded, we can double check to see what files were downloaded.\n\nfor item in os.listdir(\"./temp_data/\"):\n    print(f\"* {item}\")\n\n* sub-022_ses-01_task-GXtESCTT_eeg.fdt\n* sub-022_ses-01_task-GXtESCTT_eeg.json\n* sub-022_ses-01_task-GXtESCTT_eeg.set\n* sub-022_ses-01_task-GXtESCTT_electrodes.tsv\n* sub-022_ses-01_task-GXtESCTT_events.tsv"
  },
  {
    "objectID": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#data-import-and-visualization",
    "href": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#data-import-and-visualization",
    "title": "From Heart Beats to Algorithms With ECG Data",
    "section": "Data Import and Visualization",
    "text": "Data Import and Visualization\nOnce the data is downloaded, we can import it using mne. From there we can extract segments to process and visualize.\n\n\nCode\n\nimport mne\nimport numpy as np\nimport bokeh\n\ntemp_folder = './temp_data'\nfile_in = 'sub-001_ses-001_task-rest_eeg'\necg_electrodes = 32 # ECG electrode\n\n# Load the data\n# Set preload=False to use memory mapping (lazy loading)\nraw = mne.io.read_raw_eeglab(f'{temp_folder}{file_in}.set', preload=False)\n\n# Load the TSV file - contains events (remove `_eeg` from the end of the file name)\nevents_df = pd.read_csv(f'{temp_folder}{file_in[:-3]}events.tsv', sep='\\t')\n\n\nsfreq = raw.info['sfreq']# Data sampling frequency\n\n# Extract the ECG data for the first 10 seconds\nstart, stop = 0, 30  # in seconds\ndata, times = raw[ecg_electrodes, start * sfreq :stop * sfreq ]\n\n\n# Plot the ECG data with Bokeh\np = bokeh.plotting.figure(title=\"EEG Data\", \n                x_axis_label='Time (s)', \n                y_axis_label='Amplitude')\n                \np.line(times, data[0])\nshow(p)\n\nLet’s look at a snippet of the data.\n\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n  \n\n\n\n\n\nWhat do we see?\nIf we look closely at the signal, we notice a few things:\n\nSignal offset. On the y-axis the signal is offset from zero. This is because the signal is measured with respect to a reference point and there is a large potential difference between both the electrodes and the reference point.\nSignal drift. We notice that the signal is drifting (it’s not constantly centered at ~-2.5 mV). We can see the drift line (Low-Freq Drift) in the plot. Drifting is a common problem in ECG signals (typically 0.05-1 Hz) and can be caused by a number of factors, including changes in the participant’s position, body temperature, or the electrode-skin interface.\nSignal noise. We can see that the signal has some high frequency noise. This is highlighted in the plot (High-Freq Noise) and looks like sharp spikes (typically 20-1k Hz). In this case this was most likely caused by the movement of the participant’s arms, which results in detecting electrical signals from the underlying muscles (or an electromyographic -EMG signal)."
  },
  {
    "objectID": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#data-preprocessing",
    "href": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#data-preprocessing",
    "title": "From Heart Beats to Algorithms With ECG Data",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nFiltering\nWe can attenuate (reduce) some of this unwanted activity in the ECG signal by filtering it. We’ll apply a high pass filter to remove the low frequency noise (drift, offset etc.) and a low pass filter to remove the high frequency noise (EMG noise etc.). It’s important to note that filtering can distort the signal and attenuate fiducial points on the ECG waveform so the bandlimits of the filters should be chosen carefully. Typically, most of the useful frequency content of an ECG waveform is between 0.5-40 Hz.\nHere we’ll use the Butterworth filter (an IIR filter) to filter the ECG signal. The Butterworth filter has a maximally flat frequency response in the passband, meaning it results in minimal distortion to the signal. Other types of IIR filters include the Chebyshev filter, Bessel filter, and elliptic filter.\nThere are many other filtering techniques that can be used to remove noise from/clean ECG data (other than high/low/band pass filtering). These include (reviewed in Chatterjee et al. (2020) ):\n\nAdaptive filtering\nWavelet filtering\nEmpirical mode decomposition\nSingular spectrum analysis\nKalman filtering\nDeep learning based filtering (e.g. autoencoders)\n\n\n\nCode\n\n# Import libraries\nfrom scipy import signal\n\n# Filtering\n# High pass filter\ndef high_pass_filter(data, fs, cutoff, order=4):\n    #Design filter with Butterworth filter function\n    b, a = signal.butter(order, cutoff/(fs/2), 'highpass')\n    return signal.filtfilt(b, a, data)#Filter forward and backward\n\n# Low pass filter\ndef low_pass_filter(data, fs, cutoff, order=3):\n    #Design filter with Butterworth filter function\n    b, a = signal.butter(order, cutoff/(fs/2), 'lowpass')\n    return signal.filtfilt(b, a, data)#Filter forward and backward\n    \n# Apply filters\nfiltered_hp = high_pass_filter(data, fs, 0.5)\nfiltered_lp = low_pass_filter(data, fs, 25)\n\nWhen we filter the signal, we get:\n\nTime domain signals\n\n\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n  \n\n\n\n\n\n\nFrequency domain signals\n\n\n\n\n  \n\n\n\n\n\nWhat do we see?\nAfter filtering we see that:\n\nThe voltage offset in the filtered signal is now ~0.\nThe low frequency drift in the signal is now gone. Its stable around 0 over time.\nThe majority of the high frequency noise is gone.\nIn the frequency domain we can see that the high frequency noise is attenuated more than that in the unfiltered signal (&gt;20 Hz).\nWe can use the peak PSD in the frequency domain to roughly estimate the average heart rate from our ECG segment.\n\nWe can do more processing to clean the signal further, but this should be good enough to extract some useful features from the signal.\n\n\nDownsampling\nDownsampling is the process of reducing the sampling rate of a signal. This is done to reduce the amount of data that needs to be processed, and to make the signal easier to work with. For example, if we downsample our signal from 2000 Hz to 100 Hz, we are reducing the amount of data by a factor of 20 (2000 Hz/ 100 Hz = 20 reduction factor).\nGenerally, when downsampling follow the rules of the Nyquist–Shannon sampling theorem, which states that the sampling rate of a signal must be at least twice the highest frequency component of the signal of interest.\nFor example, if I recorded a signal that I know has a maximum frequency of 100 Hz, then I would need to sample the signal at least 200 Hz (100 Hz* 2 = 200 Hz). If I oversampled my signal, I could downsample it to about 200 Hz and still be able to reconstruct the original signal, any lower than that and I could lose information.\n\n\nCode\n\nimport scipy.signal as signal\n\nfs = 1000\nfs_down = 100\n\n\n#Calculate the downsampling factor\ndownsample_factor = fs // fs_down\n# 1000 // 100 = 10, so downsample by factor of 10\n\n\n#Downsample the signal  \ndata_ds = resample_poly(data, up=1, down=downsample_factor)\n\nBelow is an extreme example of downsampling, where we downsample from 2000 Hz to 150 Hz.\n\n\n\n  \n\n\n\n\n\nWhat do we see?\nAfter downsampling (from 2000 Hz to 150 Hz) the signal we that:\n\nThere are much less samples in the downsampled signal\nThis can speed up the processing\nThe downsampled signal still retains the characteristics (PQRST) of the original signal"
  },
  {
    "objectID": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#data-feature-extraction",
    "href": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#data-feature-extraction",
    "title": "From Heart Beats to Algorithms With ECG Data",
    "section": "Data Feature Extraction",
    "text": "Data Feature Extraction\nNow that we’ve cleaned the data, we can extract features from it.\nOne of the most common features used in ECG analysis is the heart rate, which can be extracted in a number of ways. The most common way is to use the R-peaks from the detected QRS complex.\n\nPan-Tompkins Algorithm\nThe Pan-Tompkins algorithm, developed in 1985 by Jiapu Pan and Willis J Tompkins (Pan & Tompkins, 1985) is one of the most common ways to extract the QRS-complex, get the R-peaks and then calculate the heart rate.\nThe algorithm can be broken down into the following steps:\n\nBandpass Filtering: Filter the signal between 5 and 15 Hz.\n\nLow pass filter below 15 Hz\nHigh pass filter above 5 Hz\n\nDifferentiation: Perform a first order derivative.\n\nTake the difference between the current sample and the previous sample.\n\nSquaring: Square each sample of the signal.\nMoving Window Integration: Integrate over signal in a moving window.\nThresholding: Scan the original ECG signal for peaks above a certain threshold using the integrated signal.\n\nSignal here refers to the outcome from each previous step.\nFor example, we can code the Pan-Tompkins algorithm in Python and run it on our dataset to detect QRS complexes and the R-peaks. From there we can use the R-peaks to calculate the heart rate, heart rate variability, and other interesting metrics.\n\n\nCode\n#Simplified Pan-Tompkins algorithm\n\n\nclass PanTompkinsQRS:\n    \"\"\"\n    Implementation of Pan-Tompkins QRS detection algorithm\n    \"\"\"\n    def __init__(self, sampling_rate=250):\n        self.sampling_rate = sampling_rate\n        \n\n\n    # Bandpass filter (5-15 Hz)\n    def bandpass_filter(self, signal):\n        \"\"\"\n        Bandpass filter (5-15 Hz)\n        \"\"\"\n        nyquist_freq = self.sampling_rate / 2\n        low = 5 / nyquist_freq\n        high = 15 / nyquist_freq\n        b, a = butter(1, [low, high], btype='band')\n        ecg_filt = filtfilt(b, a, signal)\n        return  ecg_filt \n    \n\n    # Differentiation\n    def differentiation(self, signal):\n        \"\"\"\n        Differentiation\n        \"\"\"\n        ecg_diff  = np.diff(signal)\n\n        #Pad with a number at the beginning\n        pad_loc = 0 #Index to add pad\n        pad_val = ecg_diff[0]\n\n        ecg_diff = np.insert(ecg_diff , pad_loc, pad_val)\n\n        return ecg_diff \n    \n    # Squaring\n    def squaring(self, signal):\n        \"\"\"\n        Squaring\n        \"\"\"\n        ecg_sqr = signal ** 2\n        return ecg_sqr\n    \n    # Moving Window Integration/ Moving average\n    def moving_average(self, signal, \n                       window_size=0.150 #In sec\n                         ):\n        \"\"\"\n        Moving average\n        Parameters:\n        - window_size: in sec\n        \"\"\"\n        window_size = int(window_size * self.sampling_rate)\n        kernel = np.ones(window_size) / window_size\n        ecg_mov_avg = np.convolve(signal, kernel, mode='same')\n        return ecg_mov_avg\n\n    #Find Peaks within the signal\n    def find_R_peaks(self, processed_signal, raw_signal):\n        \"\"\"\n        Find R-peaks using thresholding.\n        - `processed_signal`: The moving average output (used for thresholding)\n        - `raw_signal`: The original filtered ECG signal (to find the true R-wave)\n        \"\"\"\n        peak_distance = int(0.250 * self.sampling_rate)  # ~250ms between heartbeats\n        peak_prominence = 0.5 * np.max(processed_signal)  # Dynamic threshold\n\n        # 1. Detect approximate peaks in processed signal\n        peaks, _ = find_peaks(processed_signal, distance=peak_distance, prominence=peak_prominence)\n\n        # 2. Search for the true R-peak in the raw ECG signal\n        r_peaks = []\n        search_window = int(0.050 * self.sampling_rate)  # Search ±50ms around each detected peak\n        for peak in peaks:\n            search_region = raw_signal[max(0, peak - search_window):min(len(raw_signal), peak + search_window)]\n            true_r_peak = np.argmax(search_region) + (peak - search_window)  # Shift index to raw signal\n            r_peaks.append(true_r_peak)\n\n        return np.array(r_peaks)\n    \n\n    # QRS detection\n    def qrs_detection(self, signal):\n        \"\"\"\n        QRS detection\n        \"\"\"\n        signal = signal - np.mean(signal)\n        ecg_filt = self.bandpass_filter(signal)\n        ecg_diff = self.differentiation(ecg_filt)\n        ecg_sqr = self.squaring(ecg_diff)\n        ecg_mov_avg = self.moving_average(ecg_sqr)\n        ecg_r_peaks = self.find_R_peaks(ecg_mov_avg, signal)\n\n        # Return the results as a dictionary\n        return {\n            \"signal\": signal,\n            \"ecg_filt\": ecg_filt,\n            \"ecg_diff\": ecg_diff,\n            \"ecg_sqr\": ecg_sqr,\n            \"ecg_mov_avg\": ecg_mov_avg,\n            \"ecg_r_peaks\": ecg_r_peaks\n        }\n        \n\n\n\n\n\n  \n\n\n\n\n\nWhat do we see?\nFrom the plot, we can see that:\n\nThe Pan-Tompkins algorithm is able to accurately detect the R-peaks in the ECG signal.\n\n\n\nHR & HRV Extraction\nThere are several updated algorithms for automatic extraction of R-peaks and the QRS complex from the ECG signal. In Python the NeuroKit2 package contains a function for the Pan-Tompkins algorithm along with other updated algorithms.\nHere we’ll use the NeuroKit2 package to extract the R-peaks and QRS complex from the ECG signal and run some basic analysis to get HR and HRV.\n\nHR\n\n\nCode\n#====================================================\n#             Data Extraction\n#----------------------------------------------------\n# Get some data\nraw, events_df = load_data(file_in, './temp_data/')\nsfreq = fs = raw.info['sfreq']# Data sampling frequency\n\n# Extract the EEG data\nstart, stop = 5, 15 # in seconds\ndata, times = raw[32, start * sfreq :stop * sfreq ]\n\ndata = data[0]*1e3\nfs = fs_down = 250\n\n#Downsample\ndata = downsample_to_frequency(data , sfreq, fs_down)\ndata = signal.detrend(data)\ntimes = np.linspace(start,stop, num=len(data))\n#____________________________________________________\n\n# Process ecg\necg_cleaned = nk.ecg_clean(data, sampling_rate=fs)\nsignals_out_pre, info_pre = nk.ecg_process(ecg_cleaned , sampling_rate=fs)\nnk.ecg_plot(signals_out_pre, info_pre)\n\n\nfig = plt.gcf()\n# Set the figure size\nfig.set_size_inches(12, 10)  # Adjust the size as needed\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat do we see?\nFrom the plot above, we can see that:\n\nThe built-in algorithm was able to detect the majority of the R-peaks.\nWe can calculate the instantaneous heart rate between two consecutive R-peaks.\nWe can aggregate all the ECG waveforms to get a sense of the overall ECG signal.\n\n\n\nHRV\n\n\nCode\n#====================================================\n#             Data Extraction\n#----------------------------------------------------\n# Get some data\nraw, events_df = load_data(file_in, './temp_data/')\nsfreq = fs = raw.info['sfreq']# Data sampling frequency\n\n# Extract the EEG data\nstart, stop = 0, 60*10 # in seconds\ndata, times = raw[32, start * sfreq :stop * sfreq ]\n\ndata = data[0]*1e3\nfs = fs_down = 250\n\n#Downsample\ndata = downsample_to_frequency(data , sfreq, fs_down)\ndata = signal.detrend(data)\ntimes = np.linspace(start,stop, num=len(data))\n#____________________________________________________\n\n# Clean signal and Find peaks\necg_cleaned = nk.ecg_clean(data, sampling_rate=fs)\n\npeaks, info = nk.ecg_peaks(ecg_cleaned, sampling_rate=fs, correct_artifacts=True)\n\n# Compute HRV indices\nhrv_indices = nk.hrv(peaks, sampling_rate=fs, show=True)\n\nfig = plt.gcf()\n# Set the figure size\nfig.set_size_inches(12, 10)  # Adjust the size as needed  \n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat do we see?\nHere we look at a 10-minute segment of ECG data. From the plot above, we can see that:\n\nWe can aggregate all the RR intervals to get a sense of the heart rate variability (HRV) over the whole segment.\nMore or less variation in the RR intervals indicate the balance between the sympathetic and parasympathetic nervous systems.\nThe balance between the sympathetic and parasympathetic nervous systems can further be examined by looking at the balance between the high frequency(HF-orange) and low frequency (LF-green) components of spectrum of the RR intervals.\nThe Poincare plot shows the relationship between consecutive RR intervals. Within the plot we see that the long-term variability (SD2; slower fluctuations) is higher than the short-term variability (SD1; faster fluctuations).\n\n\n\n\nHR & HRV Data Summary\n\n\nFor the HR and HRV data extraction, we obtained cardiac metrics from the time domain and frequency domain.\n\nTime Domain Metrics\n\n\nMean and Median RR Interval\n\nMean and Median RR Interval\nThis is the average time between all of the RR intervals (\\(\\overline{RR}\\)) in the given data segment. We calculate it as:\n\\[\n\\text{Mean RR Interval} = \\overline{RR}  = \\frac{1}{n} \\sum_{i=1}^{n} RR_i\n\\]\n\\[\n\\text{Median RR Interval} =\n\\begin{cases}\n    RR_{ \\frac{n+1} {2}  }, & \\text{if } n \\text{ is odd} \\\\[10pt]\n    \\frac{  \\\n            RR_{ \\frac{n}{2} }  + RR_{  \\frac{n}{2} + 1}  }{2}, & \\text{if } n \\text{ is even}\n\\end{cases}\n\\]\nwhere \\(RR_i\\) is the \\(i^{th}\\) RR interval, and \\(n\\) is the number of RR intervals.\nFor our data:\n\nMean RR Interval: 1066.859 ms\nMedian RR Interval: 1072.0 ms\n\n\n\n\nStandard Deviation of RR Intervals (SDNN)\n\nStandard Deviation of RR Intervals (SDNN)\nThe standard deviation of all RR intervals provides insight into heart rate variability. We calculate it as:\n\\[\n\\text{SDNN} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (RR_i - \\overline{RR})^2}\n\\]\nwhere \\(RR_i\\) is the \\(i^{th}\\) RR interval, \\(\\overline{RR}\\) is the mean RR interval, and \\(n\\) is the number of RR intervals.\nFor our data:\n\nSDNN: 48.706 ms\n\n\n\n\nRoot Mean Square of Successive Differences (RMSSD)\n\nRoot Mean Square of Successive Differences (RMSSD)\nThis metric provides information about the variability in the time between consecutive RR intervals. We calculate it as: \\[\n\\text{RMSSD} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n-1} (RR_{i+1} - RR_i)^2}\n\\] where \\(RR_i\\) is the \\(i^{th}\\) RR interval, and \\(n\\) is the number of RR intervals.\nFor our data:\n\nRMSSD: 44.845 ms\n\n\n\n\nPercentage of RR Interval Differences &gt; 50 ms (pNN50)\n\nPercentage of RR Interval Differences &gt; 50 ms (pNN50)\nThis metric quantifies the percentage of consecutive RR intervals that differ by more than 50 ms. We calculate it as: \\[  \n\\text{NN50} = \\sum_{i=1}^{n-1} \\mathbb{1}_{|RR_i - RR_{i+1}| &gt; 50}\n\\]\n\\[\n\\text{pNN50} = \\frac{\\text{NN50}}{n-1} \\times 100%\n\\]\nwhere \\(NN50\\) is the number of consecutive RR intervals that differ by more than 50 ms.\nFor our data:\n\nPNN50: 24.242 %\n\n\n\n\nFrequency Domain Metrics\n\n\nTotal Power (TP)\n\nTotal Power (TP)\nThis metric represents the total power in the frequency domain. We calculate it as: \\[\n\\text{TP} = \\int_{0}^{0.5} P(f) \\, df\n\\] where \\(P(f)\\) is the power spectral density(PSD) of the RR intervals.\nFor our data:\n\nTP: 0.053 \\(ms^2\\)\n\n\n\n\nLow Frequency Power (LF)\n\nLow Frequency Power (LF)\nThis metric represents the power in the low frequency range (0.04-0.15 Hz). We calculate it as:\n\\[\n\\text{LF Power} = \\int_{0.04}^{0.15} P(f) \\, df\n\\]\nwhere \\(P(f)\\) is the power spectral density(PSD) of the RR intervals.\nFor our data:\n\nLF: 0.02 \\(ms^2\\)\n\n\n\n\nHigh Frequency Power (HF)\n\nHigh Frequency Power (HF)\nThis metric represents the power in the high frequency range (0.15-0.4 Hz). We calculate it as:\n\\[\n\\text{HF Power} = \\int_{0.15}^{0.40} P(f) \\, df\n\\]\nwhere \\(P(f)\\) is the power spectral density(PSD) of the RR intervals.\nFor our data:\n\nHF: 0.018 \\(ms^2\\)\n\n\n\n\nLow Frequency/High Frequency Ratio (LF/HF)\n\nLow Frequency/High Frequency Ratio (LF/HF)\nThis metric represents the ratio of high frequency power to low frequency power. We calculate it as: \\[\n\\text{LF/HF Ratio} = \\frac{LF_{power}}{HF_{power}}\n\\]\nFor our data:\n\nLF/HF: 1.066\n\n\n\nOutcomes summarized:\n\nTime Domain Metrics:\n\n\n\n\n\n\n\n\n\n\n\nMean RR Interval\nMedian RR Interval\nSDNN\nRMSSD\npNN50\n\n\n\n\n1066.86\n1072\n48.7062\n44.8448\n24.2424\n\n\n\n\nFrequency Domain Metrics:\n\n\n\n\n\n\n\n\n\n\nTotal Power\nLow Freq Power\nHigh Freq Power\nLow Freq/High Freq Ratio\n\n\n\n\n0.0527402\n0.0196355\n0.0184252\n1.06569"
  },
  {
    "objectID": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#take-away",
    "href": "projects/Short_dive_posts/ECG_analysis/ECG_proc.html#take-away",
    "title": "From Heart Beats to Algorithms With ECG Data",
    "section": "Take Away",
    "text": "Take Away\nThe electrocardiogram (ECG) has become a gold standard for monitoring cardiac and overall health, providing valuable insights beyond traditional diagnostics. We explored how ECG data can be processed to extract meaningful physiological markers such as heart rate (HR) and heart rate variability (HRV), which are key indicators used to detect arrhythmias, autonomic nervous system activity, and mental health states.\nWith advancements in machine learning and wearable technology, ECG analysis is evolving beyond conventional applications. AI-driven algorithms can enhance early disease detection, personalized health monitoring, and real-time stress and wellness assessment. The integration of ECG data with machine learning not only refines diagnostics but also opens new frontiers in preventative healthcare, digital therapeutics, and remote patient monitoring. This paves the way for continuous, intelligent ECG monitoring; empowering individuals to proactively manage their heart health and overall well-being.\n\n\nCite as\n\n\n\n@misc{Gebodh2025FromHeartBeatstoAlgorithmsWithECGData,\n  title = {From Heart Beats to Algorithms With ECG Data},\n  author = {Nigel Gebodh},\n  year = {2025},\n  url = {https://ngebodh.github.io/projects/Short_dive_posts/ECG_analysis/ECG_proc.html},\n  note = {Published: February 10, 2025}\n}"
  },
  {
    "objectID": "projects/Short_dive_posts/LLM_temp/LLM_temp.html",
    "href": "projects/Short_dive_posts/LLM_temp/LLM_temp.html",
    "title": "Why Does My LLM Have A Temperature?",
    "section": "",
    "text": "If you’ve interacted with AI assistants or LLMs (Large Language Models) in the past you may have noticed a parameter called Temperature.  Here we’ll look at how temperature affects LLM outputs, its calculation, and some examples of varying temperature values.\nYou can experiment with some small temperature changes across different LLMs here."
  },
  {
    "objectID": "projects/Short_dive_posts/LLM_temp/LLM_temp.html#the-softmax-function-and-temperature",
    "href": "projects/Short_dive_posts/LLM_temp/LLM_temp.html#the-softmax-function-and-temperature",
    "title": "Why Does My LLM Have A Temperature?",
    "section": "The Softmax Function and Temperature",
    "text": "The Softmax Function and Temperature\nThe Softmax function is a mathematical transformation that takes a vector of raw scores and converts them into a probability distribution. It does this by exponentiating each value and normalizing the results but the sum of all exponentiated values so that they sum to 1. Originally applied in physics and statistics around 1868, it was known as the Boltzmann or Gibbs distribution. The term “softmax” was coined in 1989 by John S. Bridle (Bridle, 1989, 1990).\nIn Natural Language Processing (NLP), the Softmax function is typically applied to the logits generated by an LLM to produce a probability distribution over possible next tokens. This distribution represents the likelihood of each token being the next word or subword in the sequence.\n The Softmax function is defined as:\n\\[\n\\mathbf{Softmax}(x_i) = \\frac{ \\textcolor{None}{  e^{x_i} } }{ \\textcolor{None}{ \\sum_{j=1}^{N} e^{x_j}  } }\n\\]\nWhere:\n\n\\(x_i\\) value of each input (the logit value)\n\\(e^{x_i}\\) is the exponentiation of the value of each input (Euler’s number: \\(e\\) ~ \\(2.71828\\))\n\\(\\sum_{j=1}^{N} e^{x_j}\\) is the sum of all the exponentiated (\\(e\\)) inputs (\\(x\\))\n\nThe Temperature (T) parameters is a simple modification to the Softmax function that adjusts the inputs:\n\\[\n\\mathbf{Softmax}(x_i) = \\frac{ \\textcolor{None}{  e^{  \\frac{x_i}{ \\textcolor{red}{T} }     }}}\n                             { \\textcolor{None}{ \\sum_{j=1}^{N} e^{   \\frac{x_j}{ \\textcolor{red}{T} }    }  } }\n\\]\nThe term “Temperature” is borrowed from the field of physics. It comes from its relationship to the Boltzmann distribution, which describes how energy states change with temperature. Early usage of the term “Temperature” in machine learning came from Ackley et al. (1985).\n\n\nExamples\n\nExample 1 - Simple Softmax Transformation without Temperature\n\n\nExample\n\nGiven a list of numbers calculate their softmax probabilities. \n\n\\(list = [2.0, 4.0, 3.0]\\)\n\n\nBy Hand:\n\n\n\\[\\begin{align}\nx &= [2.0, 4.0, 3.0] \\\\ \\\\ e^x &= [e^{2.0}, e^{4.0}, e^{3.0}] \\\\ \\\\ \\sum_{i=1}^{n = 3}  e^x &= e^{2.0} + e^{4.0} + e^{3.0} \\\\ \\\\ \\mathbf{\\text{Softmax}(x)} &= \\left[\\frac{e^{2.0}}{e^{2.0} + e^{4.0} + e^{3.0}}, \\frac{e^{4.0}}{e^{2.0} + e^{4.0} + e^{3.0}}, \\frac{e^{3.0}}{e^{2.0} + e^{4.0} + e^{3.0}}\\right] \\\\ \\\\ &= [0.0900, 0.6652, 0.2447]\\\\\\end{align}\\]\n\n\nUsing Python:\n\n\nCode\n#Calculating Softmax\nimport torch\nimport torch.nn.functional as F\n\n#1) Using Our Function\n#Define a softmax function\ndef my_softmax(input_vector):\n    e = np.exp(input_vector)\n    return e / e.sum()\n\nlist_in = [2.0, 4.0, 3.0]\noutput = my_softmax(list_in)\n\nprint(f\"\\nThe softmax probabilities are: \\n {output}\")\n\n#2) Using PyTorch Function\n#Convert list to torch tensor\nlist_in_torch = torch.tensor(list_in)\noutput = F.softmax(list_in_torch, dim=0)\n\nprint(f\"\\nThe softmax probabilities (using Pytorch) are: \\n {output}\")\n\n\n\nThe softmax probabilities are: \n [0.09003057 0.66524096 0.24472847]\n\nThe softmax probabilities (using Pytorch) are: \n tensor([0.0900, 0.6652, 0.2447])\n\n\n\n\n\n\nExample 2 - Simple Softmax Transformation with Varying Temperatures\n\n\nExample\n\nGiven a list of numbers calculate their Softmax probabilities at temperatures of T=[0.5, 1, 2] \n\n\n\\(list = [2.0, 4.0, 3.0]\\)\n\n\n\n\nTemperature: 0.5\n\nBy Hand:\n\n\n\\[\\begin{align}\n\\text{Given:} \\\\ x &= [2.0, 4.0, 3.0] \\\\ \\color{red}T &= 0.5 \\\\ \\\\ \\\\ \\text{Softmax}(x_{i=1:3},{\\color{red}T}) &= \\frac{\\color{steelblue}{       e^{\\frac{x_i}{ \\color{red}{T} }} }      } {\\color{purple}{         \\sum_{j=1}^{N}{ e^{\\frac{x_j}{  \\color{red}{T}  }}           }   } } \\\\ \\\\ \\\\ \\text{The numerator:} \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{x_1/\\color{red}T}}, \\color{steelblue}{e^{x_2/\\color{red}T}}, \\color{steelblue}{e^{x_3/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{2.0/\\color{red}T}}, \\color{steelblue}{e^{4.0/\\color{red}T}}, \\color{steelblue}{e^{3.0/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}0.5} &= \\left[\\color{steelblue}{e^{2.0/\\color{red}0.5}}, \\color{steelblue}{e^{4.0/\\color{red}0.5}}, \\color{steelblue}{e^{3.0/\\color{red}0.5}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}0.5} &= \\left[\\color{steelblue}{e^{{4}}}, \\color{steelblue}{e^{{8}}}, \\color{steelblue}{e^{{6}}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}0.5} &= \\left[\\color{steelblue}{54.6}, \\color{steelblue}{2.98e+03}, \\color{steelblue}{403}\\color{None}\\right] \\\\ \\\\ \\text{The denominator:} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}T} &= \\color{purple}{e^{x_1/\\color{red}T}} + \\color{purple}{e^{x_2/\\color{red}T}} + \\color{purple}{e^{x_3/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}T} &= \\color{purple}{e^{2.0/\\color{red}T}} + \\color{purple}{e^{4.0/\\color{red}T}} + \\color{purple}{e^{3.0/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}0.5} &= \\color{purple}{e^{2.0/\\color{red}0.5}} + \\color{purple}{e^{4.0/\\color{red}0.5}} + \\color{purple}{e^{3.0/\\color{red}0.5}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}0.5} &= \\color{purple}{e^{{4}}} + \\color{purple}{e^{{8}}} + \\color{purple}{e^{{6}}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}0.5} &= \\color{purple}{54.6} + \\color{purple}{2.98e+03} + \\color{purple}{403} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}0.5} &= \\color{purple}3.44e+03 \\\\ \\\\ \\\\ \\text{Combine all:} \\\\ \\text{Softmax}(x_{i=1:3},{\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{x_1/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_2/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_3/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }\\right] \\\\\\\\ \\text{Softmax}(x, {\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{2.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }, \\frac{\\color{steelblue}e^{4.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }, \\frac{\\color{steelblue}e^{3.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}0.5}) &= \\left[\\frac{  \\color{steelblue} e^{2.0/\\color{red}0.5}} {  \\color{purple} e^{2.0/\\color{red}0.5} + e^{4.0/\\color{red}0.5} + e^{3.0/\\color{red}0.5}    }, \\frac{  \\color{steelblue} e^{4.0/\\color{red}0.5}} {  \\color{purple} e^{2.0/\\color{red}0.5} + e^{4.0/\\color{red}0.5} + e^{3.0/\\color{red}0.5}    }, \\frac{  \\color{steelblue} e^{3.0/\\color{red}0.5}} {  \\color{purple} e^{2.0/\\color{red}0.5} + e^{4.0/\\color{red}0.5} + e^{3.0/\\color{red}0.5}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}0.5}) &= \\left[\\frac{  \\color{steelblue}  e^{ 4.0 }} {  \\color{purple} e^{4.0 } + e^{8.0 } + e^{6.0 }    }, \\frac{  \\color{steelblue}  e^{ 8.0 }} {  \\color{purple} e^{4.0 } + e^{8.0 } + e^{6.0 }    }, \\frac{  \\color{steelblue}  e^{ 6.0 }} {  \\color{purple} e^{4.0 } + e^{8.0 } + e^{6.0 }    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}0.5}) &= \\left[\\frac{ \\color{steelblue}  { 54.6 } } {  \\color{purple} 54.6 + 2.98e+03 + 403    }, \\frac{ \\color{steelblue}  { 2.98e+03 } } {  \\color{purple} 54.6 + 2.98e+03 + 403    }, \\frac{ \\color{steelblue}  { 403 } } {  \\color{purple} 54.6 + 2.98e+03 + 403    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}0.5}) &= \\left[\\frac{ \\color{steelblue}{ 54.6 } } {  \\color{purple} 3.44e+03    }, \\frac{ \\color{steelblue}{ 2.98e+03 } } {  \\color{purple} 3.44e+03    }, \\frac{ \\color{steelblue}{ 403 } } {  \\color{purple} 3.44e+03    }\\right] \\\\ \\\\ \\\\ \\text{Softmax}(x, {\\color{red}0.5})&= [0.0159, 0.867, 0.117] \\ \\\\ \\text{Probabilities} &= [0.0159, 0.867, 0.117] \\ \\\\\\end{align}\\]\n\n\nUsing Python:\n\n\nCode\n#Calculating Softmax\nimport torch\nimport torch.nn.functional as F\n\n\n#1) Using Our Function\n#Define a softmax function with temperature\ndef my_softmax(input_vector, Temp=1.0):\n    e = np.exp(np.divide(input_vector,Temp))\n    return e / e.sum()\n\nTemp =0.5\nlist_in = [2.0, 4.0, 3.0]\noutput = my_softmax(list_in, Temp=Temp)\n\nprint(f\"\\nThe softmax probabilities at a Temp = {Temp} are: \\n {output}\")\n\n\n\n#2) Using PyTorch Function\n#Convert list to torch tensor\nlist_in_torch = torch.tensor(list_in)\n\n# Apply temperature scaling \nscaled_logits = list_in_torch / Temp\noutput = F.softmax(scaled_logits, dim=0)\n\nprint(f\"\\nThe softmax probabilities (using Pytorch) are: \\n {output}\")\n\n\n\nThe softmax probabilities at a Temp = 0.5 are: \n [0.01587624 0.86681333 0.11731043]\n\nThe softmax probabilities (using Pytorch) are: \n tensor([0.0159, 0.8668, 0.1173])\n\n\n\n\n\nTemperature: 1.0\n\nBy Hand:\n\n\n\\[\\begin{align}\n\\text{Given:} \\\\ x &= [2.0, 4.0, 3.0] \\\\ \\color{red}T &= 1.0 \\\\ \\\\ \\\\ \\text{Softmax}(x_{i=1:3},{\\color{red}T}) &= \\frac{\\color{steelblue}{       e^{\\frac{x_i}{ \\color{red}{T} }} }      } {\\color{purple}{         \\sum_{j=1}^{N}{ e^{\\frac{x_j}{  \\color{red}{T}  }}           }   } } \\\\ \\\\ \\\\ \\text{The numerator:} \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{x_1/\\color{red}T}}, \\color{steelblue}{e^{x_2/\\color{red}T}}, \\color{steelblue}{e^{x_3/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{2.0/\\color{red}T}}, \\color{steelblue}{e^{4.0/\\color{red}T}}, \\color{steelblue}{e^{3.0/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}1.0} &= \\left[\\color{steelblue}{e^{2.0/\\color{red}1.0}}, \\color{steelblue}{e^{4.0/\\color{red}1.0}}, \\color{steelblue}{e^{3.0/\\color{red}1.0}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}1.0} &= \\left[\\color{steelblue}{e^{{2}}}, \\color{steelblue}{e^{{4}}}, \\color{steelblue}{e^{{3}}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}1.0} &= \\left[\\color{steelblue}{7.39}, \\color{steelblue}{54.6}, \\color{steelblue}{20.1}\\color{None}\\right] \\\\ \\\\ \\text{The denominator:} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}T} &= \\color{purple}{e^{x_1/\\color{red}T}} + \\color{purple}{e^{x_2/\\color{red}T}} + \\color{purple}{e^{x_3/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}T} &= \\color{purple}{e^{2.0/\\color{red}T}} + \\color{purple}{e^{4.0/\\color{red}T}} + \\color{purple}{e^{3.0/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}1.0} &= \\color{purple}{e^{2.0/\\color{red}1.0}} + \\color{purple}{e^{4.0/\\color{red}1.0}} + \\color{purple}{e^{3.0/\\color{red}1.0}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}1.0} &= \\color{purple}{e^{{2}}} + \\color{purple}{e^{{4}}} + \\color{purple}{e^{{3}}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}1.0} &= \\color{purple}{7.39} + \\color{purple}{54.6} + \\color{purple}{20.1} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}1.0} &= \\color{purple}82.1 \\\\ \\\\ \\\\ \\text{Combine all:} \\\\ \\text{Softmax}(x_{i=1:3},{\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{x_1/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_2/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_3/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }\\right] \\\\\\\\ \\text{Softmax}(x, {\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{2.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }, \\frac{\\color{steelblue}e^{4.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }, \\frac{\\color{steelblue}e^{3.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{  \\color{steelblue} e^{2.0/\\color{red}1.0}} {  \\color{purple} e^{2.0/\\color{red}1.0} + e^{4.0/\\color{red}1.0} + e^{3.0/\\color{red}1.0}    }, \\frac{  \\color{steelblue} e^{4.0/\\color{red}1.0}} {  \\color{purple} e^{2.0/\\color{red}1.0} + e^{4.0/\\color{red}1.0} + e^{3.0/\\color{red}1.0}    }, \\frac{  \\color{steelblue} e^{3.0/\\color{red}1.0}} {  \\color{purple} e^{2.0/\\color{red}1.0} + e^{4.0/\\color{red}1.0} + e^{3.0/\\color{red}1.0}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{  \\color{steelblue}  e^{ 2.0 }} {  \\color{purple} e^{2.0 } + e^{4.0 } + e^{3.0 }    }, \\frac{  \\color{steelblue}  e^{ 4.0 }} {  \\color{purple} e^{2.0 } + e^{4.0 } + e^{3.0 }    }, \\frac{  \\color{steelblue}  e^{ 3.0 }} {  \\color{purple} e^{2.0 } + e^{4.0 } + e^{3.0 }    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{ \\color{steelblue}  { 7.39 } } {  \\color{purple} 7.39 + 54.6 + 20.1    }, \\frac{ \\color{steelblue}  { 54.6 } } {  \\color{purple} 7.39 + 54.6 + 20.1    }, \\frac{ \\color{steelblue}  { 20.1 } } {  \\color{purple} 7.39 + 54.6 + 20.1    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{ \\color{steelblue}{ 7.39 } } {  \\color{purple} 82.1    }, \\frac{ \\color{steelblue}{ 54.6 } } {  \\color{purple} 82.1    }, \\frac{ \\color{steelblue}{ 20.1 } } {  \\color{purple} 82.1    }\\right] \\\\ \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0})&= [0.09, 0.665, 0.245] \\ \\\\ \\text{Probabilities} &= [0.09, 0.665, 0.245] \\ \\\\\\end{align}\\]\n\n\nUsing Python:\n\n\nCode\n#Calculating Softmax\nimport torch\nimport torch.nn.functional as F\n\n\n#1) Using Our Function\n#Define a softmax function with temperature\ndef my_softmax(input_vector, Temp=1.0):\n    e = np.exp(np.divide(input_vector,Temp))\n    return e / e.sum()\n\nTemp =1.0\nlist_in = [2.0, 4.0, 3.0]\noutput = my_softmax(list_in, Temp=Temp)\n\nprint(f\"\\nThe softmax probabilities at a Temp = {Temp} are: \\n {output}\")\n\n\n#2) Using PyTorch Function\n#Convert list to torch tensor\nlist_in_torch = torch.tensor(list_in)\n\n# Apply temperature scaling \nscaled_logits = list_in_torch / Temp\noutput = F.softmax(scaled_logits, dim=0)\n\nprint(f\"\\nThe softmax probabilities (using Pytorch) are: \\n {output}\")\n\n\n\nThe softmax probabilities at a Temp = 1.0 are: \n [0.09003057 0.66524096 0.24472847]\n\nThe softmax probabilities (using Pytorch) are: \n tensor([0.0900, 0.6652, 0.2447])\n\n\n\n\n\nTemperature: 2.0\n\nBy Hand:\n\n\n\\[\\begin{align}\n\\text{Given:} \\\\ x &= [2.0, 4.0, 3.0] \\\\ \\color{red}T &= 2.0 \\\\ \\\\ \\\\ \\text{Softmax}(x_{i=1:3},{\\color{red}T}) &= \\frac{\\color{steelblue}{       e^{\\frac{x_i}{ \\color{red}{T} }} }      } {\\color{purple}{         \\sum_{j=1}^{N}{ e^{\\frac{x_j}{  \\color{red}{T}  }}           }   } } \\\\ \\\\ \\\\ \\text{The numerator:} \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{x_1/\\color{red}T}}, \\color{steelblue}{e^{x_2/\\color{red}T}}, \\color{steelblue}{e^{x_3/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{2.0/\\color{red}T}}, \\color{steelblue}{e^{4.0/\\color{red}T}}, \\color{steelblue}{e^{3.0/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}2.0} &= \\left[\\color{steelblue}{e^{2.0/\\color{red}2.0}}, \\color{steelblue}{e^{4.0/\\color{red}2.0}}, \\color{steelblue}{e^{3.0/\\color{red}2.0}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}2.0} &= \\left[\\color{steelblue}{e^{{1}}}, \\color{steelblue}{e^{{2}}}, \\color{steelblue}{e^{{1.5}}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}2.0} &= \\left[\\color{steelblue}{2.72}, \\color{steelblue}{7.39}, \\color{steelblue}{4.48}\\color{None}\\right] \\\\ \\\\ \\text{The denominator:} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}T} &= \\color{purple}{e^{x_1/\\color{red}T}} + \\color{purple}{e^{x_2/\\color{red}T}} + \\color{purple}{e^{x_3/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}T} &= \\color{purple}{e^{2.0/\\color{red}T}} + \\color{purple}{e^{4.0/\\color{red}T}} + \\color{purple}{e^{3.0/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}2.0} &= \\color{purple}{e^{2.0/\\color{red}2.0}} + \\color{purple}{e^{4.0/\\color{red}2.0}} + \\color{purple}{e^{3.0/\\color{red}2.0}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}2.0} &= \\color{purple}{e^{{1}}} + \\color{purple}{e^{{2}}} + \\color{purple}{e^{{1.5}}} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}2.0} &= \\color{purple}{2.72} + \\color{purple}{7.39} + \\color{purple}{4.48} \\\\ \\color{purple}\\sum_{j=1}^{N=3} e^{x_j/\\color{red}2.0} &= \\color{purple}14.6 \\\\ \\\\ \\\\ \\text{Combine all:} \\\\ \\text{Softmax}(x_{i=1:3},{\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{x_1/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_2/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_3/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T}     }\\right] \\\\\\\\ \\text{Softmax}(x, {\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{2.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }, \\frac{\\color{steelblue}e^{4.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }, \\frac{\\color{steelblue}e^{3.0/\\color{red}T}} {  \\color{purple} e^{2.0/\\color{red}T} + e^{4.0/\\color{red}T} + e^{3.0/\\color{red}T}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}2.0}) &= \\left[\\frac{  \\color{steelblue} e^{2.0/\\color{red}2.0}} {  \\color{purple} e^{2.0/\\color{red}2.0} + e^{4.0/\\color{red}2.0} + e^{3.0/\\color{red}2.0}    }, \\frac{  \\color{steelblue} e^{4.0/\\color{red}2.0}} {  \\color{purple} e^{2.0/\\color{red}2.0} + e^{4.0/\\color{red}2.0} + e^{3.0/\\color{red}2.0}    }, \\frac{  \\color{steelblue} e^{3.0/\\color{red}2.0}} {  \\color{purple} e^{2.0/\\color{red}2.0} + e^{4.0/\\color{red}2.0} + e^{3.0/\\color{red}2.0}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}2.0}) &= \\left[\\frac{  \\color{steelblue}  e^{ 1.0 }} {  \\color{purple} e^{1.0 } + e^{2.0 } + e^{1.5 }    }, \\frac{  \\color{steelblue}  e^{ 2.0 }} {  \\color{purple} e^{1.0 } + e^{2.0 } + e^{1.5 }    }, \\frac{  \\color{steelblue}  e^{ 1.5 }} {  \\color{purple} e^{1.0 } + e^{2.0 } + e^{1.5 }    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}2.0}) &= \\left[\\frac{ \\color{steelblue}  { 2.72 } } {  \\color{purple} 2.72 + 7.39 + 4.48    }, \\frac{ \\color{steelblue}  { 7.39 } } {  \\color{purple} 2.72 + 7.39 + 4.48    }, \\frac{ \\color{steelblue}  { 4.48 } } {  \\color{purple} 2.72 + 7.39 + 4.48    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}2.0}) &= \\left[\\frac{ \\color{steelblue}{ 2.72 } } {  \\color{purple} 14.6    }, \\frac{ \\color{steelblue}{ 7.39 } } {  \\color{purple} 14.6    }, \\frac{ \\color{steelblue}{ 4.48 } } {  \\color{purple} 14.6    }\\right] \\\\ \\\\ \\\\ \\text{Softmax}(x, {\\color{red}2.0})&= [0.186, 0.506, 0.307] \\ \\\\ \\text{Probabilities} &= [0.186, 0.506, 0.307] \\ \\\\\\end{align}\\]\n\n\nUsing Python:\n\n\nCode\n#Calculating Softmax\nimport torch\nimport torch.nn.functional as F\n\n\n#1) Using Our Function\n#Define a softmax function with temperature\ndef my_softmax(input_vector, Temp=1.0):\n    e = np.exp(np.divide(input_vector,Temp))\n    return e / e.sum()\n\nTemp =2.0\nlist_in = [2.0, 4.0, 3.0]\noutput = my_softmax(list_in, Temp=Temp)\n\nprint(f\"\\nThe softmax probabilities at a Temp = {Temp} are: \\n {output}\")\n\n\n#2) Using PyTorch Function\n#Convert list to torch tensor\nlist_in_torch = torch.tensor(list_in)\n\n# Apply temperature scaling \nscaled_logits = list_in_torch / Temp\noutput = F.softmax(scaled_logits, dim=0)\n\nprint(f\"\\nThe softmax probabilities (using Pytorch) are: \\n {output}\")\n\n\n\nThe softmax probabilities at a Temp = 2.0 are: \n [0.18632372 0.50648039 0.30719589]\n\nThe softmax probabilities (using Pytorch) are: \n tensor([0.1863, 0.5065, 0.3072])\n\n\n\n\n\n\n\nExample 3 - LLM Output Softmax Transformation with Varying Temperatures\n\n\nExample\n\nGiven a list of logit outputs from an LLM, find the most probable word and its probability.  Assume the LLM only knows 5 words (LLM vocabularies typically contain thousands of words).  Calculate the probabilities for temperatures of 1.0 and 100.0\n\n\\(index = [0, 1, 2, 3, 4]\\)\n\\(words = [ceiling, floor, mat, car, grass]\\)\n\\(logits = [-49.82, -46.40, -45.25, -47.30, -48.32]\\)\n\n\n\n\nTemperature: 1.0\n\nBy Hand:\n\n\n\\[\\begin{align}\n\\text{Given:} \\\\ x &= [-49.82, -46.4, -45.25, -47.3, -48.32] \\\\ \\color{red}T &= 1.0 \\\\ \\\\ \\\\ \\text{Softmax}(x_{i=1:5},{\\color{red}T}) &= \\frac{\\color{steelblue}{       e^{\\frac{x_i}{ \\color{red}{T} }} }      } {\\color{purple}{         \\sum_{j=1}^{N}{ e^{\\frac{x_j}{  \\color{red}{T}  }}           }   } } \\\\ \\\\ \\\\ \\text{The numerator:} \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{x_1/\\color{red}T}}, \\color{steelblue}{e^{x_2/\\color{red}T}}, \\color{steelblue}{e^{x_3/\\color{red}T}}, \\color{steelblue}{e^{x_4/\\color{red}T}}, \\color{steelblue}{e^{x_5/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{-49.82/\\color{red}T}}, \\color{steelblue}{e^{-46.4/\\color{red}T}}, \\color{steelblue}{e^{-45.25/\\color{red}T}}, \\color{steelblue}{e^{-47.3/\\color{red}T}}, \\color{steelblue}{e^{-48.32/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}1.0} &= \\left[\\color{steelblue}{e^{-49.82/\\color{red}1.0}}, \\color{steelblue}{e^{-46.4/\\color{red}1.0}}, \\color{steelblue}{e^{-45.25/\\color{red}1.0}}, \\color{steelblue}{e^{-47.3/\\color{red}1.0}}, \\color{steelblue}{e^{-48.32/\\color{red}1.0}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}1.0} &= \\left[\\color{steelblue}{e^{{-49.82}}}, \\color{steelblue}{e^{{-46.4}}}, \\color{steelblue}{e^{{-45.25}}}, \\color{steelblue}{e^{{-47.3}}}, \\color{steelblue}{e^{{-48.32}}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}1.0} &= \\left[\\color{steelblue}{2.309e-22}, \\color{steelblue}{7.059e-21}, \\color{steelblue}{2.229e-20}, \\color{steelblue}{2.87e-21}, \\color{steelblue}{1.035e-21}\\color{None}\\right] \\\\ \\\\ \\text{The denominator:} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}T} &= \\color{purple}{e^{x_1/\\color{red}T}} + \\color{purple}{e^{x_2/\\color{red}T}} + \\color{purple}{e^{x_3/\\color{red}T}} + \\color{purple}{e^{x_4/\\color{red}T}} + \\color{purple}{e^{x_5/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}T} &= \\color{purple}{e^{-49.82/\\color{red}T}} + \\color{purple}{e^{-46.4/\\color{red}T}} + \\color{purple}{e^{-45.25/\\color{red}T}} + \\color{purple}{e^{-47.3/\\color{red}T}} + \\color{purple}{e^{-48.32/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}1.0} &= \\color{purple}{e^{-49.82/\\color{red}1.0}} + \\color{purple}{e^{-46.4/\\color{red}1.0}} + \\color{purple}{e^{-45.25/\\color{red}1.0}} + \\color{purple}{e^{-47.3/\\color{red}1.0}} + \\color{purple}{e^{-48.32/\\color{red}1.0}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}1.0} &= \\color{purple}{e^{{-49.82}}} + \\color{purple}{e^{{-46.4}}} + \\color{purple}{e^{{-45.25}}} + \\color{purple}{e^{{-47.3}}} + \\color{purple}{e^{{-48.32}}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}1.0} &= \\color{purple}{2.309e-22} + \\color{purple}{7.059e-21} + \\color{purple}{2.229e-20} + \\color{purple}{2.87e-21} + \\color{purple}{1.035e-21} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}1.0} &= \\color{purple}3.349e-20 \\\\ \\\\ \\\\ \\text{Combine all:} \\\\ \\text{Softmax}(x_{i=1:5},{\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{x_1/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_2/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_3/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_4/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_5/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }\\right] \\\\\\\\ \\text{Softmax}(x, {\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{-49.82/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-46.4/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-45.25/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-47.3/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-48.32/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{  \\color{steelblue} e^{-49.82/\\color{red}1.0}} {  \\color{purple} e^{-49.82/\\color{red}1.0} + e^{-46.4/\\color{red}1.0} + e^{-45.25/\\color{red}1.0} + e^{-47.3/\\color{red}1.0} + e^{-48.32/\\color{red}1.0}    }, \\frac{  \\color{steelblue} e^{-46.4/\\color{red}1.0}} {  \\color{purple} e^{-49.82/\\color{red}1.0} + e^{-46.4/\\color{red}1.0} + e^{-45.25/\\color{red}1.0} + e^{-47.3/\\color{red}1.0} + e^{-48.32/\\color{red}1.0}    }, \\frac{  \\color{steelblue} e^{-45.25/\\color{red}1.0}} {  \\color{purple} e^{-49.82/\\color{red}1.0} + e^{-46.4/\\color{red}1.0} + e^{-45.25/\\color{red}1.0} + e^{-47.3/\\color{red}1.0} + e^{-48.32/\\color{red}1.0}    }, \\frac{  \\color{steelblue} e^{-47.3/\\color{red}1.0}} {  \\color{purple} e^{-49.82/\\color{red}1.0} + e^{-46.4/\\color{red}1.0} + e^{-45.25/\\color{red}1.0} + e^{-47.3/\\color{red}1.0} + e^{-48.32/\\color{red}1.0}    }, \\frac{  \\color{steelblue} e^{-48.32/\\color{red}1.0}} {  \\color{purple} e^{-49.82/\\color{red}1.0} + e^{-46.4/\\color{red}1.0} + e^{-45.25/\\color{red}1.0} + e^{-47.3/\\color{red}1.0} + e^{-48.32/\\color{red}1.0}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{  \\color{steelblue}  e^{ -49.82 }} {  \\color{purple} e^{-49.82 } + e^{-46.4 } + e^{-45.25 } + e^{-47.3 } + e^{-48.32 }    }, \\frac{  \\color{steelblue}  e^{ -46.4 }} {  \\color{purple} e^{-49.82 } + e^{-46.4 } + e^{-45.25 } + e^{-47.3 } + e^{-48.32 }    }, \\frac{  \\color{steelblue}  e^{ -45.25 }} {  \\color{purple} e^{-49.82 } + e^{-46.4 } + e^{-45.25 } + e^{-47.3 } + e^{-48.32 }    }, \\frac{  \\color{steelblue}  e^{ -47.3 }} {  \\color{purple} e^{-49.82 } + e^{-46.4 } + e^{-45.25 } + e^{-47.3 } + e^{-48.32 }    }, \\frac{  \\color{steelblue}  e^{ -48.32 }} {  \\color{purple} e^{-49.82 } + e^{-46.4 } + e^{-45.25 } + e^{-47.3 } + e^{-48.32 }    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{ \\color{steelblue}  { 2.309e-22 } } {  \\color{purple} 2.309e-22 + 7.059e-21 + 2.229e-20 + 2.87e-21 + 1.035e-21    }, \\frac{ \\color{steelblue}  { 7.059e-21 } } {  \\color{purple} 2.309e-22 + 7.059e-21 + 2.229e-20 + 2.87e-21 + 1.035e-21    }, \\frac{ \\color{steelblue}  { 2.229e-20 } } {  \\color{purple} 2.309e-22 + 7.059e-21 + 2.229e-20 + 2.87e-21 + 1.035e-21    }, \\frac{ \\color{steelblue}  { 2.87e-21 } } {  \\color{purple} 2.309e-22 + 7.059e-21 + 2.229e-20 + 2.87e-21 + 1.035e-21    }, \\frac{ \\color{steelblue}  { 1.035e-21 } } {  \\color{purple} 2.309e-22 + 7.059e-21 + 2.229e-20 + 2.87e-21 + 1.035e-21    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0}) &= \\left[\\frac{ \\color{steelblue}{ 2.309e-22 } } {  \\color{purple} 3.349e-20    }, \\frac{ \\color{steelblue}{ 7.059e-21 } } {  \\color{purple} 3.349e-20    }, \\frac{ \\color{steelblue}{ 2.229e-20 } } {  \\color{purple} 3.349e-20    }, \\frac{ \\color{steelblue}{ 2.87e-21 } } {  \\color{purple} 3.349e-20    }, \\frac{ \\color{steelblue}{ 1.035e-21 } } {  \\color{purple} 3.349e-20    }\\right] \\\\ \\\\ \\\\ \\text{Softmax}(x, {\\color{red}1.0})&= [0.006895, 0.2108, 0.6657, 0.0857, 0.0309] \\ \\\\ \\text{Probabilities} &= [0.006895, 0.2108, 0.6657, 0.0857, 0.0309] \\ \\\\\\end{align}\\]\n\n\nUsing Python:\n\n\nCode\n# Example Softmax Calculation\n\n# Assume for simplicity:\n# * The model only knows the 5 words listed below (it has a vocabulary of 5).\n\nimport pandas as pd\nimport seaborn as sns\n\n#Example model output\nmodel_output_vals = {\"word_index\":[i for i in range(5)],\n                \"words\":[\"ceiling\", \"floor\", \"mat\", \"car\", \"grass\"], \n                \"logits\":[-49.82, -46.40, -45.25, -47.30, -48.32]}\ntemp = 1.0\n#Convert the data to a DataFrame\nmodel_output = pd.DataFrame(model_output_vals)\n\n#Define a softmax function with temperature\ndef my_softmax(input_vector, Temp=1.0):\n    e = np.exp(np.divide(input_vector,Temp))\n    return e / e.sum()\n\n#Calculate the probabilities\nprobs =  my_softmax(model_output[\"logits\"], Temp=temp)\nmodel_output[\"softmax_prob\"] = probs \n\n#Select the most probable word\nmost_prob = np.argmax(probs)\n\nprint(f\"\\nThe index of the most probable word is: {most_prob}\")\n\n#Pull out the most probable word\nprint(f\"\\nThe most probable word is: { model_output['words'][most_prob] }\" \\\n      f\" (Prob: {model_output['softmax_prob'][most_prob]:.5f})\")\n\n\n#Style our table\ncm = sns.light_palette(\"orange\", as_cmap=True)\ns1 = model_output\ns1 = s1.style.background_gradient(subset=[\"logits\"],cmap=cm)\n\ncm = sns.light_palette(\"green\", as_cmap=True)\ns1.background_gradient(subset=[\"softmax_prob\"],cmap=cm)\n\n\n\nThe index of the most probable word is: 2\n\nThe most probable word is: mat (Prob: 0.66571)\n\n\n\n\n\n\n\n \nword_index\nwords\nlogits\nsoftmax_prob\n\n\n\n\n0\n0\nceiling\n-49.820000\n0.006895\n\n\n1\n1\nfloor\n-46.400000\n0.210789\n\n\n2\n2\nmat\n-45.250000\n0.665712\n\n\n3\n3\ncar\n-47.300000\n0.085700\n\n\n4\n4\ngrass\n-48.320000\n0.030903\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the softmax probabilities we see that the most probable word is: mat with a probability of : 0.666\n\n\n\n\n\nTemperature: 100.0\n\nBy Hand:\n\n\n\\[\\begin{align}\n\\text{Given:} \\\\ x &= [-49.82, -46.4, -45.25, -47.3, -48.32] \\\\ \\color{red}T &= 100.0 \\\\ \\\\ \\\\ \\text{Softmax}(x_{i=1:5},{\\color{red}T}) &= \\frac{\\color{steelblue}{       e^{\\frac{x_i}{ \\color{red}{T} }} }      } {\\color{purple}{         \\sum_{j=1}^{N}{ e^{\\frac{x_j}{  \\color{red}{T}  }}           }   } } \\\\ \\\\ \\\\ \\text{The numerator:} \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{x_1/\\color{red}T}}, \\color{steelblue}{e^{x_2/\\color{red}T}}, \\color{steelblue}{e^{x_3/\\color{red}T}}, \\color{steelblue}{e^{x_4/\\color{red}T}}, \\color{steelblue}{e^{x_5/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}T} &= \\left[\\color{steelblue}{e^{-49.82/\\color{red}T}}, \\color{steelblue}{e^{-46.4/\\color{red}T}}, \\color{steelblue}{e^{-45.25/\\color{red}T}}, \\color{steelblue}{e^{-47.3/\\color{red}T}}, \\color{steelblue}{e^{-48.32/\\color{red}T}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}100.0} &= \\left[\\color{steelblue}{e^{-49.82/\\color{red}100.0}}, \\color{steelblue}{e^{-46.4/\\color{red}100.0}}, \\color{steelblue}{e^{-45.25/\\color{red}100.0}}, \\color{steelblue}{e^{-47.3/\\color{red}100.0}}, \\color{steelblue}{e^{-48.32/\\color{red}100.0}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}100.0} &= \\left[\\color{steelblue}{e^{{-0.4982}}}, \\color{steelblue}{e^{{-0.464}}}, \\color{steelblue}{e^{{-0.4525}}}, \\color{steelblue}{e^{{-0.473}}}, \\color{steelblue}{e^{{-0.4832}}}\\color{None}\\right] \\\\ \\color{steelblue}e^{x_i/\\color{red}100.0} &= \\left[\\color{steelblue}{0.6076}, \\color{steelblue}{0.6288}, \\color{steelblue}{0.636}, \\color{steelblue}{0.6231}, \\color{steelblue}{0.6168}\\color{None}\\right] \\\\ \\\\ \\text{The denominator:} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}T} &= \\color{purple}{e^{x_1/\\color{red}T}} + \\color{purple}{e^{x_2/\\color{red}T}} + \\color{purple}{e^{x_3/\\color{red}T}} + \\color{purple}{e^{x_4/\\color{red}T}} + \\color{purple}{e^{x_5/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}T} &= \\color{purple}{e^{-49.82/\\color{red}T}} + \\color{purple}{e^{-46.4/\\color{red}T}} + \\color{purple}{e^{-45.25/\\color{red}T}} + \\color{purple}{e^{-47.3/\\color{red}T}} + \\color{purple}{e^{-48.32/\\color{red}T}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}100.0} &= \\color{purple}{e^{-49.82/\\color{red}100.0}} + \\color{purple}{e^{-46.4/\\color{red}100.0}} + \\color{purple}{e^{-45.25/\\color{red}100.0}} + \\color{purple}{e^{-47.3/\\color{red}100.0}} + \\color{purple}{e^{-48.32/\\color{red}100.0}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}100.0} &= \\color{purple}{e^{{-0.4982}}} + \\color{purple}{e^{{-0.464}}} + \\color{purple}{e^{{-0.4525}}} + \\color{purple}{e^{{-0.473}}} + \\color{purple}{e^{{-0.4832}}} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}100.0} &= \\color{purple}{0.6076} + \\color{purple}{0.6288} + \\color{purple}{0.636} + \\color{purple}{0.6231} + \\color{purple}{0.6168} \\\\ \\color{purple}\\sum_{j=1}^{N=5} e^{x_j/\\color{red}100.0} &= \\color{purple}3.112 \\\\ \\\\ \\\\ \\text{Combine all:} \\\\ \\text{Softmax}(x_{i=1:5},{\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{x_1/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_2/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_3/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_4/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }, \\frac{\\color{steelblue}e^{x_5/\\color{red}T} }            { \\color{purple} e^{x_1/\\color{red}T} + e^{x_2/\\color{red}T} + e^{x_3/\\color{red}T} + e^{x_4/\\color{red}T} + e^{x_5/\\color{red}T}     }\\right] \\\\\\\\ \\text{Softmax}(x, {\\color{red}T}) &= \\left[\\frac{\\color{steelblue}e^{-49.82/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-46.4/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-45.25/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-47.3/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }, \\frac{\\color{steelblue}e^{-48.32/\\color{red}T}} {  \\color{purple} e^{-49.82/\\color{red}T} + e^{-46.4/\\color{red}T} + e^{-45.25/\\color{red}T} + e^{-47.3/\\color{red}T} + e^{-48.32/\\color{red}T}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}100.0}) &= \\left[\\frac{  \\color{steelblue} e^{-49.82/\\color{red}100.0}} {  \\color{purple} e^{-49.82/\\color{red}100.0} + e^{-46.4/\\color{red}100.0} + e^{-45.25/\\color{red}100.0} + e^{-47.3/\\color{red}100.0} + e^{-48.32/\\color{red}100.0}    }, \\frac{  \\color{steelblue} e^{-46.4/\\color{red}100.0}} {  \\color{purple} e^{-49.82/\\color{red}100.0} + e^{-46.4/\\color{red}100.0} + e^{-45.25/\\color{red}100.0} + e^{-47.3/\\color{red}100.0} + e^{-48.32/\\color{red}100.0}    }, \\frac{  \\color{steelblue} e^{-45.25/\\color{red}100.0}} {  \\color{purple} e^{-49.82/\\color{red}100.0} + e^{-46.4/\\color{red}100.0} + e^{-45.25/\\color{red}100.0} + e^{-47.3/\\color{red}100.0} + e^{-48.32/\\color{red}100.0}    }, \\frac{  \\color{steelblue} e^{-47.3/\\color{red}100.0}} {  \\color{purple} e^{-49.82/\\color{red}100.0} + e^{-46.4/\\color{red}100.0} + e^{-45.25/\\color{red}100.0} + e^{-47.3/\\color{red}100.0} + e^{-48.32/\\color{red}100.0}    }, \\frac{  \\color{steelblue} e^{-48.32/\\color{red}100.0}} {  \\color{purple} e^{-49.82/\\color{red}100.0} + e^{-46.4/\\color{red}100.0} + e^{-45.25/\\color{red}100.0} + e^{-47.3/\\color{red}100.0} + e^{-48.32/\\color{red}100.0}    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}100.0}) &= \\left[\\frac{  \\color{steelblue}  e^{ -0.4982 }} {  \\color{purple} e^{-0.4982 } + e^{-0.464 } + e^{-0.4525 } + e^{-0.473 } + e^{-0.4832 }    }, \\frac{  \\color{steelblue}  e^{ -0.464 }} {  \\color{purple} e^{-0.4982 } + e^{-0.464 } + e^{-0.4525 } + e^{-0.473 } + e^{-0.4832 }    }, \\frac{  \\color{steelblue}  e^{ -0.4525 }} {  \\color{purple} e^{-0.4982 } + e^{-0.464 } + e^{-0.4525 } + e^{-0.473 } + e^{-0.4832 }    }, \\frac{  \\color{steelblue}  e^{ -0.473 }} {  \\color{purple} e^{-0.4982 } + e^{-0.464 } + e^{-0.4525 } + e^{-0.473 } + e^{-0.4832 }    }, \\frac{  \\color{steelblue}  e^{ -0.4832 }} {  \\color{purple} e^{-0.4982 } + e^{-0.464 } + e^{-0.4525 } + e^{-0.473 } + e^{-0.4832 }    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}100.0}) &= \\left[\\frac{ \\color{steelblue}  { 0.6076 } } {  \\color{purple} 0.6076 + 0.6288 + 0.636 + 0.6231 + 0.6168    }, \\frac{ \\color{steelblue}  { 0.6288 } } {  \\color{purple} 0.6076 + 0.6288 + 0.636 + 0.6231 + 0.6168    }, \\frac{ \\color{steelblue}  { 0.636 } } {  \\color{purple} 0.6076 + 0.6288 + 0.636 + 0.6231 + 0.6168    }, \\frac{ \\color{steelblue}  { 0.6231 } } {  \\color{purple} 0.6076 + 0.6288 + 0.636 + 0.6231 + 0.6168    }, \\frac{ \\color{steelblue}  { 0.6168 } } {  \\color{purple} 0.6076 + 0.6288 + 0.636 + 0.6231 + 0.6168    }\\right] \\\\ \\\\ \\text{Softmax}(x, {\\color{red}100.0}) &= \\left[\\frac{ \\color{steelblue}{ 0.6076 } } {  \\color{purple} 3.112    }, \\frac{ \\color{steelblue}{ 0.6288 } } {  \\color{purple} 3.112    }, \\frac{ \\color{steelblue}{ 0.636 } } {  \\color{purple} 3.112    }, \\frac{ \\color{steelblue}{ 0.6231 } } {  \\color{purple} 3.112    }, \\frac{ \\color{steelblue}{ 0.6168 } } {  \\color{purple} 3.112    }\\right] \\\\ \\\\ \\\\ \\text{Softmax}(x, {\\color{red}100.0})&= [0.1952, 0.202, 0.2044, 0.2002, 0.1982] \\ \\\\ \\text{Probabilities} &= [0.1952, 0.202, 0.2044, 0.2002, 0.1982] \\ \\\\\\end{align}\\]\n\n\nUsing Python:\n\n\nCode\n# Example Softmax Calculation\n\n# Assume for simplicity:\n# * The model only knows the 5 words listed below (it has a vocabulary of 5).\n\nimport pandas as pd\nimport seaborn as sns\n\n#Example model output\nmodel_output_vals = {\"word_index\":[i for i in range(5)],\n                \"words\":[\"ceiling\", \"floor\", \"mat\", \"car\", \"grass\"], \n                \"logits\":[-49.82, -46.40, -45.25, -47.30, -48.32]}\ntemp = 100.0\n#Convert the data to a DataFrame\nmodel_output = pd.DataFrame(model_output_vals)\n\n#Define a softmax function with temperature\ndef my_softmax(input_vector, Temp=1.0):\n    e = np.exp(np.divide(input_vector,Temp))\n    return e / e.sum()\n\n#Calculate the probabilities\nprobs =  my_softmax(model_output[\"logits\"], Temp=temp)\nmodel_output[\"softmax_prob\"] = probs \n\n#Select the most probable word\nmost_prob = np.argmax(probs)\n\nprint(f\"\\nThe index of the most probable word is: {most_prob}\")\n\n#Pull out the most probable word\nprint(f\"\\nThe most probable word is: { model_output['words'][most_prob] }\" \\\n      f\" (Prob: {model_output['softmax_prob'][most_prob]:.5f})\")\n\n\n#Style our table\ncm = sns.light_palette(\"orange\", as_cmap=True)\ns1 = model_output\ns1 = s1.style.background_gradient(subset=[\"logits\"],cmap=cm)\n\ncm = sns.light_palette(\"green\", as_cmap=True)\ns1.background_gradient(subset=[\"softmax_prob\"],cmap=cm)\n\n\n\nThe index of the most probable word is: 2\n\nThe most probable word is: mat (Prob: 0.20436)\n\n\n\n\n\n\n\n \nword_index\nwords\nlogits\nsoftmax_prob\n\n\n\n\n0\n0\nceiling\n-49.820000\n0.195229\n\n\n1\n1\nfloor\n-46.400000\n0.202022\n\n\n2\n2\nmat\n-45.250000\n0.204358\n\n\n3\n3\ncar\n-47.300000\n0.200211\n\n\n4\n4\ngrass\n-48.320000\n0.198180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the softmax probabilities we see that the most probable word is: mat with a probability of : 0.204\n\n\n\n\nWhat does this mean?\n\nAs the temperature increases from 1.0 to 100.0, the probability distribution shifts from being more concentrated (or “peaky”) to more spread out (or “flat”), meaning that words with low probabilities at lower temperatures gain a higher chance of being selected.\nUsing greedy sampling, where the word with the highest probability is always chosen, the model consistently selects the top-ranked word. However, if we modify the sampling method to randomly pick a word from the top 3 highest-probability words, the potential options expand to include words like [‘mat’, ‘floor’, ‘car’]."
  },
  {
    "objectID": "projects/Short_dive_posts/LLM_temp/LLM_temp.html#llm-with-temperature-applied",
    "href": "projects/Short_dive_posts/LLM_temp/LLM_temp.html#llm-with-temperature-applied",
    "title": "Why Does My LLM Have A Temperature?",
    "section": "LLM with Temperature Applied",
    "text": "LLM with Temperature Applied\nTo see how the temperature parameter affects the outputs of a Large Language Model (LLM), we’ll use GPT-2, an open-source text generation model developed by OpenAI. GPT-2 is available through platforms like Hugging Face and is known for being a moderately sized model.  GPT-2 has the following characteristics:\n\n124 million parameters: These are the learnable weights of the model, which help it make predictions based on the input data.\n50,257 vocabulary size: The model’s vocabulary consists of a set of tokens (words or subwords using Byte Pair Encoding) that GPT-2 is trained to recognize and generate.\n768-dimensional vector embedding size: This refers to the size of the dense vector representations used to encode each token.\n12 attention heads: These are the parallel attention mechanisms used in each transformer layer to capture different aspects of the input sequence’s relationships.\n12 layers: The model has 12 transformer layers, which allow it to process and understand more complex patterns in the data.\n\n We’ll look at using the LLM for two types of tasks:\n\nSingle next-word generation: Predicting the next word based on the context of the given input.\nContinuous next-word generation: Generating a sequence of words, where each new word is predicted based on the previously generated words.\n\n\n\n\nModel Set up\n\nTo get the model(gpt2) we download it from Hugging Face and set the model’s task to text-generation\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_to_load = \"openai-community/gpt2\"\nmodel_to_load_task = \"text-generation\"\n\n# Load the model's pretrained tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_to_load)\n\n# Load the pretrained model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_to_load,\n    device_map = device, #CPU or GPU\n    torch_dtype = \"auto\",\n    trust_remote_code = True\n)\nTo pass inputs to the model we can run the following:\n# Input sentence\nprompt = \"The cat sat on the\"\n\ntemperature = 0.5\n\n# Tokenize/encode input prompt\ninput_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n# Generate the output with adjusted temperature\noutputs = model.generate(input_ids,\n                        max_new_tokens=1, #Just want one word generated\n                        temperature=temperature, #Set temp\n                        output_scores=True, #Output model word scores\n                        output_logits=True, #Outout logits\n                        return_dict_in_generate=True,\n                        do_sample=True, #Perform sampling for next word\n                        pad_token_id=tokenizer.eos_token_id)\n\n\n# Get the generated token ID/next word\ngenerated_token_id = outputs.sequences[0][-1].item()    \n\n\n# Decode the generated token ID to a word\ngenerated_word = tokenizer.decode([generated_token_id])\n\n\nSingle next-word generation\nIn single next-word generation, GPT-2 is given an initial input sequence (such as a partial sentence) and predicts the most likely next word. The model makes this prediction based on the context provided by the preceding words in the sequence. Once the next word is predicted, it is outputted and the process stops, meaning only one word is generated at a time. The word is selected based on the highest probability according to the model’s learned associations, and no further prediction occurs unless the process is repeated with a new input.  We’ll pass the same sentence we’ve been looking at to the LLM to see what it will output.\n\nInput sentence: The cat slept on the ______.\n\nprompt = \"The cat slept on the\"\ntemps = [0.1,  0.5, 1., 5., 10., 100.]\nfor ii in temps:\n  word_out = next_word_prediction(prompt, temp=ii)\n\n  print(f\"LLM Temperature: {ii} \\n {prompt} {word_out}\")\nHere we pass the same input sentence to the LLM with different temperature values and look at the probability distribution of select words in the model’s vocabulary.\n\nExamples\n\n\nSee examples\n\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n2024-12-31 20:18:26.351669: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-31 20:18:26.895123: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-31 20:18:27.126555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-31 20:18:27.205691: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-31 20:18:27.642979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nLLM Temperature: 0.1\nInput : The cat slept on the \nOutput: The cat slept on the   floor\n\n\n\n\n\n\n\n\n\nLLM Temperature: 0.5\nInput : The cat slept on the \nOutput: The cat slept on the   bed\n\n\n\n\n\n\n\n\n\nLLM Temperature: 1.0\nInput : The cat slept on the \nOutput: The cat slept on the   back\n\n\n\n\n\n\n\n\n\nLLM Temperature: 5.0\nInput : The cat slept on the \nOutput: The cat slept on the   bathroom\n\n\n\n\n\n\n\n\n\nLLM Temperature: 10.0\nInput : The cat slept on the \nOutput: The cat slept on the   corner\n\n\n\n\n\n\n\n\n\nLLM Temperature: 100.0\nInput : The cat slept on the \nOutput: The cat slept on the   inside\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does this mean?\n\nAs the temperature increases from 0.1 and 100.0, the probability distribution shifts from being more concentrated (or “peaky”) to more spread out (or “flat”), meaning that words with low probabilities at lower temperatures gain a higher chance of being selected. Note that as the temperature increases, each word results in a lower probability.\n\n\n\n\nContinuous next-word generation\nIn continuous next-word generation, GPT-2 is given an initial input sentence and predicts the next most likely word in an autoregressive manner. The model generates each word based on the previous words it has already predicted, using the context it has built up. After predicting the next word, it is added to the sentence, and the updated sequence is passed back into the model for the next iteration. This process continues until one of two conditions is met: the model generates an end-of-sequence token (such as &lt;EOS&gt; or \\n), or the maximum number of iterations (or tokens) is reached.\n We’ll pass the same sentence we’ve been looking at to the LLM to see what it will  output  over a number of iterations like below.\n\nInput sentence: The cat slept on the ______\n1: The cat slept on the floor ______\n2: The cat slept on the floor next ______\n3: The cat slept on the floor next to ______\n4: The cat slept on the floor next to the ______\n5: The cat slept on the floor next to the window ______\n6: The cat slept on the floor next to the window . ______\n7: The cat slept on the floor next to the window . &lt; EOS &gt;\n\nWe’ll pass the prompt to the LLM and append its predicted output (word_out) to the prompt and keep iterating until we reach the max number of iterations (max_gen_iteration) or and end of sentence token (&lt;EOS&gt; or \\n) is predicted.\nprompt = \"The cat slept on the\"\ntemp = 0.5\nmax_gen_iteration = 20\nfor ii in range(max_gen_iteration):\n  word_out, probs_out = next_word_prediction(prompt, temp=temp)\n  print(prompt + word_out)\n  prompt += word_out\nHere we pass the same input sentence to the LLM with different temperature values and look at the probability distribution of select words in the model’s vocabulary.\n\nExamples\n\n\nTemp: 0.5\n\nParameters:\n\nInput text: “The cat slept on the”\nTemperature: 0.5\nMax iterations: 20\n\n\n\nCode\nprompt = \"The cat slept on the\"\ntemp = 0.5\nmax_iter = 20\n\ngen_next_word_loop(prompt, temp = temp, max_iter = max_iter)\n\n\nThe cat slept on the floor\n\n\nThe cat slept on the floor,\n\n\nThe cat slept on the floor, and\n\n\nThe cat slept on the floor, and she\n\n\nThe cat slept on the floor, and she walked\n\n\nThe cat slept on the floor, and she walked over\n\n\nThe cat slept on the floor, and she walked over to\n\n\nThe cat slept on the floor, and she walked over to the\n\n\nThe cat slept on the floor, and she walked over to the table\n\n\nThe cat slept on the floor, and she walked over to the table where\n\n\nThe cat slept on the floor, and she walked over to the table where the\n\n\nThe cat slept on the floor, and she walked over to the table where the other\n\n\nThe cat slept on the floor, and she walked over to the table where the other two\n\n\nThe cat slept on the floor, and she walked over to the table where the other two cats\n\n\nThe cat slept on the floor, and she walked over to the table where the other two cats sat\n\n\nThe cat slept on the floor, and she walked over to the table where the other two cats sat.\n\n\nThe cat slept on the floor, and she walked over to the table where the other two cats sat. \n\n\n\n\n\nTemp: 2.0\n\nParameters:\n\nInput text: “The cat slept on the”\nTemperature: 2.0\nMax iterations: 20\n\n\n\nCode\nprompt = \"The cat slept on the\"\ntemp = 2.0\nmax_iter = 20\n\ngen_next_word_loop(prompt, temp = temp, max_iter = max_iter)\n\n\nThe cat slept on the edge\n\n\nThe cat slept on the edge of\n\n\nThe cat slept on the edge of town\n\n\nThe cat slept on the edge of town for\n\n\nThe cat slept on the edge of town for hours\n\n\nThe cat slept on the edge of town for hours each\n\n\nThe cat slept on the edge of town for hours each day\n\n\nThe cat slept on the edge of town for hours each day or\n\n\nThe cat slept on the edge of town for hours each day or the\n\n\nThe cat slept on the edge of town for hours each day or the day\n\n\nThe cat slept on the edge of town for hours each day or the day was\n\n\nThe cat slept on the edge of town for hours each day or the day was very\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult at\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult at all\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult at all –\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult at all – one\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult at all – one of\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult at all – one of a\n\n\nThe cat slept on the edge of town for hours each day or the day was very difficult at all – one of a kind\n\n\n\n\n\nTemp: 10.0\n\nParameters:\n\nInput text: “The cat slept on the”\nTemperature: 10.0\nMax iterations: 20\n\n\n\nCode\nprompt = \"The cat slept on the\"\ntemp = 10.0\nmax_iter = 20\n\ngen_next_word_loop(prompt, temp = temp, max_iter = max_iter)\n\n\nThe cat slept on the other\n\n\nThe cat slept on the other walls\n\n\nThe cat slept on the other walls behind\n\n\nThe cat slept on the other walls behind of\n\n\nThe cat slept on the other walls behind of me\n\n\nThe cat slept on the other walls behind of me;\n\n\nThe cat slept on the other walls behind of me; all\n\n\nThe cat slept on the other walls behind of me; all by\n\n\nThe cat slept on the other walls behind of me; all by no\n\n\nThe cat slept on the other walls behind of me; all by no small\n\n\nThe cat slept on the other walls behind of me; all by no small deal\n\n\nThe cat slept on the other walls behind of me; all by no small deal,“\n\n\nThe cat slept on the other walls behind of me; all by no small deal,“ we\n\n\nThe cat slept on the other walls behind of me; all by no small deal,” we’ll\n\n\nThe cat slept on the other walls behind of me; all by no small deal,” we’ll go\n\n\nThe cat slept on the other walls behind of me; all by no small deal,” we’ll go onto\n\n\nThe cat slept on the other walls behind of me; all by no small deal,” we’ll go onto another\n\n\nThe cat slept on the other walls behind of me; all by no small deal,” we’ll go onto another bit\n\n\nThe cat slept on the other walls behind of me; all by no small deal,” we’ll go onto another bit (\n\n\nThe cat slept on the other walls behind of me; all by no small deal,” we’ll go onto another bit (one\n\n\n\n\n\n\nWhat does this mean?\n\nWhen comparing outputs at temperatures of 0.5 and 10.0, we observe that the text generated at a temperature of 0.5 is more coherent, while at a temperature of 10.0, the output becomes increasingly incoherent and less understandable to a human reader.\nThis highlights how the temperature parameter affects continuous word generation by altering the probability distribution of possible next words within the model’s vocabulary."
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "For the latest publications see Google Scholar"
  },
  {
    "objectID": "publications/publications.html#papers",
    "href": "publications/publications.html#papers",
    "title": "Publications",
    "section": "Papers",
    "text": "Papers\n\n\n\n\n\n\n\n  \n    \n      Frontal HD-tACS enhances behavioral and EEG biomarkers of vigilance in continuous attention task \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2024. Nigel Gebodh, Vladimir Miskovic, Sarah Laszlo, Abhishek Datta, Marom Bikson. Brain Stimulation.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n         \n           GitHub \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      A pilot randomized controlled trial of transcranial direct current stimulation adjunct to moderate-intensity aerobic exercise in hypertensive individuals \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2024. Edson Silva-Filho, Marom Bikson, Nigel Gebodh, Niranjan Khadka, Amilton da Cruz Santos, Rodrigo Pegado, Maria do Socorro Brasileiro-Santos. Frontiers in Neuroergonomics.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      A Scalable Framework for Closed-Loop Neuromodulation with Deep Learning \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2023. Nigel Gebodh, Vladimir Miskovic, Sarah Laszlo, Abhishek Datta, Marom Bikson. BioRxiv.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n         \n           GitHub \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Novel Evoked Synaptic Activity Potentials (ESAPs) Elicited by Spinal Cord Stimulation \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2023. Mahima Sharma, Vividha Bhaskar, Lillian Yang, Mohamad FallahRad, Nigel Gebodh, Tianhe Zhang, Rosana Esteller, John Martin, Marom Bikson. eNeuro.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Dataset of concurrent EEG, ECG, and behavior with multiple doses of transcranial electrical stimulation \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2021. Nigel Gebodh, Zeinab Esmaeilpour, Abhishek Datta, Marom Bikson. Nature Scientific Data.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           OSF\n        \n      \n\n      \n         \n           Zenodo\n        \n      \n\n      \n         \n           figshare\n        \n      \n      \n      \n         \n           GitHub \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Electrical stimulation of cranial nerves in cognition and disease \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2020. Devin Adair, Dennis Truong, Zeinab Esmaeilpour, Nigel Gebodh, Helen Borges, Libby Ho, Douglas J. Bremmer, Bashar W. Badran, Vitaly Napadow, Vincent P. Clark, Marom Bikson. Brain Stimulation.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Experimental-design Specific Changes in Spontaneous EEG and During Intermittent Photic Stimulation by High Definition Transcranial Direct Current Stimulation \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2020. Vladimir Lazarev, Nigel Gebodh, Tiago Tamborino, Marom Bikson, Egas M Caparelli-Daquer. Neuroscience.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Antiepileptic Effects of a Novel Non-invasive Neuromodulation Treatment in a Subject With Early-Onset Epileptic Encephalopathy: Case Report With 20 Sessions of HD-tDCS Intervention \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2019. Oded Meiron, Rena Gale, Julia Namestnic, Odeya Bennet-Back, Nigel Gebodh, Zeinab Esmaeilpour, Vladislav Mandzhiyev, Marom Bikson. Frontiers in Neuroscience.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Inherent physiological artifacts in EEG during tDCS \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2018. Nigel Gebodh, Zeinab Esmaeilpour, Devin Adair, Kenneth Chelette, Jacek Dmochowski, Adam J. Woods, Emily S. Kappenman, Lucas C. Parra, Marom Bikson. NeuroImage.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Effects of stimulus size and contrast on the initial primary visual cortical response in humans \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2017. Nigel Gebodh, Marta Isabel Vanegas, Simon P. Kelly. Brain Topogr.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Clinically Effective Treatment of Fibromyalgia Pain With High-Definition Transcranial Direct Current Stimulation: Phase II Open-Label Dose Optimization \n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n        \n          2015. Nigel Gebodh, Laura Castillo-Saavedra, Marom Bikson, Camilo Diaz-Cruz, Rivail Brandao, Livia Coutinho, Dennis Truong, Abhishek Datta, Revital Shani-Hershkovich, Michal Weiss, Ilan Laufer, Amit Reches, Ziv Peremen, Amir Geva, Lucas C. Parra, Felipe Fregni. J. Pain.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#book-chapters",
    "href": "publications/publications.html#book-chapters",
    "title": "Publications",
    "section": "Book Chapters",
    "text": "Book Chapters\n\n\n\n\n\n\n\n  \n    \n      Transcranial direct current stimulation among technologies for low-intensity transcranial electrical stimulation: classification, history, and terminology \n      \n      \n\n      \n      \n        \n          Jan 24, 2019. Nigel Gebodh, Zeinab Esmaeilpour, Devin Adair, Pedro Schestattsky, Felipe Fregni, Marom Bikson. Chapter 1. Practical guide to transcranial direct current stimulation. \n        \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Transcranial direct current stimulation integration with magnetic resonance imaging, magnetic resonance spectroscopy, near infrared spectroscopy imaging, and electroencephalography \n      \n      \n\n      \n      \n        \n          Jan 24, 2019. Adam J. Woods, Marom Bikson, Kenneth Chelette, Jacek Dmochowski, Anirban Dutta, Zeinab Esmaeilpour, Nigel Gebodh, Michael A. Nitsche, Charlotte Stagg. Chapter 11. Practical guide to transcranial direct current stimulation. \n        \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Transcranial Electrical Stimulation: transcranial Direct Current Stimulation (tDCS), transcranial Alternating Current Stimulation (tACS), transcranial Pulsed Current Stimulation (tPCS), and Transcranial Random Noise Stimulation (tRNS) \n      \n      \n\n      \n      \n        \n          Jul 11, 2014. Ingrid Moreno-Duarte, Nigel Gebodh, Pedro Schestatsky, Berkan Guleyupoglu, Davide Reato, Marom Bikson, Felipe Fregni. Chapter 2. The Stimulated Brain. \n        \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n\n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#research-presentations",
    "href": "publications/publications.html#research-presentations",
    "title": "Publications",
    "section": "Research Presentations",
    "text": "Research Presentations\n\nGebodh N, Datta A, Bikson M. A Machine Learning Framework of Closed-loop Neuromodulation Applied to Sustained Attentional Enhancement. Poster session presented at Military Health System Research Symposium; 2022 Sep 12- 15; Kissimmee, FL.\nGebodh N, Miskovic V, Laszlo S, Datta A, Bikson M. Deep learning framework for non-invasive closed-loop neuromodulation for attention. Poster session presented at Neuroergonomics & NYC Neuromodulation; 2022 Jul 28- Aug 1; New York, NY.\nSharma M, Bhaskar V, Yang L, Gebodh N, FallahRad M, Zhang T, Esteller R, Martin J, Bikson M. Evoked Synaptic Excitatory Potentials (ESAPs): A novel electrophysiological biomarker for Spinal Cord Stimulation. Poster session presented at Neuroergonomics & NYC Neuromodulation; 2022 Jul 28- Aug 1; New York, NY.\nGebodh N, Bikson M. A Machine Learning Based Closed-Loop Neuromodulation Framework. Poster session presented at The International Neuromodulation Society; 2022 May 21-26; Barcelona, Spain. Doi: https://doi.org/10.1016/j.neurom.2022.08.002\nGebodh N, Esmaeilpour Z, Datta A, Bikson M. A novel approach to closed-loop neuromodulation with machine learning. Poster session presented at 4th International Brain Stimulation Conference; 2021 Dec 7-10;Charleston,South Carolina. Doi: https://doi.org/10.1016/j.brs.2021.10.235\nGebodh N, Esmaeilpour Z, Datta A, Bikson M. A large open source neuromodulation dataset of concurrent EEG, ECG, behavior, and transcranial electrical stimulation. Poster session presented at 4th International Brain Stimulation Conference; 2021 Dec 7-10;Charleston,South Carolina. Doi: https://doi.org/10.1016/j.brs.2021.10.108\nGebodh N, Vacchi L, Adair D, Esmaeilpour Z, Poltorak A, Poltorak V, Bikson M. Modulation of Sleepiness and Physiology with Brain-Derived and Narrow-Band tACS. Poster session presented at 2019 Neuromodulation: The Science and NYC Neuromodulation Joint Meeting; 2019 Oct 2-5; Napa, California.\nVacchi L, Gebodh N, Adair D, Unal G, Poltorak A, Poltorak V, Bikson M. Transcranial Endogenous Sleep-Derived waveform to induce sleepiness. Poster session presented at The Annual Meeting of Milan Center for Neuroscience; 2018 Nov 21; Milan, Italy.\nGebodh N, Adair D, Esmaeilpour Z, Chelette K, Dmochowski J, Woods A, Kappenman E, Bikson M. Failure of Conventional Signal Processing Techniques to Remove “Physiological” Artifacts from EEG During tDCS. Poster session presented at 2018 NYC Neuromodulation Conference and NANS Summer Series; 2018 Aug 24-26; New York, NY.\nMeiron O, Gale R, Namestnic J, Bennet-Back O, David J, Gebodh N, Adair D, Esmaeilpour Z, Bikson M. Attenuation of pathological EEG features in nonatal electroclinical syndromes: HD-tDCS in catastrophic epilepsies. Poster session presented at Brain Stimulation: Basic, Translational, and Clinical Research in Neuromodulation; 2018 Aug 24-26; New York, NY.\nGebodh N, Vacchi L, Adair D, Unal G, Poltorak A, Poltorak V, Bikson M. Replay of endogenous sleep rhythms to produce sleepiness. Poster session presented at 2018 NYC Neuromodulation Conference and NANS Summer Series; 2018 Aug 24-26; New York, NY. Doi: https://doi.org/10.1016/j.brs.2018.12.180\nGebodh N, Adair D, Esmaeilpour Z, Chelette K, Dmochowski J, Woods A, Kappenman E, Bikson M. Physiologic artifacts when combining EEG and tDCS. Poster session presented at 2nd International Brain Stimulation Conference; 2017 Mar 5-8; Barcelona, Spain.\nGebodh N, Adair D, Esmaeilpour Z, Chelette K, Dmochowski J, Woods A, Kappenman E, Bikson M. Modulation of physiologic artifacts during concurrent tDCS and EEG. Poster session presented at 6th International Conference on Transcranial Brain Stimulation; 2016 Sept 7-10; Gottingen, DE.\nGebodh N, Castillo L, Truong D, Bikson M, Shani-Hershkovich R, Weiss M, Laufer I, Reches A, Peremen Z, Geva A, Parra LC, and Fregni F. Clinically effective treatment of fibromyalgia pain with HD-tDCS – Phase II open-label dose-optimization. Poster session presented at Military Health Research Systems Symposium; 2015 Aug. 16-21; Ft. Lauderdale, FL.\nGebodh N, Vanegas I, Kelly S. Characterization of the effects of stimulus size and contrast on the initial afferent response in human primary visual cortex. Poster session presented at Society for Neuroscience; 2014 Nov. 15-19; Washington DC.\nVanegas I, Blangero A, Gebodh N, Ali A, Kelly S. Task-dependent attentional modulation of initial afferent activity in human primary visual cortex. Poster session presented at Society for Neuroscience; 2014 Nov. 15-19; Washington DC."
  },
  {
    "objectID": "publications/publications.html#ad-hoc-review",
    "href": "publications/publications.html#ad-hoc-review",
    "title": "Publications",
    "section": "Ad Hoc Review",
    "text": "Ad Hoc Review\n\neNeuro\nScientific Reports\nBrain Stimulation\n\nSleep Medicine\n\nPLOS Comp Bio\n\nJ.NeuroEng & Rehab\nNeuroImage\nNeuropsychologia\nApplied Intelligence\nPsychophysiology\nHemodialysis Intl"
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "",
    "text": "Large Language Models (LLMs) have transformed the way un/structured data analytics is performed.\nHere we leverage prompt engineering techniques and LLMs on Amazon Web Services (AWS) Bedrock (and Hugging Face) to build an analytics pipeline specific to healthcare applications with clinical data."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#anatomy-of-prompts",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#anatomy-of-prompts",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Anatomy of Prompts",
    "text": "Anatomy of Prompts\nThere are several key aspects to effectively creating a prompt to feed to an LLM. We’ll leverage combinations of these prompt components in our data analysis pipeline.\nTo get a more in-depth understanding of the state of the art in prompting, check out these papers (Brown et al., 2020; Qiao et al., 2023; Schulhoff et al., 2025).\nThese are some of the basic prompt aspects (You can test them with different LLMs here):\n\n\nDirective\n\nThe user’s intent for the LLM. These can be explicitly stated as an instruction or implied with the text format or examples provided (one example → referred to as one-shot)\nExample: We want a direct set of information returned\n\nExplicit: Instruction given on what output to produce\n\nINPUT: Generate a list of the five longest rivers in the world. \nOUTPUT: Nile River, Amazon River, Yangtze River, Mississippi River, Yenisei River\n\nImplicit: Task implied with structure of the text\n\nINPUT: New York City: United States Paris:\nOUTPUT: France\n\n\n\n\n\nOutput Formatting\n\nIf a structured output is needed (JSON, CSV, XML etc.) from the LLM, specifying and defining the desired output guides the model to return the desired format. This is also helpful for evaluating a model’s performance if you have a structured ground truth reference/expected output to compare to the models output.\nExample: We want JSON structured (basically a nested dictionary) information returned. We can give an example or have the LLM extract and classify the information to JSON tags (dict keys).\n\nINPUT: Return the following text in `JSON` format: My name is Sam Mitra. I’m currently 28 and I live in Redmond, WA. I have 2 dogs and 1 cat.\nOUTPUT:\n{\n\"name\": \"Sam Mitra\",\n\"age\": 28,\n\"location\": \"Redmond, WA\",\n\"pets\": [\n    {\"type\": \"dog\", \"count\": 2},\n    {\"type\": \"cat\", \"count\": 1}\n        ]\n}\n\n\n\n\nStyle Instructions\n\nThese are instructions that control the LLM’s output style by including descriptive adjectives like “clear”, “concise”, “casual”, or “professional” in our prompts, which directly affect the vocabulary used.\nExample: We want a clear, concise definition of what a dog is\n\nINPUT:  Give a clear and concise definition of what a dog is\nOUTPUT: A dog is a domesticated carnivorous mammal (Canis lupus familiaris) that belongs to the family Canidae.\n\nExample:\nWe want a casual, friendly definition of what a dog is\n\nINPUT: Give a casual and friendly definition of what a dog is\nOUTPUT: Dogs are basically the best! They're furry, four-legged friends that are all about giving you love, snuggles, and slobbery kisses.\n\n\n\n\nRole\n\nThis is a persona or perspective that you define the LLM to have in producing outputs. This can influence the output style and content returned.\nExample: Define the LLM’s role as a “biomedical engineer and medical doctor”\n\nINPUT: As a biomedical engineer and medical doctor, give a short, concise explanation of what sea sickness is.\nOUTPUT: Sea sickness, also known as motion sickness, is a complex phenomenon resulting from the mismatch between the vestibular system's perception of motion and the visual and proprioceptive systems' input.\n\nExample: Define the LLM’s role as a “dog”\n\nINPUT: As a dog, give a short, concise explanation of what sea sickness is.\nOUTPUT: WOOF WOOF! whine Oh no, oh no! Sea sickness is when my hooman gets all wobbly and queasy on the boat!"
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#prompting",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#prompting",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Prompting",
    "text": "Prompting\n\n\nPrompting\n\nFor prompt engineering we can think of the input (\\(\\mathbf{X}\\)) as having two parts:\n\n\\(\\mathcal{T}\\) - the prompt, meaning the general instruction or task you want the LLM to do\n\nExample \\(\\mathcal{T}\\): Translate from English to French:\n\n\\(\\mathcal{Q}\\) - the specific example or question that you’re passing to the LLM\n\nExample \\(\\mathcal{Q}\\): The cat and the dog sleep\n\n\nThe full input with returned output would be:\n\nInput: \\(\\mathcal{T}\\), \\(\\mathcal{Q}\\): Translate from English to French: The cat and the dog sleep\nOutput: \\(\\mathbf{Y}\\) or \\(\\mathbf{A}\\): Le chat et le chien dorment\n\nWe can rewrite our general LLM parameterization as:\n\\[\\begin{align*}\n\nP( \\mathbf{Y} | \\textcolor{Plum}{\\mathbf{X}})\n    & = \\prod_{t=1}^{|\\mathbf{Y} |} p_{\\text{LM}}(y_t |  \n      y_{1:t-1},\n      \\textcolor{Plum}{\\mathbf{X}}) \\\\\n\nP( \\mathbf{Y} | \\textcolor{Plum}{\\mathcal{T} }, \\textcolor{Plum}{\\mathcal{Q}})\n    & = \\prod_{t=1}^{|\\mathbf{Y} |} p_{\\text{LM}}(y_t |  \n      y_{1:t-1},\n      \\textcolor{Plum}{\\mathcal{T} }, \\textcolor{Plum}{\\mathcal{Q}}) \\\\\n\n\\end{align*}\\]\nFor convention we can change \\(\\mathbf{Y} \\to \\mathbf{A}\\) meaning Complete Answer and \\(a_t\\) is the \\(t^{th}\\) generated token:\n\\[\\begin{align*}\nP( \\textcolor{SeaGreen}{\\mathbf{Y}} | \\textcolor{Plum}{\\mathcal{T} }, \\textcolor{Plum}{\\mathcal{Q}})\n    & = \\prod_{t=1}^{|\\mathbf{Y} |} p_{\\text{LM}}(y_t |  \n      y_{1:t-1},\n      \\textcolor{Plum}{\\mathcal{T} }, \\textcolor{Plum}{\\mathcal{Q}}) \\\\\n\nP( \\textcolor{SeaGreen}{\\mathbf{A}} | \\textcolor{Plum}{\\mathcal{T} }, \\textcolor{Plum}{\\mathcal{Q}})\n    & = \\prod_{t=1}^{|\\textcolor{SeaGreen}{\\mathbf{A}} |} p_{\\text{LM}}(\\textcolor{SeaGreen}{a}_{t} |  \n      \\textcolor{SeaGreen}{a}_{1:t-1},\n      \\textcolor{Plum}{\\mathcal{T} }, \\textcolor{Plum}{\\mathcal{Q}}) \\\\\n\n\\end{align*}\\]\nNote: The calculations stop when an end-of-sequence token (i.e. &lt;EOS&gt; or &lt;|endoftext|&gt;) is generated."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#prompt-templating",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#prompt-templating",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Prompt Templating",
    "text": "Prompt Templating\n\n\nPrompt Templating\n\nWe can generalize the equation even more by thinking of the prompt (\\(\\mathcal{T}\\)) as a function or a template that takes in different questions/queries (\\(\\mathcal{Q} \\to x^{*}\\)). We can represent the prompt template as \\(\\mathcal{T(\\cdot)}\\) and when a question/query is plugged in we get \\(\\mathcal{T(x^{*})}\\). The equation then becomes:\n\\[\\begin{align*}\n\nP( \\textcolor{SeaGreen}{\\mathbf{A}} | \\textcolor{Plum}{\\mathcal{T(x^{*})} } )\n    & = \\prod_{t=1}^{|\\textcolor{SeaGreen}{\\mathbf{A}} |} p_{\\text{LM}}(\\textcolor{SeaGreen}{a}_{t} |  \n      \\textcolor{SeaGreen}{a}_{1:t-1},\n      \\textcolor{Plum}{\\mathcal{T(x^{*})} }) \\\\\n\n\\end{align*}\\]\nFor example:\n\n\\(T(\\cdot)\\) - The template could be:  What is the capital of {ENTER COUNTY}?\n\\(x^{*}\\) - The query or question would be:  France\n\\(\\mathcal{T(x^{*})}\\) - The full input would be: What is the capital of France?\n\\(\\textcolor{SeaGreen}{\\mathbf{A}}\\) - The returned answer would be: Paris"
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#few-shot-prompting",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#few-shot-prompting",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Few-Shot Prompting",
    "text": "Few-Shot Prompting\n\n\nFew-Shot Prompting\n\nWith few-shot prompting we can think of it as just supplying the template \\(\\mathcal{T(\\cdot)}\\) with another input which are examples to use to do the task. We can represent the set of examples as \\(\\phi\\) where:\n\\[\\begin{align*}\n\\phi = \\{ (z_{i}, y_{i} ) \\}_{i=1}^{n}\n\\end{align*}\\]\nIn this case:\n\n\\(z_i\\) is just example \\(i\\) that we’re passing to the LLM with our query\n\\(y_i\\) is the corresponding answer to the \\(i^{th}\\) example.\n\nFor example, for the task of getting country capitals, a 5-shot prompt (\\(n\\) is 5 examples, \\(z_i\\) - country and \\(y_i\\) - the country’s capital) would look like the following where \\(\\phi\\) is: \\[\\begin{align*}\n\\phi = \\{\n  (z_1 = Algeria, y_1 = Algiers)_{1}, \\\\\n  (z_2 =France, y_2 = Paris)_{2}, \\\\\n  (z_3 =Spain, y_3 = Madrid)_{3}, \\\\\n  (z_4 =Norway, y_4 = Oslo)_{4}, \\\\\n  (z_5 =Chile, y_5 = Santiago)_{5}, \\\\\n          \\}\n\\end{align*}\\]\nThe few-shot prompted equation then looks like:\n\\[\\begin{align*}\n\nP( \\textcolor{SeaGreen}{\\mathbf{A}} | \\textcolor{Plum}{\\mathcal{T(\\phi, x^{*})} } )\n    & = \\prod_{t=1}^{|\\textcolor{SeaGreen}{\\mathbf{A}} |} p_{\\text{LM}}(\\textcolor{SeaGreen}{a}_{t} |  \n      \\textcolor{SeaGreen}{a}_{1:t-1},\n      \\textcolor{Plum}{\\mathcal{T(\\phi, x^{*})} }) \\\\\n\n\\end{align*}\\]\nFor example the full input prompt would then be:\n\n\\(\\mathcal{T(\\cdot)}\\): What is the capital of {ENTER COUNTRY}?\n\\(\\phi\\): (Algeria, Algiers) (France, Paris) (Spain, Madrid) (Norway, Oslo) (Chile, Santiago)\n\\(x^{*}\\): Germany\n\\({\\mathcal{T(\\phi, x^{*})} }\\): What is the capital of Algeria? Algiers What is the capital of France? Paris What is the capital of Spain? Madrid What is the capital of Norway? Oslo What is the capital of Chile? Santiago What is the capital of Germany?\n\nThis full prompt (\\({\\mathcal{T(\\phi, x^{*})} }\\)) would then be passed to the LLM which should return the response (\\(\\textcolor{SeaGreen}{\\mathbf{A}}\\)) Berlin."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#aws---getting-started",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#aws---getting-started",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "AWS - Getting Started",
    "text": "AWS - Getting Started\n\n\nGetting started\n\nTo begin using AWS Bedrock:\n\nCreate an AWS account.\nCreate an IAM user and obtain access keys (AWS Secret Access Key ID and AWS Secret Access Key - see here).\n\nThese keys are used for API authentication and should be stored securely as environment secrets or in a .env file.\n\nRequest model access in the Bedrock console.\n\nOn Bedrock we can go to Model access then request the models we want to work with. This will take a few minutes for access to be granted.\n\nInstall the boto3 Python library for API interaction with Bedrock\n\nBedrock uses the boto3 Python library to set up information transfer to and from AWS."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#aws---setting-up-bedrock",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#aws---setting-up-bedrock",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "AWS - Setting up Bedrock",
    "text": "AWS - Setting up Bedrock\n\n\nSetting up Bedrock\n\nOnce we have all our credentials in order we set up our connection to AWS. To establish a connection to AWS we import the:\n\nboto3 library to set up our runtime connection\nos library to get our environment secrets (API ID and key)\n\nImport libraries:\nimport boto3\nimport os\nSet up our AWS Bedrock access:\n\n# Configure AWS credentials\nbedrock_runtime = boto3.client(\n    service_name='bedrock-runtime',\n    region_name=os.environ['AWS_DEFAULT_REGION'], #Region where the Bedrock service is running e.g. us-east-1\n    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'], #AWS Access Key ID\n    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'] #AWS Secret Access Key\n)\n\nbedrock_runtime\n\n&lt;botocore.client.BedrockRuntime at 0x78c610c21750&gt;"
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#aws---llm-api-templates",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#aws---llm-api-templates",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "AWS - LLM API Templates",
    "text": "AWS - LLM API Templates\n\n\nLLM API Templates\n\nBedrock’s API required model-specific request formats (in some cases it still does), varying significantly between models like Llama 3.3 70B Instruct and Claude V2.\nExample Request Differences:\n\nLlama 3.3 70B Instruct:\n{\n\"modelId\": \"meta.llama3-3-70b-instruct-v1:0\",\n\"contentType\": \"application/json\",\n\"accept\": \"application/json\",\n\"body\": \"{\\\"prompt\\\":\\\"this is where you place your input text\\\",\\\"max_gen_len\\\":512,\\\"temperature\\\":0.5,\\\"top_p\\\":0.9}\"\n}\nAnthropic Claude V2:\n{\n  \"modelId\": \"anthropic.claude-v2\",\n  \"contentType\": \"application/json\",\n  \"accept\": \"*/*\",\n  \"body\": \"{\\\"prompt\\\":\\\"\\\\n\\\\nHuman: Hello world\\\\n\\\\nAssistant:\\\",\\\"max_tokens_to_sample\\\":300,\\\"temperature\\\":0.5,\\\"top_k\\\":250,\\\"top_p\\\":1,\\\"stop_sequences\\\":[\\\"\\\\n\\\\nHuman:\\\"],\\\"anthropic_version\\\":\\\"bedrock-2023-05-31\\\"}\"\n}\n\nTo streamline model interaction, Bedrock introduced the Converse API, providing a unified format. Now, you only need to specify:\n\nmodelId: The model identifier.\nprompt: The input text.\nmodel parameters: Settings like max_tokens_to_sample and temperature."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#aws---example",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#aws---example",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "AWS - Example",
    "text": "AWS - Example\nScenario: We want to ask an LLM about the symptoms of pneumonia using Anthropic’s Claude model on AWS Bedrock.\n\nDefine the User’s Question:  First, we create the user’s question, which we’ll call the user_message or query.\n# Start a conversation with the user message.\nuser_message= \"What are the symptoms of pneumonia?\"\nSpecify the Model:  Next, we choose the LLM we want to use. In this case, we’ll use Anthropic’s Claude 3 Haiku.\n# Set the model ID, e.g., Claude 3 Haiku.\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\nPrepare the Conversation Format:  The Bedrock API expects the input in a specific “conversation” format, where each message has a role (either “user” or “assistant”) and content.\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [{\"text\": user_message}],\n    }\n]\nDefine Inference Configuration:  We define the inference_config for the model, which includes parameters like maximum tokens and temperature.\ninference_config = {\n    \"maxTokens\": 512,\n    \"temperature\": 0.5,\n    \"topP\": 0.9,\n}\nSend the Request and Receive the Response:  We use the bedrock_runtime.converse function to send the conversation and inference_config to the specified model_id.\nbedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", \n                                region_name=\"us-east-1\")\n\nresponse =  bedrock_runtime.converse(\n    modelId=model_id,\n    messages=conversation,\n    inferenceConfig=inference_config\n    )\nExtract the Response Text:  We extract the model’s response text from the JSON response.\nresponse_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\nprint(response_text)\nEncapsulate in a Function:  We can create a function invoke_model to encapsulate these steps, making it easier to reuse (see code in ‘Putting it all together’ section). We’ll display the User Query and LLM Response in chat bubbles so it’s easier to follow the exchange of information.\n\n\ninvoke_model\n\n\n #Define a helper function to call the model and get a response\n def invoke_model(model_id, user_message, conversation=None, inference_config=None, verbose=False):\n     # # Set the model ID, e.g., Claude 3 Haiku.\n     # model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n\n     # # Start a conversation with the user message.\n     # user_message = \"Describe the python language.\"\n\n     if conversation is None:\n         conversation = [\n             {\n                 \"role\": \"user\",\n                 \"content\": [{\"text\": user_message}],\n             }\n         ]\n\n     # Set the inference configuration.\n     if inference_config is None:\n         inference_config = {\n             \"maxTokens\": 512,\n             \"temperature\": 0.5,\n             \"topP\": 0.9,\n         }\n\n     try:\n         # Send the message to the model, using a basic inference configuration.\n         response =  client.converse(\n             modelId=model_id,\n             messages=conversation,\n             inferenceConfig=inference_config\n         )\n\n         # Extract and print the response text.\n         response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n         if verbose:\n             print(f\"Request: {user_message}\")\n             print(f\"Response: {response_text}\")\n\n         return response_text\n\n     except (Exception) as e:\n         print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n         exit(1)\n\n\n\n # Start a conversation with the user message.\n user_message = user_query = \"What are the symptoms of pneumonia?\"\n\n #Pass the message to the model\n model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n response = model_response = invoke_model(model_id, user_message)\n\n\n\n #Display the user query and model response\n display_chat_bubble(user_query, sender=\"user\")\n display_chat_bubble(model_response, sender=\"model\")\n\n\n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                User Query \n                What are the symptoms of pneumonia?\n            \n        \n    \n    \n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                 LLM Response\n                The main symptoms of pneumonia include:\n\nCough - This is often the first symptom and may produce mucus or phlegm. The cough may be dry at first.\nFever and chills - Pneumonia often causes a high fever, sometimes over 102°F (39°C).\nDifficulty breathing - Breathing may be faster and shallower than normal. Patients may feel short of breath, especially with activity.\nChest pain - There is often a sharp or stabbing chest pain, especially when breathing deeply or coughing.\nFatigue - Pneumonia can cause significant tiredness and weakness.\n\nOther potential symptoms include:\n\nNausea and vomiting\nDiarrhea\nConfusion, especially in older adults\n\nThe specific symptoms can vary depending on the type of pneumonia, the causative agent, and the patient's age and overall health. Symptoms are usually more severe in the elderly, young children, and those with underlying medical conditions.\nSeeking medical attention is important, as pneumonia can be a serious and potentially life-threatening condition that requires prompt treatment."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#clinical-narrative-summarization",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#clinical-narrative-summarization",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Clinical Narrative Summarization",
    "text": "Clinical Narrative Summarization\n\n\n\nLLM workflow for clinical narrative summarization.\n\n\nOur goal is to get the model to summarize a clinical report.\nHere we’ll use a few-shot prompt to get the model to understand the task by showing it examples of the task. The task will be to summarize some medical text into:\n\nKey Findings\nRecommendations\n\nWe will provide the model with these examples:\n\nExample 1:\n\n\nINPUT: MRI of the right knee demonstrates a complex tear of the posterior horn and body of the medial meniscus. There is moderate joint effusion and mild chondromalacia of the patellofemoral compartment. The anterior and posterior cruciate ligaments are intact.\nOUTPUT: Key Findings: Complex tear of medial meniscus (posterior horn and body); Moderate joint effusion; Mild patellofemoral chondromalacia Recommendations: Orthopedic consultation; Consider arthroscopic evaluation and treatment\n\n\nExample 2: \n\n\nINPUT: Chest X-ray shows clear lung fields bilaterally with no evidence of consolidation, effusion, or pneumothorax. The heart size is within normal limits. No acute cardiopulmonary process identified.\nOUTPUT: Key Findings: Clear lung fields; Normal heart size; No acute cardiopulmonary process Recommendations: No further imaging needed at this time\n\n\nModel tasked to summarize text:\n\n\nINPUT: Patient presents with a 3-day history of high fever (103°F), chills, productive cough with yellow sputum, and pleuritic chest pain exacerbated by deep breathing. Physical exam reveals decreased breath sounds and crackles in the right lower lung field. ECG shows sinus tachycardia, but is otherwise unremarkable. Chest X-ray demonstrates consolidation in the right lower lobe, and a small right-sided pleural effusion. White blood cell count is elevated.\nOUTPUT:\n\n\nWhat do we expect?\nThe model should learn from the examples to extract ailment-related information (e.g., elevated WBC, high fever) and provide appropriate treatment recommendations.\nFor example an output like below would be consistent with what we expect:\n\nOUTPUT: Key Findings: Right lower lobe consolidation; Right-sided pleural effusion; High fever; Decreased breath sounds and crackles; Elevated WBC. Recommendations: Initiate broad-spectrum antibiotics for bacterial pneumonia; Consider sputum cultures; Monitor oxygen saturation; Pain management; Follow-up chest X-ray to assess resolution.\n\n\n\nModel Output\n\n\nCode\n# Few-shot learning example for medical report summarization\n\n# Define the examples and query\nexample_1 = \"\"\"\nINPUT: \nMRI of the right knee demonstrates a complex tear of the posterior horn and body of the medial meniscus. \nThere is moderate joint effusion and mild chondromalacia of the patellofemoral compartment. \nThe anterior and posterior cruciate ligaments are intact.\nOUTPUT:\n**Key Findings:** Complex tear of medial meniscus (posterior horn and body); Moderate joint effusion; Mild patellofemoral chondromalacia\n**Recommendations:** Orthopedic consultation; Consider arthroscopic evaluation and treatment\n\"\"\"\nexample_2 = \"\"\"\nINPUT:\nChest X-ray shows clear lung fields bilaterally with no evidence of consolidation, effusion, or pneumothorax. \nThe heart size is within normal limits. \nNo acute cardiopulmonary process identified.\nOUTPUT:\n**Key Findings:** Clear lung fields; Normal heart size; No acute cardiopulmonary process\n**Recommendations:** No further imaging needed at this time\n\"\"\"\n\nquery = \"\"\"\nINPUT:\nPatient presents with a 3-day history of high fever (103°F), chills, productive cough with yellow sputum, and pleuritic chest pain exacerbated by deep breathing. \nPhysical exam reveals decreased breath sounds and crackles in the right lower lung field. \nECG shows sinus tachycardia, but is otherwise unremarkable. \nChest X-ray demonstrates consolidation in the right lower lobe, and a small right-sided pleural effusion. White blood cell count is elevated.\n\"\"\"\n\n# Combine the examples and query into a single prompt\n# We'll use f-strings to insert the content from the examples and query into the full prompt.\nfew_shot_prompt = f\"\"\"\nTask: Summarize the following medical report into key findings and recommendations.\nFollow  the format of the examples strictly. Make the summary concise and actionable.\nThe key findings should be short and to the point.\nThe key findings should be less than 5 words.\nThe recommendations should be clear and specific.\nThe summary should be in the following format:\n**Key Findings:** [colon-separated list of key findings]\n**Recommendations:** [colon-separated list of recommendations]\n\n{example_1}\n{example_2}\nNow summarize this medical report:\n{query}\n\"\"\"\n\nprint(few_shot_prompt)\n\n\n\n\nCode\n#Pass the few-shot prompt to the model\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\nresponse = invoke_model(model_id, few_shot_prompt)\n\n\n\n\nCode\n#Display the user query and model response\ndisplay_chat_bubble(user_query, sender=\"user\")\ndisplay_chat_bubble(model_response, sender=\"model\")\n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                User Query \n                Task: Summarize the following medical report into key findings and recommendations.\nFollow  the format of the examples strictly. Make the summary concise and actionable.\nThe key findings should be short and to the point.\nThe key findings should be less than 5 words.\nThe recommendations should be clear and specific.\nThe summary should be in the following format:\nKey Findings: [colon-separated list of key findings]\nRecommendations: [colon-separated list of recommendations]\nINPUT: \nMRI of the right knee demonstrates a complex tear of the posterior horn and body of the medial meniscus. \nThere is moderate joint effusion and mild chondromalacia of the patellofemoral compartment. \nThe anterior and posterior cruciate ligaments are intact.\nOUTPUT:\nKey Findings: Complex tear of medial meniscus (posterior horn and body); Moderate joint effusion; Mild patellofemoral chondromalacia\nRecommendations: Orthopedic consultation; Consider arthroscopic evaluation and treatment\nINPUT:\nChest X-ray shows clear lung fields bilaterally with no evidence of consolidation, effusion, or pneumothorax. \nThe heart size is within normal limits. \nNo acute cardiopulmonary process identified.\nOUTPUT:\nKey Findings: Clear lung fields; Normal heart size; No acute cardiopulmonary process\nRecommendations: No further imaging needed at this time\nNow summarize this medical report:\nINPUT:\nPatient presents with a 3-day history of high fever (103°F), chills, productive cough with yellow sputum, and pleuritic chest pain exacerbated by deep breathing. \nPhysical exam reveals decreased breath sounds and crackles in the right lower lung field. \nECG shows sinus tachycardia, but is otherwise unremarkable. \nChest X-ray demonstrates consolidation in the right lower lobe, and a small right-sided pleural effusion. White blood cell count is elevated.\n            \n        \n    \n    \n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                 LLM Response\n                Key Findings: High fever; Productive cough; Right lower lobe consolidation; Pleural effusion; Elevated WBC\nRecommendations: Initiate antibiotic therapy; Consider chest CT scan; Monitor for clinical improvement\n            \n        \n    \n    \n\n\n\n\nWhat do we see?\nThe model’s output aligned with the prompt, providing contextually accurate Key Findings and Recommendations for the pneumonia scenario, and respecting the defined response parameters."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#diagnosis-generation",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#diagnosis-generation",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Diagnosis Generation",
    "text": "Diagnosis Generation\n\n\n\nLLM workflow for diagnosis generation.\n\n\nOur goal is to provide a preliminary medical diagnosis of a patient based on their symptoms.\nHere we’ll use a chain-of-thought prompting to guide the model through a systematic reasoning process.\nWe will define the following steps for the model to follow to arrive at the answer:\n\nStep 1: List potential diagnoses that could explain these symptoms.\nStep 2: For each potential diagnosis, evaluate the likelihood based on the symptom presentation.\nStep 3: Identify what additional tests or information would be most valuable for differential diagnosis.\nStep 4: Recommend a preliminary diagnosis and next steps.\n\nData provided to the model:\n\nPatient’s symptoms:\n\nPersistent cough for 3 weeks, after returning from a trip to a humid tropical country\nChest pain while breathing deeply\nLow-grade fever (99.5°F)\nFatigue and decreased appetite\n\n\n\nWhat do we expect?\nThe given symptoms are consistent with pneumonia (although it overlaps with other conditions). We would expect the model to consider pneumonia as a potential diagnosis and provide treatment recommendations consistent with pneumonia or other similar conditions. We’ll look at Step 2 to verify that pneumonia was considered in coming to a potential diagnosis.\n\n\nModel Output\n\n\nCode\n# Chain-of-Thought Prompting example for medical diagnosis\n\nsymptoms = \"\"\"\n- Persistent cough for 3 weeks, after returning from a trip to a humid tropical country.\n- Chest pain while breathing deeply\n- Low-grade fever (99.5°F)\n- Fatigue and decreased appetite\n\"\"\"\n\n\ncot_prompt = f\"\"\"\nYou are a medical expert. You are given the following patient's symptoms.\n{symptoms}\n\nFollow these steps to get to a preliminary diagnosis and recommendations:\nStep 1: List potential diagnoses that could explain these symptoms.\nStep 2: For each potential diagnosis, evaluate the likelihood based on the symptom presentation.\nStep 3: Identify what additional tests or information would be most valuable for differential diagnosis.\nStep 4: Recommend a preliminary diagnosis and next steps.\n\nYour response should be in the following format:   \n**Step 1:** [potential diagnoses]\n**Step 2:** [likelihood for each diagnosis]\n**Step 3:** [brief list of tests or information]\n**Step 4:** [preliminary diagnosis]\n\n**Diagnosis:** [diagnosis]\n**Likelihood:** [likelihood]\n**Next steps:** [next steps]\n\"\"\"\nprint(cot_prompt)\n\n\n\n\nCode\n#Pass the CoT prompt to the model\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\nresponse = invoke_model(model_id, cot_prompt)\n\n\n\n\nCode\n#Display the user query and model response\ndisplay_chat_bubble(user_query, sender=\"user\")\ndisplay_chat_bubble(model_response, sender=\"model\")\n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                User Query \n                You are a medical expert. You are given the following patient's symptoms.\n\nPersistent cough for 3 weeks, after returning from a trip to a humid tropical country.\nChest pain while breathing deeply\nLow-grade fever (99.5°F)\nFatigue and decreased appetite\n\nFollow these steps to get to a preliminary diagnosis and recommendations:\nStep 1: List potential diagnoses that could explain these symptoms.\nStep 2: For each potential diagnosis, evaluate the likelihood based on the symptom presentation.\nStep 3: Identify what additional tests or information would be most valuable for differential diagnosis.\nStep 4: Recommend a preliminary diagnosis and next steps.\nYour response should be in the following format: \nStep 1: [potential diagnoses]\nStep 2: [likelihood for each diagnosis]\nStep 3: [brief list of tests or information]\nStep 4: [preliminary diagnosis]\nDiagnosis: [diagnosis]\nLikelihood: [likelihood]\nNext steps: [next steps]\n            \n        \n    \n    \n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                 LLM Response\n                As a medical expert, I will follow the steps you provided to arrive at a preliminary diagnosis and recommendations for the given patient's symptoms.\nStep 1: Potential Diagnoses\n- Pneumonia\n- Tuberculosis\n- Fungal lung infection (e.g., histoplasmosis, coccidioidomycosis)\n- Lung cancer\n- Pulmonary embolism\nStep 2: Likelihood of Potential Diagnoses\n- Pneumonia: High likelihood, as the patient has a persistent cough, chest pain, and low-grade fever, which are common symptoms of pneumonia.\n- Tuberculosis: Moderate likelihood, as the patient has recently returned from a humid tropical country, which is a risk factor for tuberculosis.\n- Fungal lung infection: Moderate likelihood, as the patient has recently returned from a humid tropical country, which is a risk factor for certain fungal infections.\n- Lung cancer: Low likelihood, as the patient's symptoms are more indicative of an infectious or inflammatory process.\n- Pulmonary embolism: Low likelihood, as the patient does not have any known risk factors for this condition.\nStep 3: Additional Tests or Information\n- Chest X-ray or CT scan to evaluate the lungs and identify any abnormalities\n- Sputum culture or acid-fast bacilli (AFB) test to check for bacterial or mycobacterial infections\n- Blood tests, including complete blood count (CBC) and inflammatory markers (e.g., C-reactive protein, erythrocyte sedimentation rate)\n- Travel history and potential exposures during the trip to the humid tropical country\nStep 4: Preliminary Diagnosis and Next Steps\nPreliminary Diagnosis: Pneumonia\nLikelihood: High\nNext Steps:\n1. Order a chest X-ray or CT scan to assess the lungs and identify any signs of pneumonia or other pulmonary abnormalities.\n2. Obtain sputum samples for culture and AFB testing to rule out bacterial or mycobacterial infections, such as tuberculosis.\n3. Perform blood tests, including a CBC and inflammatory markers, to support the diagnosis of pneumonia and monitor the patient's response to treatment.\n4. Gather more information about the patient's recent travel history and potential exposures during the trip to the humid tropical country\n            \n        \n    \n    \n\n\n\n\nWhat do we see?\nWe see that the model was able to follow the steps we defined in order to get to a preliminary diagnosis and provide some next steps. Based on the symptoms the model identified that the patient has a high likelihood of having pneumonia as we expected."
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#clinical-classification",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#clinical-classification",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Clinical Classification",
    "text": "Clinical Classification\n\n\n\nLLM workflow for clinical classification.\n\n\nOur goal is to classify medical data into different categories using zero-shot prompting. Here we’ll pass the model a prompt with the category labels and the data to be classified.\nHere’s an example of what we’ll be passing to the model:\n\nmedical_text_1 = “Patient presents with a fever of 103.5 degrees Fahrenheit, accompanied by severe chills, rigors, body aches, and a persistent throbbing headache. The patient reports feeling disoriented, has a flushed face, and exhibits petechiae on the chest and extremities. White blood cell count is elevated. Patient reports recent lower backpain on their left side and some blood in their urine.”\ncategories_1 = [“Infection”, “Heat Stroke”, “Drug Reaction”]\n\nIn this case the classification should be Infection based on the information provided.\n\nWhat do we expect?\nIn each case we would expect that the model would recognize the classic symptoms presented in the clinical information. From there it should be able to link that information to one of the three classes provided, selecting the most likely class.\n\n\nModel Output\n\n\nCode\n# Zero-shot learning example for medical classification\n\n# Example 1: Fever with More Symptoms\nmedical_text_1 = \"\"\"\nPatient presents with a fever of 102.5 degrees Fahrenheit, accompanied by severe chills, body aches,\nand a persistent headache. The patient reports feeling disoriented and has a flushed face.\n\"\"\"\nmedical_text_1 =\" \".join(medical_text_1.splitlines())\ncategories_1 = [\"Infection\", \"Heat Stroke\", \"Drug Reaction\"]\n\n\n# Example 2: Broken Bone with Injury Details\nmedical_text_2 = \"\"\"\nPatient sustained a fall from a ladder, with immediate onset of severe pain and swelling in the lower leg. \nX-ray shows a complete, displaced fracture of the tibia, with evidence of soft tissue injury.\n\"\"\"\nmedical_text_2 =\" \".join(medical_text_2.splitlines())\ncategories_2 = [\"Fracture\", \"Sprain\", \"Muscle Strain\"]\n\n\n# Example 3: Fever, Cough, Nasal Congestion Symptoms\nmedical_text_3 = \"\"\"\nPatient presents with a sudden onset of high fever (102-104°F), accompanied by severe chills, \nintense muscle aches (myalgia), and a persistent, dry cough. The patient reports a severe headache, \nsignificant fatigue, and a sore throat. Nasal congestion is minimal. \nThe patient reports that many people in their workplace have been experiencing similar symptoms recently.\n\"\"\"\nmedical_text_3 =\" \".join(medical_text_3.splitlines())\ncategories_3 = [\"Common Cold\", \"Flu\", \"Allergies\"]\n\n\ndef zero_shot_classification_prompt(text, categories):\n    import textwrap\n\n    # Combine the examples into a single prompt\n    prompt = (\n    f\"Task: Classify the following medical cases into the correct category.\\n\"\n    f\"Choose the most appropriate category from the list provided: {', '.join(categories)}\\n\"\n    f\"\\n\"\n    f\"Text: {text}\\n\"\n    f\"\\n\"\n    f\"Response format:\\n\"\n    f\"**Category:** [selected category]\\n\"\n    f\"**Confidence:** [high/medium/low]\\n\"\n    f\"**Reasoning:** [brief explanation]\"\n)\n    return prompt\n\nzero_shot_prompt = zero_shot_classification_prompt(text = medical_text_1, categories = categories_1)\nprint(zero_shot_prompt)\n\n\n\n\n● Example 1\n\n\nCode\n#Pass the zero-shot prompt to the model\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\nzero_shot_prompt = zero_shot_classification_prompt(text = medical_text_1, categories = categories_1)\nresponse = invoke_model(model_id, zero_shot_prompt)\n\n\n\n\nCode\n#Display the user query and model response\ndisplay_chat_bubble(user_query, sender=\"user\")\ndisplay_chat_bubble(model_response, sender=\"model\")\n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                User Query \n                Task: Classify the following medical cases into the correct category.\nChoose the most appropriate category from the list provided: Infection, Heat Stroke, Drug Reaction\nText:  Patient presents with a fever of 102.5 degrees Fahrenheit, accompanied by severe chills, body aches, and a persistent headache. The patient reports feeling disoriented and has a flushed face.\nResponse format:\nCategory: [selected category]\nConfidence: [high/medium/low]\nReasoning: [brief explanation]\n            \n        \n    \n    \n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                 LLM Response\n                Category: Infection\nConfidence: High\nReasoning: The patient's symptoms, including fever, chills, body aches, and headache, are typical of an infection. The disorientation and flushed face could also be indicative of an underlying infection, rather than heat stroke or a drug reaction.\n            \n        \n    \n    \n\n\n\n\n● Example 2\n\n\nCode\n#Pass the zero-shot prompt to the model\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\nzero_shot_prompt = zero_shot_classification_prompt(text = medical_text_2, categories = categories_2)\nresponse = invoke_model(model_id, zero_shot_prompt)\n\n\n\n\nCode\n#Display the user query and model response\ndisplay_chat_bubble(user_query, sender=\"user\")\ndisplay_chat_bubble(model_response, sender=\"model\")\n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                User Query \n                Task: Classify the following medical cases into the correct category.\nChoose the most appropriate category from the list provided: Fracture, Sprain, Muscle Strain\nText:  Patient sustained a fall from a ladder, with immediate onset of severe pain and swelling in the lower leg.  X-ray shows a complete, displaced fracture of the tibia, with evidence of soft tissue injury.\nResponse format:\nCategory: [selected category]\nConfidence: [high/medium/low]\nReasoning: [brief explanation]\n            \n        \n    \n    \n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                 LLM Response\n                Category: Fracture\nConfidence: High\nReasoning: The given description indicates that the patient sustained a fall from a ladder, resulting in severe pain and swelling in the lower leg. The X-ray findings show a complete, displaced fracture of the tibia, which is a clear indication of a fracture. The presence of soft tissue injury further supports the diagnosis of a fracture.\n            \n        \n    \n    \n\n\n\n\n● Example 3\n\n\nCode\n#Pass the zero-shot prompt to the model\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\nzero_shot_prompt = zero_shot_classification_prompt(text = medical_text_3, categories = categories_3)\nresponse = invoke_model(model_id, zero_shot_prompt)\n\n\n\n\nCode\n#Display the user query and model response\ndisplay_chat_bubble(user_query, sender=\"user\")\ndisplay_chat_bubble(model_response, sender=\"model\")\n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                User Query \n                Task: Classify the following medical cases into the correct category.\nChoose the most appropriate category from the list provided: Common Cold, Flu, Allergies\nText:  Patient presents with a sudden onset of high fever (102-104°F), accompanied by severe chills,  intense muscle aches (myalgia), and a persistent, dry cough. The patient reports a severe headache,  significant fatigue, and a sore throat. Nasal congestion is minimal.  The patient reports that many people in their workplace have been experiencing similar symptoms recently.\nResponse format:\nCategory: [selected category]\nConfidence: [high/medium/low]\nReasoning: [brief explanation]\n            \n        \n    \n    \n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                 LLM Response\n                Category: Flu\nConfidence: High\nReasoning: The patient's symptoms, including high fever, severe chills, intense muscle aches, persistent dry cough, headache, fatigue, and sore throat, are characteristic of the flu (influenza) rather than a common cold or allergies. The sudden onset of these symptoms and the report of similar cases in the patient's workplace further support the diagnosis of the flu.\n            \n        \n    \n    \n\n\n\n\nWhat do we see?\nAs anticipated, the model accurately classified symptoms across all cases, demonstrating its ability to recognize clinical indications from diverse domains, including:\n\nInfectious disease (Example 1)\nOrthopedic trauma (Example 2)\nViral illness (Example 3)"
  },
  {
    "objectID": "projects/Short_dive_posts/aws/aws-analysis.html#structured-data-extraction",
    "href": "projects/Short_dive_posts/aws/aws-analysis.html#structured-data-extraction",
    "title": "Building LLM Clinical Data Analytics Pipelines with AWS Bedrock",
    "section": "Structured Data Extraction",
    "text": "Structured Data Extraction\n\n\n\nLLM workflow for structured data extraction.\n\n\nOur goal is to use the LLM to extract specific information from medical data, guided by XML tags.\nXML’s (Extensible Markup Language) structured format is key here, making it easier for the LLM to understand the data’s organization. We use tags like &lt;instructions&gt; and &lt;example&gt; to provide direct guidance and illustrative examples. We’ll use XML tags along with one-shot learning to show the LLMs how to extract and format clinical data to be returned as an output.\nFor example we can use XML tags to indicate where each part of the prompt begins and ends. The example below defines 3 main parts:\n\nThe &lt;instruction&gt; or task that we want the model to perform.\nThe input data that we want the model to use, in this case a &lt;clinical_note&gt;.\nThe format that we want returned for the extracted data. Defined as &lt;output_schema&gt;.\n\n\n&lt;instructions&gt; Extract the patient’s name, age, and diagnosis from the following clinical note. &lt;/instructions&gt;\n&lt;clinical_note&gt; Patient: John Smith, 67 years old, presented with symptoms of acute myocardial infarction. &lt;/clinical_note&gt;\n&lt;output_schema&gt; &lt;patient_name&gt; &lt;/patient_name&gt; &lt;patient_age&gt; &lt;/patient_age&gt; &lt;diagnosis&gt; &lt;/diagnosis&gt; &lt;/output_schema&gt;\n\n\nWhat do we expect?\nWe expect that the model would be able to accurately follow the instructions given, effectively analyze the input data, correctly extract and format the data given the formatting guide.\nFor example, for the given input above we would expect the model to return:\n\n&lt;output_schema&gt; &lt;patient_name&gt; John Smith &lt;/patient_name&gt; &lt;patient_age&gt; 67 &lt;/patient_age&gt; &lt;diagnosis&gt; Acute Myocardial Infarction &lt;/diagnosis&gt; &lt;/output_schema&gt;\n\n\n\nModel Output\n\n\nCode\nclinical_note = \"\"\"\nTodays date is March 12th, 2025.\nPatient, Marty K. Martin, is a 62-year-old male with a history of hypertension, type 2 diabetes, and hyperlipidemia. \nCurrently taking Lisinopril 20mg daily, Metformin 1000mg BID, and Atorvastatin 40mg at bedtime. \nPatient reports having a coronary artery bypass graft (CABG) 5 years ago.\nKnown allergies to penicillin (hives) and sulfa drugs (rash).\nVitals on admission: BP 142/88, HR 78, RR 16, Temp 98.6°F, SpO2 97% on room air.\nRecent HbA1c was 7.2% down from 7.8%. Patient scheduled for routine colonoscopy next month. \nPatient celebrated his 62nd birthday on Tuesday, March 8th, 2025.\nMedical record number: 87654321\n\"\"\"\nclinical_note =\" \".join(clinical_note.splitlines())\n\nxml_prompt = f\"\"\"\nYou're a medical records analyst at a Clinical Electronics Health Records company. \nGenerate a comprehensive patient summary for our medical team.\nOur hospital uses structured data formats for efficient information transfer between departments. \nThe medical team needs clear, concise, and actionable patient information.\nUse this clinical note for your summary:\n&lt;clinical_note&gt;\n{clinical_note}\n&lt;/clinical_note&gt;\n&lt;instructions&gt;\n1. Extract patient information including: Patient Name, MRN,  DOB (ONLY calculate DOB if enough information is given)\n2. Extract and organize: Medications, Allergies, Vital Signs, and Recent Procedures\n3. Highlight critical values and potential drug interactions\n4. Format as a nested table structure for easy reference\n&lt;/instructions&gt;\nPlease include all relevant information and maintain the nested table structure in your summary.\nReplace missing data with N/A.\nMake your tone clinical and precise. \nFollow this structure exactly and only return the formatted output:\n&lt;formatting_example&gt;\n**PATIENT SUMMARY**\n**Patient**: John Doe \n**MRN**: 12345678 \n**DOB**: 01/15/1965\n**Exam Date**: 06/12/2020\n**MEDICATIONS**\n\n| Medication | Dosage | Frequency | Route | Indication |\n|------------|--------|-----------|-------|------------|\n| Lisinopril | 20mg   | Daily     | PO    | HTN        |\n\n**ALLERGIES**\n\n| Allergen   | Reaction  | Severity  |\n|------------|-----------|-----------|\n| Penicillin | Urticaria | Moderate  |\n\n**VITAL SIGNS** (Date: 03/15/2025)\n\n| Parameter       | Value      | Status    |\n|-----------------|------------|-----------|\n| BP              | 132/78     | WNL       |\n| Heart Rate      | 88 bpm     | WNL       |\n| Temperature     | 99.1°F     | ELEVATED  |\n| Oxygen Sat      | 97%        | WNL       |\n\n**LAB VALUES**\n\n| Test  | Value | Units | Date      |\n|-------|-------|-------|-----------|\n|Glucose| 70    | mg/dL | 12/12/2024|\n\n**RECENT & SCHEDULED PROCEDURES**\n\n| Procedure     | Date       | Provider | Key Findings |\n|---------------|------------|----------|--------------|\n| Heart Surgery | Next month | Dr. Smith| N/A          |\n\n&lt;/formatting_example&gt;\n\"\"\"\n\n\n\n\nCode\n#Pass the few-shot prompt to the model\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\nresponse = invoke_model(model_id, xml_prompt)\n\n\n\n\nCode\n#Display the user query and model response\ndisplay_chat_bubble(user_query, sender=\"user\")\ndisplay_chat_bubble(model_response, sender=\"model\")\n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                User Query \n                You're a medical records analyst at a Clinical Electronics Health Records company. \nGenerate a comprehensive patient summary for our medical team.\nOur hospital uses structured data formats for efficient information transfer between departments. \nThe medical team needs clear, concise, and actionable patient information.\nUse this clinical note for your summary:\n\n Todays date is March 12th, 2025. Patient, Marty K. Martin, is a 62-year-old male with a history of hypertension, type 2 diabetes, and hyperlipidemia.  Currently taking Lisinopril 20mg daily, Metformin 1000mg BID, and Atorvastatin 40mg at bedtime.  Patient reports having a coronary artery bypass graft (CABG) 5 years ago. Known allergies to penicillin (hives) and sulfa drugs (rash). Vitals on admission: BP 142/88, HR 78, RR 16, Temp 98.6°F, SpO2 97% on room air. Recent HbA1c was 7.2% down from 7.8%. Patient scheduled for routine colonoscopy next month.  Patient celebrated his 62nd birthday on Tuesday, March 8th, 2025. Medical record number: 87654321\n\n\n1. Extract patient information including: Patient Name, MRN,  DOB (ONLY calculate DOB if enough information is given)\n2. Extract and organize: Medications, Allergies, Vital Signs, and Recent Procedures\n3. Highlight critical values and potential drug interactions\n4. Format as a nested table structure for easy reference\n\nPlease include all relevant information and maintain the nested table structure in your summary.\nReplace missing data with N/A.\nMake your tone clinical and precise. \nFollow this structure exactly and only return the formatted output:\n\nPATIENT SUMMARY\nPatient: John Doe \nMRN: 12345678 \nDOB: 01/15/1965\nExam Date: 06/12/2020\nMEDICATIONS\n\n\n\n\nMedication\nDosage\nFrequency\nRoute\nIndication\n\n\n\n\nLisinopril\n20mg\nDaily\nPO\nHTN\n\n\n\nALLERGIES\n\n\n\nAllergen\nReaction\nSeverity\n\n\n\n\nPenicillin\nUrticaria\nModerate\n\n\n\nVITAL SIGNS (Date: 03/15/2025)\n\n\n\nParameter\nValue\nStatus\n\n\n\n\nBP\n132/78\nWNL\n\n\nHeart Rate\n88 bpm\nWNL\n\n\nTemperature\n99.1°F\nELEVATED\n\n\nOxygen Sat\n97%\nWNL\n\n\n\nLAB VALUES\n\n\n\nTest\nValue\nUnits\nDate\n\n\n\n\nGlucose\n70\nmg/dL\n12/12/2024\n\n\n\nRECENT & SCHEDULED PROCEDURES\n\n\n\nProcedure\nDate\nProvider\nKey Findings\n\n\n\n\nHeart Surgery\nNext month\nDr. Smith\nN/A\n\n\n\n\n\n            \n        \n    \n    \n\n\n\n    \n    \n        \n        \n        Chat Bubble\n    \n    \n        \n            \n                 LLM Response\n                \nPATIENT SUMMARY\nPatient: Marty K. Martin\nMRN: 87654321\nDOB: 03/08/1963\nExam Date: 03/12/2025\nMEDICATIONS\n\n\n\n\nMedication\nDosage\nFrequency\nRoute\nIndication\n\n\n\n\nLisinopril\n20mg\nDaily\nPO\nHypertension\n\n\nMetformin\n1000mg\nBID\nPO\nDiabetes\n\n\nAtorvastatin\n40mg\nBedtime\nPO\nHyperlipidemia\n\n\n\nALLERGIES\n\n\n\nAllergen\nReaction\nSeverity\n\n\n\n\nPenicillin\nHives\nModerate\n\n\nSulfa Drugs\nRash\nModerate\n\n\n\nVITAL SIGNS (Date: 03/12/2025)\n\n\n\nParameter\nValue\nStatus\n\n\n\n\nBlood Pressure\n142/88\nELEVATED\n\n\nHeart Rate\n78 bpm\nWNL\n\n\nTemperature\n98.6°F\nWNL\n\n\nOxygen Saturation\n97%\nWNL\n\n\nRespiratory Rate\n16 bpm\nWNL\n\n\n\nLAB VALUES\n\n\n\nTest\nValue\nUnits\nDate\n\n\n\n\nHbA1c\n7.2\n%\n03/12/2025\n\n\n\nRECENT & SCHEDULED PROCEDURES\n\n\n\nProcedure\nDate\nProvider\nKey Findings\n\n\n\n\nCoronary Artery Bypass Graft (CABG)\n5 years ago\nN/A\nN/A\n\n\nColonoscopy\nNext month\nN/A\nN/A\n\n\n\n\n\n            \n        \n    \n    \n\n\n\n\nWhat do we see?\nWe see that the model was able to successfully extract all the relevant information from the provided text. This was after several rounds of refining the input prompt. After extracting the relevant information, the model followed the formatting example and correctly formatted the output data.\nThere are a few interesting things to note in the model’s output:\n\nAlthough the term MRN does not explicitly appear in the text, the model identified that MRN stood for Medical Record Number and pulled out the listed number.\nThe patient’s DOB was not explicitly mentioned but context clues were given to figure out the patient’s DOB like:\n\nPatient, Marty K. Martin, is a 62-year-old\nPatient celebrated his 62nd birthday on Tuesday, March 8th, 2025\n\nThe model was able to correctly match each of the medications to their indications:\n\nLisinopril –&gt; Hypertension\nMetformin –&gt; Diabetes\nAtorvastatin –&gt; Hyperlipidemia\n\nWithin the vital signs table, the model correctly identified that the blood pressure reported was ELEVATED (typical 120/80)\n\nThe patient’s HBA1C was correctly extracted and the units were correctly identified\nBoth the past and upcoming procedures for the patient were correctly identified and extracted with an informative date assigned."
  },
  {
    "objectID": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#norm-notation",
    "href": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#norm-notation",
    "title": "A Brief Guide to Vector Norms in Machine Learning",
    "section": "Norm Notation",
    "text": "Norm Notation\nNorms are typically denoted with double vertical lines around a transformation being applied (here a \\(\\cdot\\) is used as a space holder) and the norm type as a subscript on the outside (\\(p\\), where \\(p\\) can be \\(p \\geq 0, 1, 2, 3, ...,\\infty\\)). If \\(p\\) is not explicitly given, the norm is assumed to be \\(p=2\\) since it is the most commonly used norm. Norms can also be denoted with either a capital letter \\(L\\) or a lower case \\(\\ell\\) with the letter 𝑝 as either a superscript (\\(L^p\\)) or subscript (\\(L_p\\)) .\n\\[\nL_p = \\ell_p = \\Vert \\cdot \\Vert_p\n\\]\nTypical transformations can be:\n\nAbsolute value  —   \\(|\\mathbf{x}|\\)\nSquaring  —   \\(|\\mathbf{x}|^2\\)\nMin/Max operations  —   \\(min(|\\mathbf{x}|)\\) or \\(max(|\\mathbf{x}|)\\)"
  },
  {
    "objectID": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_1-norm-manhattan-norm",
    "href": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_1-norm-manhattan-norm",
    "title": "A Brief Guide to Vector Norms in Machine Learning",
    "section": "\\(L_1\\) Norm (Manhattan Norm)",
    "text": "\\(L_1\\) Norm (Manhattan Norm)\nThe \\(L_1\\) norm or \\(\\ell_1\\), also known as Manhattan norm or Taxicab norm, is defined as the sum of the absolute values of the vector components. It’s referred to as the Manhattan norm because distance is calculated in a gridwise format similar to the gridded blocks in Manhattan.\n\n\\[\n\\textcolor{teal}{L_1 := ||\\mathbf{x}||_{p=1} := \\sum_{i=1}^n  |x_i|}\n\\]\n\n\\[\nor\n\\] \\[\n\\begin{align*}\n||\\mathbf{x}||_{p=1} & :=  \\left[ \\sum_{i=1}^n  |x_i|^1 \\right]^\\frac{1}{1}\n\\end{align*}\n\\]\n\\[\nor\n\\]\n\\[\n||\\mathbf{x}||_{p=1} := |x_1|^1 + |x_2|^1 + |x_3|^1 + ... + |x_N|^1\n\\]\n\\[\nor\n\\]\n\\[\n||\\mathbf{x}||_{p=1} := |x_1| + |x_2| + |x_3| + ... + |x_N|\n\\]\n\\(\\textcolor{darkorange}{Note}\\): A number to the power of 1 is just that number returned (i.e. \\(3^1 = 3\\), \\(x^1 = x\\)). Here the powers are added for demonstration and later generalization.\n\n\n\\(\\cdot\\) Example: \\(L_1\\) Norm\n\n\nExample\n\n\n1. \\(L_1\\) Example\nGiven a vector \\(\\mathbf{x} = [3,4]\\), compute the \\(\\ell_\\textcolor{red}1\\) norm.\n\\[\n||\\mathbf{x}||_{1} := \\sum_{i=1}^n  |x_i| := \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}1 \\right]^\\frac{1}{\\textcolor{red}1}\n\\] \\[\n\\begin{align*}\n||\\mathbf{x}||_{1} & = \\left[ \\sum_{i=1}^{n=2}  |x_i|^\\textcolor{red}1 \\right]^\\frac{1}{\\textcolor{red}1} \\\\\n& = (|3|^\\textcolor{red}1 + |4|^\\textcolor{red}1)^\\frac{1}{\\textcolor{red}1}\\\\\n& = ( 3 + 4)^\\frac{1}{\\textcolor{red}1}\\\\\n& = ( 7)^1 \\\\\n& = 7\n\\end{align*}\n\\]\n\\[\nor\n\\] \\[\n||\\mathbf{x}||_{1} = |3| + |4| =  3 + 4 = 7\n\\]\n\nimport numpy as np\nfrom numpy.linalg import norm\n\nnorm_type = 1\nx = np.array([3,4])\n\nnorm_l1 = np.sum(np.abs(x))\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 7\n\nnorm_l1 = norm(x, 1)\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 7\n\n\\(\\displaystyle L_{1}-norm: 7\\)\n\n\n\\(\\displaystyle L_{1}-norm: 7.0\\)\n\n\n\n\n2. Distance Example\nGiven two vectors compute their Manhattan distance. \\(\\mathbf{u} = [1, 2, 3]\\) \\(\\mathbf{v} = [4, 5, 6]\\)\nSteps:\n\nCompute the error\nFind the absolute value\nSum all elements\n\n\\[\n\\begin{align*}\n||\\mathbf{x}||_{1} & = |1-4| + |2-5| + |3-6| \\\\\n& = |-3| + |-3| + |-3| \\\\\n& = 3 + 3 + 3 \\\\\n& = 9\\\\\n\\end{align*}\n\\]\n\nimport numpy as np\nfrom numpy.linalg import norm\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nnorm_type = 1\nx = np.subtract(u,v)\n\nnorm_l1 = np.sum(np.abs(x))\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 9\n\nnorm_l1 = norm(x, 1)\nprint_result(type_in=norm_type, val=norm_l1)\n# norm_l1 : 9\n\n\\(\\displaystyle L_{1}-norm: 9\\)\n\n\n\\(\\displaystyle L_{1}-norm: 9.0\\)"
  },
  {
    "objectID": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_2-norm-euclidean-norm",
    "href": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_2-norm-euclidean-norm",
    "title": "A Brief Guide to Vector Norms in Machine Learning",
    "section": "\\(L_2\\) Norm (Euclidean Norm)",
    "text": "\\(L_2\\) Norm (Euclidean Norm)\nThe \\(L_2\\) norm or \\(\\ell_2\\), also known as the Euclidean norm is defined as the square root of the sum of squared vector components.\n\n\\[\n\\textcolor{teal}{L_2 := ||\\mathbf{x}||_{p=2} := \\sqrt[2]{\\sum_{i=1}^n  |x_i|^2 }  }\n\\]\n\n\\[\nor\n\\]\n\\[\n||\\mathbf{x}||_{p=2} :=  \\left[ \\sum_{i=1}^n  |x_i|^2 \\right]^\\frac{1}{2}\n\\]\n\\[\nor\n\\]\n\\[\n||\\mathbf{x}||_{p=2} := (|x_1|^2 + |x_2|^2 + |x_3|^2 + ... + |x_N|^2)^\\frac{1}{2}\n\\]\n\\(\\textcolor{darkorange}{Note}\\): Values raised to the power of a fraction are equivalent to the \\(n^{th}\\) root (i.e. \\(x^\\frac{1}{n} = \\sqrt[n]{x}\\) or \\(25^\\frac{1}{2} = \\sqrt[2]{25} = 5\\)). Here the powers are added for demonstration and later generalization.\n\n\n\\(\\cdot\\) Example: \\(L_2\\) Norm\n\n\nExample\n\n\n1. \\(L_2\\) Example\nGiven a vector \\(\\mathbf{x} = [3,4]\\), compute the \\(\\ell_\\textcolor{red}2\\) norm\n\\[\n||\\mathbf{x}||_{p=\\textcolor{red}2} = \\sqrt[\\textcolor{red}2]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}2 } : =  \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}2 \\right]^\\frac{1}{\\textcolor{red}2}\n\\] \\[\n\\begin{align*}\n||\\mathbf{x}||_{\\textcolor{red}{2}} & = \\sqrt[\\textcolor{red}{2}]{ |3|^\\textcolor{red}{2} + |4|^\\textcolor{red}{2} }\\\\\n& = \\sqrt[\\textcolor{red}{2}]{ 9 + 16 }\\\\\n& = \\sqrt[\\textcolor{red}{2}]{25} \\\\\n& = 5\n\\end{align*}\n\\]\n\nimport numpy as np\nfrom numpy.linalg import norm\n\nnorm_type = 2\nx = np.array([3,4])\n\nnorm_l2 = np.sqrt(np.sum(x**2))\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5\n\nnorm_l2 = norm(x, 2)\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5 \n\n\\(\\displaystyle L_{2}-norm: 5.0\\)\n\n\n\\(\\displaystyle L_{2}-norm: 5.0\\)\n\n\n\n\n2. Distance Example\nGiven two vectors compute their Euclidian distance. \\(\\mathbf{u} = [1, 2, 3]\\) \\(\\mathbf{v} = [4, 5, 6]\\)\nSteps:\n\nCompute the error\nFind the absolute value, squared\nSum all elements\nCompute the square root\n\n\\[\n\\begin{align*}\n||\\mathbf{x}||_{\\textcolor{red}{2}} & = (|1-4|^\\textcolor{red}{2} + |2-5|^\\textcolor{red}{2} + |3-6|^\\textcolor{red}{2})^{\\frac{1}{\\textcolor{red}{2}}} \\\\\n& = (|-3|^\\textcolor{red}{2} + |-3|^\\textcolor{red}{2} + |-3|^\\textcolor{red}{2})^{\\frac{1}{\\textcolor{red}{2}}} \\\\\n& = (9 + 9 + 9)^{\\frac{1}{\\textcolor{red}{2}}} \\\\\n& = (27)^{\\frac{1}{\\textcolor{red}{2}}}\\\\\n& = 5.196\\\\\n\\end{align*}\n\\]\n\nimport numpy as np\nfrom numpy.linalg import norm\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nnorm_type = 2\nx = np.subtract(u,v)\n\nnorm_l2 = np.sqrt(np.sum(x**2))\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5.196\n\nnorm_l2 = norm(x, 2)\nprint_result(type_in=norm_type, val=norm_l2)\n# norm_l2 : 5.196\n\n\\(\\displaystyle L_{2}-norm: 5.196152422706632\\)\n\n\n\\(\\displaystyle L_{2}-norm: 5.196152422706632\\)"
  },
  {
    "objectID": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_infty-norm-max-norm",
    "href": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_infty-norm-max-norm",
    "title": "A Brief Guide to Vector Norms in Machine Learning",
    "section": "\\(L_\\infty\\) Norm (Max Norm)",
    "text": "\\(L_\\infty\\) Norm (Max Norm)\nThe \\(L_\\infty\\) norm or \\(\\ell_\\infty\\), also known as the Max norm is defined as the maximum value of a vector. It is sometimes referred to as Chebyshev distance or Chebyshev norm. It is useful when the largest deviation is of interest like in outlier or anomaly detection.\n\n\\[\n\\textcolor{teal}{ \\\\\n||\\mathbf{x}||_{p=\\infty} := {max_{1 \\leq i \\leq N}} |x_i| \\\\\n}\n\\]\n\n\n\n\\(\\cdot\\)Example: \\(L_\\infty\\) Norm\n\n\nExample\n\n\n1. \\(L_\\infty\\) Example\nGiven a vector \\(\\mathbf{x} = [3,4]\\), compute the \\(\\ell_\\infty\\) norm\n\\[\n||\\mathbf{x}||_{p=\\infty} = {max_{1 \\leq i \\leq N}} |x_i|\n\\] \\[\n||\\mathbf{x}||_{\\infty} := max [|3|, |4|] := 4\n\\]\n\nimport numpy as np\nfrom numpy.linalg import norm\n\nnorm_type = '\\infty'\nx = np.array([3,4])\n\nnorm_linf = np.max(np.abs(x))\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 4\n\nnorm_linf = norm(x, np.inf)\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 4\n\n\\(\\displaystyle L_{\\infty}-norm: 4\\)\n\n\n\\(\\displaystyle L_{\\infty}-norm: 4.0\\)\n\n\n\n\n2. Distance Example\nGiven two vectors compute their \\(L_\\infty\\) distance. \\(\\mathbf{u} = [1, 2, 3]\\) \\(\\mathbf{v} = [4, 5, 6]\\)\nSteps:\n\nCompute the error\nFind the absolute value\nFind the maximum value of all elements\n\n\\[\n\\begin{align*}\n||\\mathbf{x}||_{1} & = max [|1-4| , |2-5| , |3-6|] \\\\\n& = max [|-3| , |-3| , |-3|] \\\\\n& = max [3 , 3 , 3] \\\\\n& = 3\n\\end{align*}\n\\]\n\nimport numpy as np\nfrom numpy.linalg import norm\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nnorm_type = '\\infty'\nx = np.subtract(u,v)\n\nnorm_linf = np.max(np.abs(x))\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 3\n\nnorm_linf = norm(x, np.inf)\nprint_result(type_in=norm_type, val=norm_linf)\n# norm_linf : 3\n\n\\(\\displaystyle L_{\\infty}-norm: 3\\)\n\n\n\\(\\displaystyle L_{\\infty}-norm: 3.0\\)"
  },
  {
    "objectID": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_p-norm-generalized-norm",
    "href": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#l_p-norm-generalized-norm",
    "title": "A Brief Guide to Vector Norms in Machine Learning",
    "section": "\\(L_p\\) Norm (Generalized Norm)",
    "text": "\\(L_p\\) Norm (Generalized Norm)\nNorms can be generalized to the \\(L_p\\) or \\(\\ell_p\\) form. This generalized form allows for the computation of any \\(p\\) norms.\n\n\\[\n\\textcolor{teal}{ \\\\\nL_p := ||\\mathbf{x}||_{p} := \\sqrt[p]{\\sum_{i=1}^n  |x_i|^p} := \\left[ \\sum_{i=1}^n  |x_i|^p   \\right]^\\frac{1}{p} \\\\\n}\n\\]\n\n\nRules of norms:\n\nNon-Negativity: Norms are non-negative values. It makes sense since they are computed to get the length or size of a vector or matrix. \\[\n||\\mathbf{u}||_{p} \\geq 0\n\\]\nDefiniteness: Norms are only 0 if the vector being measured is a zero vector (it has a length of 0).\n\n\\[\n||\\mathbf{u}||_{p} = 0   \\quad only \\quad if \\quad \\mathbf{u} = 0\n\\]\n\nTriangle inequality: The norm of the sum of two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) (||𝐮+𝐯||) is not more than the sum of their norms (||𝐮|| + ||𝐯||). This is similar to the Pythagorean Theorem where \\(||\\mathbf{u}||\\) and \\(||\\mathbf{v}||\\) are the sides of a triangle and \\(||\\mathbf{u} + \\mathbf{v} ||\\) is the hypotenuse of the triangle. It is sometimes referred to as Minkowski’s Inequality. Mathematically this is: \\[\n||\\mathbf{u} + \\mathbf{v} ||_p \\leq ||\\mathbf{u} ||_p + ||\\mathbf{v}||_p\n\\]\nHomogeneity: If you multiply a vector \\(\\mathbf{u}\\) by a scalar \\(k\\) and compute the norm its equivalent to multiplying the norm of the vector \\(\\mathbf{u}\\) by the absolute value of that scalar \\(k\\):\n\n\\[\n||k\\mathbf{u} || = |k| ||\\mathbf{u} ||\n\\]\n\n\n\\(\\cdot\\) Example: \\(L_p\\) Norms\n\n\nExample\n\n\n1. \\(L_3\\) Norm\nGiven a vector \\(\\mathbf{x} = [3,4]\\), compute the \\(\\ell_\\textcolor{red}3\\) norm \\[\n||\\mathbf{x}||_{p=\\textcolor{red}3} = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}3 \\right]^\\frac{1}{\\textcolor{red}3}  :=  \\sqrt[\\textcolor{red}3]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}3}\n\\] \\[\n\\begin{align*}\n||\\mathbf{x}||_{3} & = \\sqrt[3]{ |3|^3 + |4|^3 } \\\\\n& = \\sqrt[3]{ 27 + 64 }\\\\\n& = \\sqrt[3]{91}\\\\\n& = {91}^\\frac{1}{3}\\\\\n& = 4.498\n\\end{align*}\n\\]\n\nimport numpy as np\n\nx = np.array([3,4])\n\np=3\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n\n\\(\\displaystyle L_{3}-norm: 4.497941445275415\\)\n\n\n\n\n2. \\(L_4\\) Norm\nGiven a vector \\(\\mathbf{x} = [3,4]\\), compute the \\(\\ell_\\textcolor{red}4\\) norm \\[\n||\\mathbf{x}||_{p=\\textcolor{red}4} = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}4 \\right]^\\frac{1}{\\textcolor{red}4}  :=  \\sqrt[\\textcolor{red}4]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}4}\n\\] \\[\n\\begin{align*}\n||\\mathbf{x}||_{4} & = \\sqrt[4]{ |3|^4 + |4|^4 }\\\\\n& = \\sqrt[4]{ 81 + 256 }\\\\\n& = \\sqrt[4]{337}\\\\\n& = {337}^\\frac{1}{4}\\\\\n& =  4.285\n\\end{align*}\n\\]\n\nimport numpy as np\n\nx = np.array([3,4])\n\np=4\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n\n\\(\\displaystyle L_{4}-norm: 4.284572294953817\\)"
  },
  {
    "objectID": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#other-l_p-norms",
    "href": "projects/Short_dive_posts/ML_reg_norm/ML_reg_norm.html#other-l_p-norms",
    "title": "A Brief Guide to Vector Norms in Machine Learning",
    "section": "Other \\(L_p\\) “Norms”",
    "text": "Other \\(L_p\\) “Norms”\nWhat happens to norms when 0 ≤ p&lt;1?\nWell, we get a calculated length back, however these cases are not strictly norms (hence “norms”) since we get a violation of the triangle inequality (0&lt;p&lt;1) or homogeneity (p=0).\n\n\\(L_p\\) for \\(0&lt;p&lt;1\\) Norm\nWhat happens when \\(p\\) is between \\(0\\) and \\(1\\) (\\(0&lt;p&lt;1\\))?\nIn this case (\\(0&lt;p&lt;1\\)) the triangle inequality is violated. The means that, strictly speaking, \\(L_{0&lt;p&lt;1}\\) is not actually a norm but it still returns a measurement of size of a vector. The same generalized formula can be used:\n\n\\[\n\\textcolor{teal}{ \\\\\nL_{0 \\lt p \\lt 1} := ||\\mathbf{x}||_{0 \\lt p \\lt 1} := \\sqrt[p]{\\sum_{i=1}^n  |x_i|^p} := \\left[ \\sum_{i=1}^n  |x_i|^p   \\right]^\\frac{1}{p} \\\\\n}\n\\]\n\n\n\nExample\n\n\n1. \\(L_{0.5}\\) Example\nGiven a vector \\(\\mathbf{x} = [3,4]\\), compute the \\(\\ell_{\\textcolor{red}{0.5}}\\) norm \\[\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} = \\left[ \\sum_{i=1}^n  |x_i|^{\\textcolor{red}{0.5}}\\right]^\\frac{1}{\\textcolor{red}{0.5}}  :=  \\sqrt[\\textcolor{red}{0.5}]{\\sum_{i=1}^n  |x_i|^\\textcolor{red}{0.5}}\n\\]\n\\[\nSimplifying:\n\\]\n\\[\n\\begin{align*}\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & = \\left[ \\sum_{i=1}^n  |x_i|^{\\textcolor{red}{0.5}}\\right]^\\frac{1}{\\textcolor{red}{0.5}}  \\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}{\\frac{1}{2}}\\right]^\\frac{1}{\\textcolor{red}{\\frac{1}{2}}}\\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & = \\left[ \\sum_{i=1}^n  |x_i|^\\textcolor{red}{\\frac{1}{2}}\\right]^{\\textcolor{red}{2}}\\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & =\n        (|x_1|^{\\textcolor{red}{\\frac{1}{2}}} +\n         |x_2|^{\\textcolor{red}{\\frac{1}{2}}} +\n         |x_3|^{\\textcolor{red}{\\frac{1}{2}}} + ... +\n         |x_N|^{\\textcolor{red}{\\frac{1}{2}}}\n         )^{\\textcolor{red}{2}}\\\\\n||\\mathbf{x}||_{p={\\textcolor{red}{0.5}}} & =  (\\sqrt[\\textcolor{red}{2}]{|x_1|} + \\sqrt[\\textcolor{red}{2}]{|x_2|} + \\sqrt[\\textcolor{red}{2}]{|x_3|} + ... + \\sqrt[\\textcolor{red}{2}]{|x_N|})^{\\textcolor{red}{2}}\\\\\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n||\\mathbf{x}||_{0.5} & = ( \\sqrt[2]{|3|} + \\sqrt[2]{|4|} )^2 \\\\\n                     & = (1.73 + 2)^2  \\\\\n                     & = (3.73)^2 \\\\\n                     & =  13.93 \\\\\n\\end{align*}\n\\]\n\nimport numpy as np\n\nx = np.array([3,4])\n\np=0.5\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n\n\\(\\displaystyle L_{0.5}-norm: 13.928203230275509\\)\n\n\n\n\n2. Distance Example\nGiven two vectors compute their \\(L_{0.5}\\) distance. \\(\\mathbf{u} = [1, 2, 3]\\) \\(\\mathbf{v} = [4, 5, 6]\\)\nSteps:\n\nCompute the error\nFind the absolute value\nCompute the square root of all elements\nSum all elements and square the result\n\n\\[\n\\begin{align*}\n||\\mathbf{x}||_{0.5} & = (\\sqrt[2]{|1-4|} + \\sqrt[2]{|2-5|} + \\sqrt[2]{|3-6|})^2 \\\\\n& = (\\sqrt[2]{|-3|} + \\sqrt[2]{|-3|} + \\sqrt[2]{|-3|})^2\\\\\n& = (\\sqrt[2]{3} + \\sqrt[2]{3} + \\sqrt[2]{3})^2 \\\\\n& = (1.732 + 1.732 + 1.732)^2 \\\\\n& = (5.196)^2 \\\\\n& = 27\\\\\n\\end{align*}\n\\]\n\nimport numpy as np\n\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nx = np.subtract(u,v)\n\np=0.5\n\nnorm_p = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in=p, val=norm_p )\n\n\\(\\displaystyle L_{0.5}-norm: 27.0\\)\n\n\n\n\n3. \\(L_{0.5}\\) Violates Triangle Inequality Example\nGiven two vectors show that \\(L_{0.5}\\) norm violates the triangle inequality.  \\(\\mathbf{u} = [1, 2, 3]\\) \\(\\mathbf{v} = [4, 5, 6]\\)\nRemember that the triangle inequality states: * The \\(p\\) norm of the sum of two vectors is less than or equal to the sum of the \\(p\\) of each vector\n\\[\n||\\mathbf{u} + \\mathbf{v} ||_p \\leq ||\\mathbf{u} ||_p + ||\\mathbf{v}||_p\n\\]\nSteps:\n\nCompute the sum of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)\nFind the \\(p\\) norm of the sum\nCompute the \\(p\\) norm of \\(\\mathbf{u}\\) then \\(\\mathbf{v}\\)\nSum the individual norms\nCompare both sides of the inequality\n\n\\[\n\\begin{align*}\n        ||\\mathbf{u} + \\mathbf{v} ||_p     & \\leq ||\\mathbf{u} ||_p + ||\\mathbf{v}||_p\\\\\n        ||\\mathbf{u} + \\mathbf{v} ||_{0.5} & \\leq ||\\mathbf{u} ||_{0.5} + ||\\mathbf{v}||_{0.5}\\\\\n\\quad\\\\        \n        (\\sqrt[2]{|1+4|} + \\sqrt[2]{|2+5|} + \\sqrt[2]{|3+6|})^2 & \\leq\n             (\\sqrt[2]{|1|} + \\sqrt[2]{|2|} + \\sqrt[2]{|3|})^2 +\n             (\\sqrt[2]{|4|} + \\sqrt[2]{|5|} + \\sqrt[2]{|6|})^2\\\\\n\\quad\\\\\n        (\\sqrt[2]{|5|} + \\sqrt[2]{|7|} + \\sqrt[2]{|9|})^2 & \\leq\n            (\\sqrt[2]{|1|} + \\sqrt[2]{|2|} + \\sqrt[2]{|3|})^2 +\n            (\\sqrt[2]{|4|} + \\sqrt[2]{|5|} + \\sqrt[2]{|6|})^2\\\\     \n\\quad\\\\\n        (2.236 + 2.646 + 3)^2 & \\leq\n            (1 + 1.414 + 1.732)^2 +\n            (2 + 2.236 + 2.449)^2\\\\\n\\quad\\\\\n        ( 7.882)^2 & \\leq\n            (4.146)^2 +\n            (6.686)^2\\\\\n\\quad\\\\\n        62.123 & \\leq\n            17.192 +\n            44.697\\\\\n\\quad\\\\\n        62.123 & \\leq 61.888  (\\textcolor{red}{Violation!})   \\\\     \n\\end{align*}\n\\]\n\nimport numpy as np\n\nu = [1, 2, 3]\nv = [4, 5, 6]\n\nx = np.add(u,v)\n\np=0.5\n\nnorm_p_uv = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in='||u+v||_{0.5}', val=norm_p_uv )\n\n\nx = u\np=0.5\nnorm_p_u = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in='||u||_{0.5}', val=norm_p_u  )\n\nx = v\np=0.5\nnorm_p_v = np.sum(np.abs(x)**p)**(1/p)\nprint_result(type_in='||v||_{0.5}', val=norm_p_v )\n\nnorm_p_v_u = norm_p_u + norm_p_v\nprint_result(type_in='||u||_{0.5} + ||v||_{0.5}', val=norm_p_v_u)\n\n\\(\\displaystyle L_{||u+v||_{0.5}}-norm: 62.123075297585515\\)\n\n\n\\(\\displaystyle L_{||u||_{0.5}}-norm: 17.191508225450303\\)\n\n\n\\(\\displaystyle L_{||v||_{0.5}}-norm: 44.69668203123519\\)\n\n\n\\(\\displaystyle L_{||u||_{0.5} + ||v||_{0.5}}-norm: 61.88819025668549\\)"
  },
  {
    "objectID": "projects/Short_dive_posts/aws-demo/aws-analysis-demo.html",
    "href": "projects/Short_dive_posts/aws-demo/aws-analysis-demo.html",
    "title": "LLM Healthcare Data Analysis",
    "section": "",
    "text": "Photo by Towfiqu barbhuiya on Unsplash"
  },
  {
    "objectID": "projects/2024-03-05/index.html",
    "href": "projects/2024-03-05/index.html",
    "title": "Large Language Models - Chatting with AI Chatbots from Google, Mistral AI, and Hugging Face",
    "section": "",
    "text": "Show me the demo!\n  LLMs\n  \n  LLMs what are they good for?\n  LLMs how do they work?\n  What is Gemma?\n  What is Mistral?\n  What is Zephyr?\n  \n  Setting Up Models with Hugging Face and Streamlit\n  What’s Next"
  },
  {
    "objectID": "projects/2024-03-05/index.html#show-me-the-demo",
    "href": "projects/2024-03-05/index.html#show-me-the-demo",
    "title": "Large Language Models - Chatting with AI Chatbots from Google, Mistral AI, and Hugging Face",
    "section": "Show me the demo!",
    "text": "Show me the demo!\nJump straight to the chatbot demo"
  },
  {
    "objectID": "projects/2024-03-05/index.html#llms",
    "href": "projects/2024-03-05/index.html#llms",
    "title": "Large Language Models - Chatting with AI Chatbots from Google, Mistral AI, and Hugging Face",
    "section": "LLMs",
    "text": "LLMs\nLarge language models (LLMs) have become widespread and accessible. Hugging Face has helped accelerate the accessibility and use of these models by aggregating them in one place and creating helpful APIs1 to use them.\n1 API - Application programming interfaceHere, we’ll use Streamlit to look at some of the state-of-the-art LLMs on Hugging Face that can be easily set-up for quick inference in a chatbot interface:\n\nMistral 7B\nGemma 7B and 2B\nZephyr 7B-β\nAnd more (Meta Llama etc.)\n\n\nLLMs what are they good for?\nLarge language models (LLMs) are powerful AI systems that can generate natural language texts based on a given input. They’ve been used for a number of tasks in the field of natural language processing (NLP)2. These have included text summarization, translation, sentiment analysis (e.g. tell you if your text has a positive or negative sentiment), etc.\n2 NLP -Natural Language Processing3 These are AI models that you can prompt with questionsOne of the most exciting applications of LLMs has been the creation of interactive question and answer chatbots3. These have been used to augment writing, have conversations, help generate code, and more! If you’ve tried OpenAI’s ChatGPT then you know how powerful a tool LLMs can be.\n\n\nLLMs how do they work?\nLLMs work through a number of different complex steps (greatly oversimplified):\n\nWords are first converted into high dimensional numeric vectors, representing their semantic meanings. Think of techniques like Word2Vec or GloVe4. These vectors embed different dimensions of meaning that correspond to each word5.\nThese vectors are fed into a massive machine learning model (typically a Transformer architecture) that learns patterns, probabilities, and relationships between words.\nThis training enables the model to generate coherent and contextually relevant text, based on new input it receives.\n\n4 GloVe - Global Vectors for Word Representation5 Jay Alammar does a great job explaining it here.\n\nWhat is Gemma?\n\nGemma is described as a family of lightweight models. It was developed by DeepMind and other teams at Google. These models were used to create Google’s Gemini models. The name of the model comes from the Latin word gemma, translating to ‘gem’ or precious stone.\nThe open source Gemma model is based on the transformer decoder architecture and comes in two sizes: a 7B and 2B. These represent the number of parameters that each model has, which are actually 7,751,248,896 (7B) and 1,981,884,416 (2B). On benchmarks the Gemma model performs on par or better than the LLaMA and Mistral models. You can check out the interactive benchmark comparisons here.\nOne thing to note is that the current 7B Hugging Face implementation of the Gemma model may have some hiccups, which are being looked into by the Hugging Face team6 so use it with caution.\n6 Jeremy Howard’s take\n\nWhat is Mistral?\n\nMistral was created by the Paris-based Mistral AI team. The founders of the team came from Meta Platforms and Google’s DeepMind. Similar to the Gemma model the Mistral model (specifically the Mistral-7B model) is based on the transformer architecture and has about 7 billion parameters. The name Mistral presumably comes from the term describing strong, cold, winds that blow through France from south to the Mediterranean.\n\n\nWhat is Zephyr?\n\nThe Zephyr models are a series of models that were fine-tuned by the team at Hugging Face. The Zephyr 7B-β model is the second of (currently) 3 models and is a fine-tuned version of the Mistral 7B model. The idea behind model fine-tuning is to improve the accuracy of the models. In this case the Mistral model was additionally fine-tuned with AI feedback (AIF). Similar to the term mistral the word zephyr refers to a soft, gentle breeze.\nOne of the latest Zephyr models, Zephyr 7B Gemma is a fine-tuned version of the Gemma model by Google, however it has not yet been configured for inference on Hugging Face. There is an interactive demo available though, check it out here."
  },
  {
    "objectID": "projects/2024-03-05/index.html#setting-up-models-with-hugging-face-and-streamlit",
    "href": "projects/2024-03-05/index.html#setting-up-models-with-hugging-face-and-streamlit",
    "title": "Large Language Models - Chatting with AI Chatbots from Google, Mistral AI, and Hugging Face",
    "section": "Setting Up Models with Hugging Face and Streamlit",
    "text": "Setting Up Models with Hugging Face and Streamlit\nHugging Face hosts a number of open-source, pre-trained LLMs and allows you to directly interface with them through API calls. When getting started we’ll first need an API key7, which is free after creating a Hugging Face account.\n7 It will be an alpha numeric code starting with hf\nWe first install all the libraries we’ll be using. We’ll use the OpenAI API to stream model output from but other APIs exist.\n!pip install openai python-dotenv\nNext we assign the client interface that’ll be helping us with API calls to HuggingFace. If you’re using Hugging Face spaces be sure to add your Hugging Face API key to your secret codes in your space settings.\nimport streamlit as st\nfrom openai import OpenAI\nimport os\nimport sys\nfrom dotenv import load_dotenv, dotenv_values\n\n#This loads env variables like your API keys\nload_dotenv() \nWe create a dictionary with the model names and their URLs.\n#Create supported models\nmodel_links ={\n              \"Mistral\":\"mistralai/Mistral-7B-Instruct-v0.2\",\n              \"Gemma-7B\":\"google/gemma-7b-it\",\n              \"Gemma-2B\":\"google/gemma-2b-it\",\n              \"Zephyr-7B-β\":\"HuggingFaceH4/zephyr-7b-beta\",\n              }\nWe’ll also create a dictionary with some info about the models.\n#Pull info about the model to display\nmodel_info ={\n    \"Mistral\":\n        {'description':\n            \"\"\"The Mistral model is a **Large Language Model (LLM)** that's able to have question and answer interactions...\"\"\",\n        'logo':\n            'https://mistral.ai/images/logo_hubc88c4ece131b91c7cb753f40e9e1cc5_2589_256x0_resize_q97_h2_lanczos_3.webp'},\n    \"Gemma-7B\":        \n        {'description':\n            \"\"\"The Gemma model is a **Large Language Model (LLM)** that's able to have question and answer interactions...\"\"\",\n        'logo':\n            'https://pbs.twimg.com/media/GG3sJg7X0AEaNIq.jpg'},\n    \"Gemma-2B\":        \n        {'description':\n            \"\"\"The Gemma model is a **Large Language Model (LLM)** that's able to have question and answer interactions...\"\"\",\n        'logo':\n            'https://pbs.twimg.com/media/GG3sJg7X0AEaNIq.jpg'},\n    \"Zephyr-7B\":        \n        {'description':\n            \"\"\"The Zephyr model is a **Large Language Model (LLM)** that's able to have question and answer interactions...\"\"\",\n        'logo':\n            'https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1/resolve/main/thumbnail.png'},\n    \"Zephyr-7B-β\":        \n        {'description':\n            \"\"\"The Zephyr model is a **Large Language Model (LLM)** that's able to have question and answer interactions...\"\"\",\n        'logo':\n            'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png'},  \n\n              }              \nCreate a function that wipes the session conversation if a button is pushed\ndef reset_conversation():\n    '''\n    Resets Conversation\n    '''\n    st.session_state.conversation = []\n    st.session_state.messages = []\n    return None\nWe configure the session settings in Streamlit and add some buttons.\n  # Define the available models\n  models =[key for key in model_links.keys()]\n\n  # Create the sidebar with the dropdown for model selection\n  selected_model = st.sidebar.selectbox(\"Select Model\", models)\n\n  #Create a temperature slider\n  temp_values = st.sidebar.slider('Select a temperature value', 0.0, 1.0, (0.5))\n\n\n  #Add reset button to clear conversation\n  st.sidebar.button('Reset Chat', on_click=reset_conversation) #Reset button\n\n\n  # Create model description\n  st.sidebar.write(f\"You're now chatting with **{selected_model}**\")\n  st.sidebar.markdown(model_info[selected_model]['description'])\n  st.sidebar.image(model_info[selected_model]['logo'])\n  st.sidebar.markdown(\"*Generated content may be inaccurate or false.*\")\n\n  st.subheader(f'AI - {selected_model}')\nWe do some configurations to clean up our messages and make sure we have the model selected.\n  #Keep track of which model we're using\n  if \"prev_option\" not in st.session_state:\n      st.session_state.prev_option = selected_model\n\n  #Clear conv if we change models\n  if st.session_state.prev_option != selected_model:\n      st.session_state.messages = []\n      st.session_state.prev_option = selected_model\n      reset_conversation()\n\n  #Pull in the model we want to use\n  repo_id = model_links[selected_model]\n\n  # Set a default model\n  if selected_model not in st.session_state:\n      st.session_state[selected_model] = model_links[selected_model] \n\n  # Initialize chat history\n  if \"messages\" not in st.session_state:\n      st.session_state.messages = []\n\n  # Display chat messages from history on app rerun\n  for message in st.session_state.messages:\n      with st.chat_message(message[\"role\"]):\n          st.markdown(message[\"content\"])\nFinally we set up the chat interface in Streamlit where the user’s questions or prompt gets passed on to the model we select and the model or the assistant returns a response. We make sure to keep adding our prompt and response to the messages structure we created so the model has some context to keep a conversation going.\n# Accept user input\nif prompt := st.chat_input(f\"Hi I'm {selected_model}, ask me a question\"):\n\n    # Display user message in chat message container\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n    # Add user message to chat history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n\n    # Display assistant response in chat message container\n    with st.chat_message(\"assistant\"):\n        stream = client.chat.completions.create(\n            model=model_links[selected_model],\n            messages=[\n                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                for m in st.session_state.messages\n            ],\n            temperature=temp_values,#0.5,\n            stream=True,\n            max_tokens=3000,\n        )\n\n        response = st.write_stream(stream)\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\nIf we’re running this locally we can use streamlit run app.py or on Hugging Face wait for all parts to build automatically. When everything’s up and running the interface will look like below. On the left panel the drop-down lets us switch between models and the temperature slider let’s us adjust the model’s temperature value.\n\n\n      \n      \n \nThe full script can be found here."
  },
  {
    "objectID": "projects/2024-03-19/index.html#what-is-this",
    "href": "projects/2024-03-19/index.html#what-is-this",
    "title": "Interactive Neuromodulation Landscape",
    "section": "What is this?",
    "text": "What is this?\nThis is an interactive mapping of different types of neuromodulation or brain stimulation techniques.\nThis mapping is based on my book chapter."
  },
  {
    "objectID": "projects/2024-03-19/index.html#what-is-neuromodulation",
    "href": "projects/2024-03-19/index.html#what-is-neuromodulation",
    "title": "Interactive Neuromodulation Landscape",
    "section": "What is neuromodulation?",
    "text": "What is neuromodulation?\nNeuromodulation is the action of interacting with or altering activity in the nervous system through the delivery of a stimulus. These stimuli can range from electrical currents, magnetic pulses, light, ultrasound, heat, etc. The terms neuromodulation and brain stimulation are often used interchangeably, although in some cases differences exist.\nCheck out this deeper discussion on the difference."
  },
  {
    "objectID": "projects/2024-03-19/index.html#how-do-i-use-this",
    "href": "projects/2024-03-19/index.html#how-do-i-use-this",
    "title": "Interactive Neuromodulation Landscape",
    "section": "How do I use this?",
    "text": "How do I use this?\n\nUse the navigation buttons at the top to change the layout of the chart.\nTo expand the chart categories click on the dropdown arrows.\nClick on the numbers on the upper right within each block to create a root mapping.\nUse the clear button to remove tree mapping.\nYou can drag the map around to fit your screen.\nYou can enter full screen mode.\nYou can export your selection with the buttons on top.\nThe data to make the chart can be found here.\n\n\nFeedback\nThis is a work in progress and will be periodically updated. Email me with any feedback."
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_PCA.html",
    "href": "projects/Past_projects/ImageRecog_PCA.html",
    "title": "Facial Recognition with Principal Component Analysis",
    "section": "",
    "text": "Here we use principal component analysis (PCA) to reduce the number of features in a dataset of faces. The PC’s are then fed into a Support Vector Machine (SVM) classifier to classify the faces based on learned features.\nThe dataset used in this example is a preprocessed excerpt of the “Labeled Faces in the Wild”, aka LFW"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_PCA.html#import-the-libraries",
    "href": "projects/Past_projects/ImageRecog_PCA.html#import-the-libraries",
    "title": "Facial Recognition with Principal Component Analysis",
    "section": "Import the libraries",
    "text": "Import the libraries\nHere we import the libraries that we need for later\n\nfrom time import time\nimport logging\nimport pylab as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# !pip install -U scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\n\n\n#Display progress\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_PCA.html#get-the-data",
    "href": "projects/Past_projects/ImageRecog_PCA.html#get-the-data",
    "title": "Facial Recognition with Principal Component Analysis",
    "section": "Get the data",
    "text": "Get the data\nHere we pull in the data and store it in numpy arrays\n\n#Download the data\nlfw_people =fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n#Find out shape infomration about the images to help with plotting them\nn_samples, h, w=lfw_people.images.shape\n\nnp.random.seed(42)\n\n# for machine learning we use the data directly (as relative pixel\n# position info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the ID of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint (\"Total dataset size:\")\nprint (\"n_samples: %d\" % n_samples)\nprint (\"n_features: %d\" % n_features)\nprint (\"n_classes: %d\" % n_classes)\nprint (\"Classes: %s\" % target_names)\n\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976012\n2019-07-19 05:21:21,839 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976009\n2019-07-19 05:21:28,215 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976006\n2019-07-19 05:21:29,542 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006\nDownloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015\n2019-07-19 05:21:31,138 Downloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015\n\n\nTotal dataset size:\nn_samples: 1288\nn_features: 1850\nn_classes: 7\nClasses: ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n 'Gerhard Schroeder' 'Hugo Chavez' 'Tony Blair']\n\n\n\n#Lets look at the data to see what they look like\n\npl.figure\nfor i in range(0,3):\n  pl.subplot(1,3,i+1)\n  pl.imshow(X[i].reshape((h,w)), cmap=pl.cm.bone)\n  pl.title(target_names[lfw_people.target[i]])\n  pl.xticks(())\n  pl.yticks(())\n  \n\n\n\n\n\n\n\n\n##Split Data (Test|Train) Here we split the data into a testing and train set\n\nX_train, X_test, y_train, y_test =train_test_split(X,y, test_size=0.25, random_state=42)\ny_test.dtype\n\ndtype('int64')\n\n\n##Do PCA and Dimensionality Reduction on Data Now we compute the PC’s using PCA\n\n\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 250\n\nprint(\"Extracting the top %d eignefaces from %d faces\" % (n_components, X_train.shape[0]))\n\n#Initiate a time counter (kinda like tic toc)\nt0=time()\n\n#Here we take the training data and compute the PCs\npca=PCA(n_components=n_components, whiten=True).fit(X_train)\n\n\n#Print the time it took to compute\nprint(\"Done in %0.3fs\" % (time()-t0))\n\n\n\n#Reshape the PCs to the image format\neigenfaces =pca.components_.reshape((n_components,h,w))\n\npl.figure\nfor i in range(0,3):\n  pl.subplot(1,3,i+1)\n  pl.imshow(eigenfaces[i], cmap=pl.cm.bone)\n  pl.title(\"Eigenface PC- %d\" % (i+1))\n  pl.xticks(())\n  pl.yticks(())\n#   cbar = pl.colorbar()\n#   cbar.solids.set_edgecolor(\"face\")\n#   pl.draw()\n\n# print(pca.explained_variance_ )\n# print(pca.explained_variance_ratio_)\n\n\nExtracting the top 250 eignefaces from 966 faces\nDone in 0.399s\n\n\n\n\n\n\n\n\n\n\nLook at the top PC’s\nHere we’re going to plot the top PCs identified\n\ntop_pcs=21\nplt.figure( figsize=(9, 3))\nplt.subplot(121)\nplt.bar(np.arange(top_pcs),pca.explained_variance_[0:top_pcs])\nplt.xlabel('PCs')\nplt.ylabel('Var')\n\nplt.subplot(122)\nplt.bar(np.arange(top_pcs),pca.explained_variance_ratio_[0:top_pcs])\nplt.xlabel('PCs')\nplt.ylabel('Ratio')\n\nnp.shape(pca.explained_variance_)\n\n\n\n\n\n\n\n\n\n\nProjecting the PCs\nNow that we have the PCs (the vectors that account for the max variances) we can project the data down to the PCs. In this case they are the eigenfaces from above.\n\nprint(\"Projecting the input data on the eignefaces orthonormal basis\")\n\nt0=time()#tic\n\nX_train_pca = pca.transform(X_train) #take the training data and project it to eigenfaces\nX_test_pca  = pca.transform(X_test)#take the test data and project it to eigenfaces\nprint(\"Done in %0.3fs\" % (time()- t0)) #toc\n\nProjecting the input data on the eignefaces orthonormal basis\nDone in 0.035s\n\n\n##Train a SVM Classification Model\nSo now that we have the most important features extracted out by PCA we can use those to train a classifier. The SVM classifier can then be used to predict who’s face is being presented\n\n#Training\n\nprint (\"Fitting the classifier to the training set\")\nt0 = time()\n#These set parameters that we want to optimize. These are passed to GridSearch\n#which uses the optimal paramters in the fitting classifier =clf\nparam_grid = {\n         'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n          'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n          }\n# for sklearn version 0.16 or prior, the class_weight parameter value is 'auto'\nclf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint (\"done in %0.3fs\" % (time() - t0))\nprint (\"Best estimator found by grid search:\")\nprint (clf.best_estimator_)\n\nFitting the classifier to the training set\ndone in 39.016s\nBest estimator found by grid search:\nSVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\n\n\n/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n  warnings.warn(CV_WARNING, FutureWarning)\n\n\n\n#Testing the classifier \n\nprint (\"Predicting the people names on the testing set\")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint (\"done in %0.3fs\" % (time() - t0))\n\nprint (classification_report(y_test, y_pred, target_names=target_names))\nprint (confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n\nPredicting the people names on the testing set\ndone in 0.125s\n                   precision    recall  f1-score   support\n\n     Ariel Sharon       0.64      0.69      0.67        13\n     Colin Powell       0.75      0.90      0.82        60\n  Donald Rumsfeld       0.82      0.67      0.73        27\n    George W Bush       0.91      0.92      0.91       146\nGerhard Schroeder       0.87      0.80      0.83        25\n      Hugo Chavez       0.80      0.53      0.64        15\n       Tony Blair       0.82      0.78      0.80        36\n\n         accuracy                           0.84       322\n        macro avg       0.80      0.76      0.77       322\n     weighted avg       0.84      0.84      0.84       322\n\n[[  9   1   1   2   0   0   0]\n [  1  54   2   2   0   1   0]\n [  2   3  18   3   0   0   1]\n [  1   6   1 134   1   1   2]\n [  0   2   0   1  20   0   2]\n [  0   4   0   2   0   8   1]\n [  1   2   0   3   2   0  28]]\n\n\n##Visualizing Results Now that we’ve trained and tested the SVM to classify faces lets visualized the results\n\n#Create a helper function to look at the pictures\n                         \ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        pl.subplot(n_row, n_col, i + 1)\n        pl.imshow(images[i].reshape((h, w)), cmap=pl.cm.gray)\n        pl.title(titles[i], size=12)\n        pl.xticks(())\n        pl.yticks(())\n\n\n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                         for i in range(y_pred.shape[0])] \n\n\n#Now print out preductions\n# prediction_titles=(X_test, prediction_titles,h,w)\n\nplot_gallery(X_test, prediction_titles, h, w )\n\n#Plot the eignefaces\n\neigenface_titles =[\"Eigneface %d \" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_PCA.html#archived",
    "href": "projects/Past_projects/ImageRecog_PCA.html#archived",
    "title": "Facial Recognition with Principal Component Analysis",
    "section": "Archived",
    "text": "Archived\nProject Archive Note:\nThis project is archived.  Please note that library and framework versions may be outdated. Last updated:\n\nApril 2025"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_VGG16.html#about-this-project",
    "href": "projects/Past_projects/ImageRecog_VGG16.html#about-this-project",
    "title": "Image Recognition & Classification with VGG16",
    "section": "About this project",
    "text": "About this project\nOur goal with this library is to leverage a pretrained model to perform image classification. We will use the VGG16 model trained on the ImageNet dataset to perform some simple image classification.\nThe process will work similar to the image below:\n\n\n\n\n\n\nImage Classification"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_VGG16.html#import-libraries",
    "href": "projects/Past_projects/ImageRecog_VGG16.html#import-libraries",
    "title": "Image Recognition & Classification with VGG16",
    "section": "Import Libraries",
    "text": "Import Libraries\nHere we import the libraries that we will be using throughout the notebook.\nThe libraries that we will be using include:\n\nNumpy\nKeras using the Tensorflow backend:\n\nKeras-VGG16\nKeras-image processing library\n\nMatplotlib &lt;-for plotting and visualizations\n\n\n#Import\n\nimport numpy as np\nfrom keras.applications import vgg16\nfrom keras.preprocessing import image\n\n#Image visualization\n%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nUsing TensorFlow backend.\n\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\nmodel = vgg16.VGG16(weights='imagenet') #Load the model weights\n\nWARNING: Logging before flag parsing goes to stderr.\nW0820 06:55:47.559752 139747390875520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nW0820 06:55:47.580525 139747390875520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nW0820 06:55:47.584277 139747390875520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nW0820 06:55:47.619110 139747390875520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\nW0820 06:55:49.527782 139747390875520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nW0820 06:55:49.529251 139747390875520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead."
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_VGG16.html#load-an-image",
    "href": "projects/Past_projects/ImageRecog_VGG16.html#load-an-image",
    "title": "Image Recognition & Classification with VGG16",
    "section": "Load an Image",
    "text": "Load an Image\nNow we’re going to load an image in and walk through the processing steps so that we can successfully load the image into Keras. We’re going to pick an image that’s a little tricky to see how the model does at predicting what’s in the image.\n\nimgaddress='panda.jpeg'\nimg = image.load_img(imgaddress,target_size=(224,224)) #Try loading an image with the size specified \nimg #Show image\n\n\n\n\n\n\n\n\n\n# Convert to Numpy array\narr = image.img_to_array(img) #Here we convert the image to a numpy array inorder to do further numeric manipulations to it.\narr.shape #Height X Width X Color channel (RGB)\n\n(224, 224, 3)\n\n\n\n# Expand dimension\narr = np.expand_dims(arr,axis=0) #Here we add an additional dimension to satisfy the input parameters for keras\narr.shape #print out shape of image\n\n(1, 224, 224, 3)\n\n\n\n# Preprocessing\narr = vgg16.preprocess_input(arr) #Normalize data.\narr \n\narray([[[[126.061, 113.221, 106.32 ],\n         [126.061, 113.221, 106.32 ],\n         [126.061, 113.221, 106.32 ],\n         ...,\n         [127.061, 114.221, 107.32 ],\n         [127.061, 114.221, 107.32 ],\n         [127.061, 114.221, 107.32 ]],\n\n        [[126.061, 113.221, 106.32 ],\n         [126.061, 113.221, 106.32 ],\n         [126.061, 113.221, 106.32 ],\n         ...,\n         [127.061, 114.221, 107.32 ],\n         [127.061, 114.221, 107.32 ],\n         [127.061, 114.221, 107.32 ]],\n\n        [[126.061, 113.221, 106.32 ],\n         [126.061, 113.221, 106.32 ],\n         [126.061, 113.221, 106.32 ],\n         ...,\n         [127.061, 114.221, 107.32 ],\n         [127.061, 114.221, 107.32 ],\n         [127.061, 114.221, 107.32 ]],\n\n        ...,\n\n        [[121.061, 108.221, 101.32 ],\n         [121.061, 108.221, 101.32 ],\n         [121.061, 108.221, 101.32 ],\n         ...,\n         [121.061, 108.221, 101.32 ],\n         [121.061, 108.221, 101.32 ],\n         [119.061, 106.221,  99.32 ]],\n\n        [[119.061, 106.221,  99.32 ],\n         [119.061, 106.221,  99.32 ],\n         [119.061, 106.221,  99.32 ],\n         ...,\n         [121.061, 108.221, 101.32 ],\n         [121.061, 108.221, 101.32 ],\n         [124.061, 111.221, 104.32 ]],\n\n        [[119.061, 106.221,  99.32 ],\n         [118.061, 105.221,  98.32 ],\n         [115.061, 102.221,  95.32 ],\n         ...,\n         [120.061, 107.221, 100.32 ],\n         [123.061, 110.221, 103.32 ],\n         [113.061, 100.221,  93.32 ]]]], dtype=float32)"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_VGG16.html#predict-and-pull-out-predicted-class",
    "href": "projects/Past_projects/ImageRecog_VGG16.html#predict-and-pull-out-predicted-class",
    "title": "Image Recognition & Classification with VGG16",
    "section": "Predict and Pull Out Predicted Class",
    "text": "Predict and Pull Out Predicted Class\nNow that we’ve set up out model and image let’s now use it to predict what’s depicted in the image.\n\n# Predict\npreds = model.predict(arr)\npreds #Prints out all the prections for a number of classes. We just want to top predctions. \n\narray([[2.30740028e-07, 2.09631480e-07, 1.16082921e-09, 3.73188147e-09,\n        8.20546209e-09, 2.03246824e-08, 2.84081003e-09, 6.54241177e-08,\n        9.36755953e-08, 2.08457944e-08, 6.61266029e-08, 1.06568439e-06,\n        2.21695558e-07, 1.14869820e-08, 3.40318365e-07, 2.71161067e-08,\n        1.96107976e-07, 1.51007441e-07, 2.22093831e-07, 1.83282197e-07,\n        7.00448854e-09, 3.47982336e-07, 1.48229120e-07, 1.64141980e-07,\n        1.41322687e-07, 1.68453028e-06, 1.67549302e-07, 2.24668202e-06,\n        6.26418750e-06, 1.27461942e-06, 2.70466984e-07, 3.85677049e-06,\n        1.08100730e-05, 5.89471334e-08, 3.55724090e-08, 1.06533427e-07,\n        4.32566225e-07, 7.23472127e-08, 3.31999900e-07, 2.48433025e-08,\n        4.63609311e-08, 7.92857691e-09, 1.24308713e-07, 3.48936915e-08,\n        4.36078409e-08, 2.84841406e-07, 9.13415121e-08, 2.05572917e-07,\n        2.11722515e-08, 1.17623493e-08, 4.48982096e-09, 4.73372864e-07,\n        1.25886459e-07, 8.65207937e-07, 3.21610294e-08, 6.76665408e-08,\n        1.22866857e-06, 8.04934874e-09, 4.57685800e-08, 9.89627580e-08,\n        1.37686982e-07, 3.21550374e-07, 4.67060985e-08, 6.20166929e-07,\n        4.29578932e-08, 7.61357910e-08, 1.06795767e-07, 2.93389135e-08,\n        1.04514939e-07, 1.26362840e-08, 3.92909669e-08, 5.12591569e-08,\n        1.12368596e-07, 1.36531881e-07, 3.62430974e-08, 5.80957874e-07,\n        1.08994712e-07, 1.32111069e-07, 6.31206831e-07, 1.52073994e-07,\n        9.38977280e-07, 1.74692811e-08, 1.16386765e-07, 6.56322712e-08,\n        7.57403384e-09, 2.86673838e-07, 1.46441849e-07, 6.07505740e-07,\n        6.70098188e-07, 6.49351421e-07, 5.35652873e-07, 3.84898762e-08,\n        4.12868353e-08, 1.78363649e-07, 7.06433241e-08, 1.89389411e-08,\n        2.25836686e-07, 2.97790592e-08, 1.72217494e-08, 4.68255372e-08,\n        4.74925876e-08, 1.86267741e-07, 6.03362196e-08, 3.32126888e-07,\n        8.38132497e-09, 1.27715211e-07, 7.55305436e-08, 8.77682105e-09,\n        2.55688269e-06, 2.67960019e-08, 1.59058033e-07, 2.93469043e-07,\n        4.56972060e-07, 3.80638312e-07, 4.43869823e-07, 9.52950302e-07,\n        2.98638660e-08, 1.68837229e-08, 7.52986011e-08, 2.10233097e-07,\n        1.76909126e-07, 3.41121087e-08, 9.37658697e-08, 6.87844661e-08,\n        8.40257712e-08, 4.46965629e-07, 3.03340840e-07, 7.24343652e-09,\n        1.86818339e-09, 4.12252810e-09, 5.67342795e-08, 4.39383552e-09,\n        1.85187901e-08, 7.27469995e-09, 5.36931921e-09, 8.79188367e-09,\n        1.06165512e-08, 8.51799413e-08, 1.66270713e-08, 1.31700411e-08,\n        2.37485480e-08, 3.62283581e-09, 3.14623594e-09, 4.51901494e-09,\n        9.81340698e-09, 4.32230763e-06, 5.90467160e-08, 7.22333482e-09,\n        5.42055574e-08, 4.06835063e-08, 2.02358681e-08, 8.14627947e-06,\n        1.11762461e-06, 1.67708788e-06, 2.01941702e-06, 4.33945019e-07,\n        5.72450531e-08, 4.98712780e-07, 5.06023639e-07, 2.48144261e-07,\n        1.57320116e-07, 9.26459833e-08, 1.54363818e-07, 8.92200589e-08,\n        1.99415751e-07, 1.19492498e-07, 1.43114079e-07, 7.50381659e-08,\n        2.48550919e-07, 3.68729403e-08, 3.25495684e-08, 6.59341936e-07,\n        6.72650970e-07, 1.35678704e-07, 1.31095161e-08, 8.72792967e-08,\n        1.48460288e-07, 2.54546592e-07, 1.42769181e-07, 2.28453132e-06,\n        5.91747096e-07, 3.34318457e-08, 1.32749688e-06, 6.22414655e-06,\n        6.25722180e-07, 7.02322177e-06, 1.24519261e-06, 2.63988181e-06,\n        1.14330044e-06, 5.72108661e-07, 1.09037808e-06, 5.82663517e-07,\n        4.01251242e-07, 1.14455497e-06, 1.54444507e-07, 2.27040709e-06,\n        1.79260965e-07, 3.04547513e-07, 1.41533235e-07, 1.39204815e-07,\n        3.44765624e-07, 2.18203127e-07, 6.97724090e-07, 5.08941469e-07,\n        2.43415741e-07, 2.06829299e-07, 1.41577118e-07, 2.99716163e-07,\n        1.32351238e-06, 1.32063192e-07, 1.15778327e-07, 4.94967196e-07,\n        1.58050444e-08, 6.66455620e-08, 6.01057479e-08, 6.06416108e-08,\n        6.37214299e-08, 9.23016827e-08, 4.50936533e-08, 1.54943152e-07,\n        4.27466716e-07, 3.79468865e-08, 2.67693224e-07, 4.73522334e-07,\n        8.52471445e-08, 1.78649131e-07, 1.22580516e-07, 1.89995717e-07,\n        2.70702181e-08, 3.90288591e-07, 5.53862165e-08, 3.21281846e-07,\n        4.33772470e-07, 1.41284957e-07, 4.31418840e-07, 3.65242201e-08,\n        1.86620866e-07, 1.28293914e-06, 5.09924348e-07, 3.84684427e-08,\n        9.01986539e-07, 3.60616269e-07, 9.11655036e-07, 5.40120766e-07,\n        2.34421602e-08, 2.93961449e-07, 2.18722931e-07, 3.94994792e-08,\n        6.48091429e-08, 4.13592716e-08, 5.62787044e-08, 2.19680282e-06,\n        6.90981096e-07, 1.17152535e-06, 4.16749526e-06, 2.86457791e-08,\n        2.96173795e-07, 5.87741269e-08, 1.72613881e-07, 1.66389850e-06,\n        1.53144427e-07, 4.87860490e-08, 8.40172925e-06, 8.18051831e-08,\n        3.47647301e-07, 1.29341572e-06, 1.72425317e-07, 1.31130236e-07,\n        5.66247934e-07, 9.04584674e-09, 1.73176371e-08, 2.41903990e-08,\n        9.35978584e-09, 3.29720791e-07, 4.32366143e-09, 3.89148482e-08,\n        1.22352546e-07, 1.39844500e-08, 2.98412601e-08, 8.49396571e-08,\n        1.47494932e-08, 1.40953617e-07, 1.63191260e-07, 1.93809015e-07,\n        4.50804087e-08, 6.14120211e-07, 2.03362376e-08, 8.31917433e-08,\n        8.38911021e-08, 1.10642153e-08, 1.22347060e-07, 1.46274076e-07,\n        1.87119866e-07, 2.53398298e-07, 8.59659011e-08, 2.66953020e-08,\n        3.05486793e-07, 4.33444747e-08, 5.80405185e-07, 1.00526711e-06,\n        1.67631060e-06, 2.97465022e-06, 7.26791143e-07, 1.72061721e-06,\n        2.95490372e-06, 4.75534762e-07, 1.79628223e-05, 6.66515064e-07,\n        4.02796907e-08, 4.09895371e-08, 2.17734382e-07, 3.82573511e-08,\n        2.04364113e-07, 6.79272683e-09, 1.33294876e-07, 4.56201512e-08,\n        1.10466658e-07, 6.80765879e-08, 1.40555283e-08, 5.87065117e-08,\n        1.44609018e-08, 8.24203369e-07, 6.52484644e-08, 4.94799863e-08,\n        2.63742805e-08, 1.64423586e-08, 1.88935260e-07, 2.74232264e-07,\n        4.55387799e-08, 1.43832821e-07, 3.07482679e-08, 2.44847946e-08,\n        9.27687349e-09, 9.74918294e-07, 7.54552687e-09, 4.95618764e-08,\n        1.93481675e-08, 4.52532234e-08, 2.22582457e-06, 7.28586436e-09,\n        1.17542257e-07, 9.28539379e-08, 5.74220991e-08, 1.08532824e-07,\n        4.70160693e-08, 1.01746657e-07, 5.20004733e-08, 2.97083709e-08,\n        9.96368286e-08, 1.69071850e-08, 4.82118772e-08, 2.06702389e-08,\n        2.61470809e-08, 1.18879555e-08, 3.00920640e-08, 1.42195011e-08,\n        1.55393093e-06, 1.25310208e-07, 2.62512600e-07, 3.60448496e-07,\n        1.24466723e-07, 1.33048798e-06, 2.70772694e-06, 3.58778109e-08,\n        1.21350851e-07, 2.23774563e-08, 8.82985134e-08, 9.88200739e-08,\n        1.15604109e-07, 3.82766139e-08, 4.83422411e-07, 5.71778628e-07,\n        3.34457354e-07, 1.92857442e-07, 2.31056688e-07, 1.54931627e-07,\n        1.61167790e-07, 6.56304337e-07, 1.04844730e-06, 1.80460461e-07,\n        4.64523453e-07, 1.31840210e-07, 2.42890280e-07, 1.42358999e-07,\n        5.03785088e-07, 1.66625099e-07, 7.28182457e-08, 6.08862152e-08,\n        1.51019975e-07, 2.32699389e-08, 1.24165950e-07, 8.44717789e-08,\n        4.49987766e-07, 3.67735794e-07, 2.21215188e-08, 2.39472762e-08,\n        5.08758760e-08, 3.79272755e-08, 1.35908245e-08, 1.79140997e-07,\n        4.60847183e-08, 1.49218835e-07, 2.11217497e-08, 3.51093754e-09,\n        7.35311279e-09, 1.68405066e-07, 9.53853601e-08, 2.92592091e-07,\n        4.96845644e-07, 8.54975860e-06, 2.13233209e-09, 5.07116397e-07,\n        1.62518154e-05, 2.23540695e-07, 6.32358569e-06, 9.54755524e-06,\n        4.96491133e-08, 2.12956238e-07, 2.98555256e-06, 1.22388847e-05,\n        5.07114706e-08, 2.09368824e-08, 2.14847034e-07, 8.61394369e-07,\n        5.76402996e-08, 9.51128065e-09, 6.84745558e-08, 2.61448768e-05,\n        1.92413609e-07, 9.08089646e-07, 1.16331220e-08, 2.05686160e-06,\n        5.34091704e-08, 6.96576535e-07, 9.27639633e-07, 7.85441443e-06,\n        3.78948108e-08, 1.98729921e-08, 1.87666103e-06, 8.78381707e-06,\n        2.86733575e-06, 1.25621427e-05, 3.66315902e-08, 7.50137303e-07,\n        1.16340374e-07, 1.06631411e-07, 1.17933006e-07, 1.92094958e-05,\n        2.02627220e-07, 2.22119434e-09, 2.33889580e-07, 6.88466514e-07,\n        1.91087543e-06, 2.56423675e-07, 8.63026060e-08, 5.15929014e-06,\n        4.71249081e-07, 1.43907328e-05, 9.59242552e-09, 6.33050590e-08,\n        7.11355863e-09, 1.90989704e-06, 7.04786629e-08, 5.86291681e-05,\n        6.22968189e-04, 7.09655183e-07, 5.36632250e-08, 9.96110305e-09,\n        4.09386132e-07, 5.33454644e-04, 1.37213457e-04, 1.91851640e-07,\n        3.06269854e-08, 3.45783446e-05, 4.97320684e-07, 1.12767043e-06,\n        3.57932777e-07, 2.15330215e-06, 5.25317887e-07, 1.56554743e-05,\n        4.78038320e-09, 7.46765409e-07, 3.28704687e-06, 1.56892966e-07,\n        1.03753877e-08, 7.85365387e-07, 4.84578457e-08, 8.91920718e-06,\n        1.16398587e-06, 8.63011351e-08, 2.24844271e-07, 6.31793137e-06,\n        5.70030352e-07, 6.19336831e-08, 1.16691396e-07, 2.69378603e-07,\n        4.77650076e-07, 1.17892123e-08, 2.42671607e-08, 1.49556274e-06,\n        1.61047549e-08, 2.74794075e-07, 1.08856548e-05, 1.12808026e-04,\n        5.09483039e-01, 8.45756568e-03, 1.62210870e-06, 3.57126260e-06,\n        1.33839535e-07, 1.00126343e-07, 3.68348445e-08, 3.32293055e-08,\n        1.26010436e-05, 8.26668440e-08, 5.91857497e-06, 2.72094144e-06,\n        7.18639683e-07, 1.22508766e-07, 5.74260193e-05, 1.22780122e-07,\n        4.23249773e-08, 3.77007891e-05, 6.21546832e-08, 4.93676907e-07,\n        6.86424926e-07, 6.96312874e-09, 1.65719584e-07, 1.07889377e-07,\n        3.86126958e-05, 1.21791545e-05, 1.01053211e-05, 1.63983168e-05,\n        8.64890737e-08, 7.18710567e-08, 4.84098337e-07, 1.01615951e-05,\n        5.42548939e-08, 5.81623922e-08, 1.74807457e-08, 2.67341932e-07,\n        5.00734600e-08, 5.36412745e-06, 1.85197720e-07, 1.31648176e-06,\n        1.62742504e-06, 1.20514540e-06, 3.09547659e-07, 1.69230248e-08,\n        1.99864605e-08, 5.18494403e-08, 2.25793221e-03, 7.12145675e-06,\n        1.05833351e-06, 2.99498275e-08, 2.14378758e-07, 2.91862623e-08,\n        1.25276983e-07, 1.74415149e-08, 2.53638859e-07, 4.18758646e-08,\n        1.09169478e-05, 8.03377304e-07, 3.65722371e-07, 4.98785539e-07,\n        1.37837217e-08, 1.00747208e-07, 6.06082509e-08, 3.94741028e-06,\n        7.10235071e-08, 1.69653077e-07, 1.73707249e-05, 1.54607278e-06,\n        4.89220729e-05, 7.40523831e-07, 8.54893926e-07, 1.00471595e-06,\n        9.98703342e-09, 3.06131476e-08, 1.40731188e-07, 5.20920977e-08,\n        1.31625066e-08, 9.34547515e-08, 3.24858114e-08, 4.00238243e-08,\n        1.91738349e-04, 3.14479166e-06, 1.32722164e-07, 6.89278841e-06,\n        7.29645808e-06, 4.90466891e-06, 2.12085354e-07, 4.38568520e-07,\n        5.70686382e-07, 8.97580264e-07, 1.56464051e-08, 1.07001803e-07,\n        7.50558797e-07, 9.51741413e-07, 3.36923840e-08, 5.34097779e-08,\n        1.77375223e-05, 1.72772133e-07, 3.55589762e-08, 8.38164471e-09,\n        9.12969710e-07, 1.32454159e-06, 2.05620763e-05, 6.18671208e-07,\n        1.56490842e-07, 8.27659292e-08, 1.04609571e-05, 2.30176511e-07,\n        1.91370226e-08, 1.81046983e-06, 9.00357975e-07, 5.29871431e-05,\n        7.40103758e-07, 3.64682023e-07, 5.21576703e-06, 3.12630868e-06,\n        1.75079506e-07, 1.49572236e-06, 1.39119747e-05, 2.16342323e-06,\n        5.10878593e-08, 7.57715171e-08, 2.91329430e-04, 7.05078307e-08,\n        4.76907474e-08, 1.81246867e-06, 3.56844012e-08, 1.39066424e-05,\n        1.64770972e-06, 1.75037949e-05, 2.87580058e-08, 5.07249661e-06,\n        2.15312548e-06, 8.98535575e-08, 2.11578185e-06, 5.05718951e-08,\n        1.54342104e-08, 1.89451919e-06, 1.68519367e-08, 2.59888602e-05,\n        2.74932461e-07, 2.00012185e-08, 1.97279704e-08, 2.23891875e-05,\n        2.34228423e-07, 6.33858690e-07, 3.52894244e-06, 6.23561618e-07,\n        1.59773478e-07, 2.20319460e-04, 3.95545356e-07, 3.39762863e-07,\n        4.59922553e-08, 4.72152166e-08, 1.57768386e-06, 5.90417631e-06,\n        8.65793925e-09, 7.61667280e-08, 6.85525734e-08, 1.12686944e-08,\n        1.39720186e-07, 1.64703977e-07, 1.55557009e-05, 2.25209746e-07,\n        2.34410757e-09, 1.32394282e-08, 1.03016976e-06, 2.98369230e-07,\n        7.26395433e-09, 2.41500743e-07, 1.08037386e-06, 5.97784222e-08,\n        2.72129855e-05, 3.23625699e-07, 2.60866454e-05, 1.82006920e-06,\n        4.56080656e-04, 2.77210631e-07, 2.13454285e-08, 1.93574422e-07,\n        1.08476015e-05, 1.02433638e-07, 9.38850553e-06, 1.37056766e-08,\n        2.66808460e-08, 3.90996036e-07, 2.12439293e-08, 5.76670209e-06,\n        3.54932149e-06, 1.92957987e-08, 5.46688206e-09, 3.46115507e-06,\n        4.60707452e-06, 8.13636859e-07, 7.05712333e-09, 7.61370202e-08,\n        1.58197563e-05, 1.59634112e-07, 4.25841016e-08, 8.63675780e-08,\n        3.15127153e-07, 1.37078018e-07, 8.26236146e-09, 7.20345440e-07,\n        7.70038412e-07, 5.76401953e-06, 1.76408343e-04, 6.24262839e-06,\n        1.91485956e-08, 5.60556614e-08, 2.70809073e-06, 9.02508020e-07,\n        2.95503586e-08, 1.55460680e-06, 1.79604989e-08, 1.41473531e-04,\n        7.05745742e-06, 5.13960731e-06, 8.36539414e-07, 4.59378988e-08,\n        1.47361149e-07, 1.56827085e-02, 5.41640084e-08, 3.66196460e-08,\n        2.05718635e-07, 5.28138003e-07, 2.88077587e-07, 1.40117641e-06,\n        6.73106251e-06, 1.06973950e-07, 4.91104792e-08, 4.73942492e-07,\n        3.94559038e-07, 2.09052200e-06, 2.24188443e-05, 3.96605492e-06,\n        5.94256699e-06, 6.54013110e-09, 1.66611756e-07, 2.93421021e-08,\n        1.59431323e-07, 4.18593390e-06, 1.75135465e-05, 5.42386829e-07,\n        2.94390543e-06, 3.06368929e-06, 1.10581439e-07, 1.37935990e-06,\n        4.67533425e-08, 2.87463394e-07, 1.09019925e-06, 3.78070943e-08,\n        1.95305256e-05, 2.49075583e-07, 1.88775157e-05, 2.43823506e-05,\n        1.42132376e-06, 1.68880410e-06, 1.35500656e-07, 9.74761178e-07,\n        4.05859140e-07, 7.13980057e-07, 5.38881864e-07, 2.40283862e-05,\n        1.32920263e-07, 6.72690078e-07, 2.31001380e-07, 5.29700870e-08,\n        3.78082837e-06, 1.23375179e-02, 7.97549365e-06, 4.27160302e-07,\n        6.40690558e-08, 3.79926405e-07, 2.92688952e-07, 4.44884591e-08,\n        3.11600665e-08, 1.39019543e-08, 1.04999273e-07, 1.77168033e-06,\n        1.93643405e-06, 8.88002978e-06, 4.42980927e-05, 2.48587895e-07,\n        1.17284152e-07, 6.74648604e-08, 4.71922789e-07, 1.98075540e-07,\n        5.25653661e-07, 2.69821498e-06, 3.84169937e-07, 4.97369570e-07,\n        1.24303801e-06, 8.41954346e-08, 2.03592112e-08, 4.53609594e-09,\n        7.37740535e-08, 2.57440161e-06, 3.67120805e-07, 2.96055891e-08,\n        4.95262095e-04, 4.38857199e-07, 9.76660317e-07, 5.29256283e-09,\n        6.05305331e-07, 1.11715795e-04, 4.06630285e-08, 1.13087867e-06,\n        9.70791945e-08, 2.07313428e-06, 1.46531249e-07, 6.29572794e-09,\n        4.29507281e-06, 7.06429546e-07, 1.90187802e-06, 8.96300136e-08,\n        3.10500106e-08, 4.11359746e-09, 2.76818866e-07, 1.82589592e-05,\n        2.08955339e-06, 1.29551552e-08, 4.04326011e-06, 1.43453362e-05,\n        4.76714958e-05, 3.86165802e-08, 1.10205278e-07, 6.61844552e-08,\n        3.30068275e-08, 2.15795843e-08, 2.05103703e-07, 3.29985816e-08,\n        5.43825900e-06, 1.20013965e-05, 5.67736333e-06, 1.09627951e-08,\n        9.58354804e-07, 4.88013825e-07, 7.25631082e-07, 5.84639508e-07,\n        2.96557209e-06, 2.47400664e-07, 5.33883394e-06, 5.80091744e-08,\n        1.04031699e-06, 5.33550838e-03, 1.83874741e-04, 3.44726857e-07,\n        5.14731846e-06, 1.81518001e-09, 5.55293305e-08, 3.86142405e-04,\n        2.38253932e-07, 1.85367881e-07, 2.17458478e-08, 2.51325895e-04,\n        4.90558065e-08, 1.98287512e-06, 1.05115441e-06, 1.89473880e-07,\n        1.15158309e-06, 1.69682278e-06, 7.51792413e-07, 9.05999258e-08,\n        2.11496390e-05, 2.63543797e-07, 2.37221093e-06, 2.13503561e-08,\n        6.15924682e-06, 3.40474315e-08, 3.20646043e-09, 6.14307325e-08,\n        1.45927561e-05, 8.64352216e-08, 1.43941094e-07, 5.15253191e-07,\n        6.36787192e-08, 1.10993224e-08, 2.80374434e-06, 6.52698742e-04,\n        1.50569051e-08, 1.22862764e-06, 1.08190960e-07, 3.12823225e-08,\n        3.62867247e-09, 4.03211828e-07, 3.97046342e-08, 1.65756101e-05,\n        1.71216607e-06, 1.03340312e-06, 6.63309265e-08, 6.99213203e-08,\n        5.04177979e-06, 5.02478542e-07, 1.48223538e-04, 5.60587971e-03,\n        1.22078291e-07, 1.86379359e-03, 2.41065627e-05, 3.56008854e-06,\n        2.09815774e-08, 1.45322669e-08, 8.51550794e-08, 4.63173137e-06,\n        7.07669523e-09, 7.60982232e-07, 1.78681694e-05, 5.82860002e-06,\n        6.26186036e-09, 6.71652529e-08, 3.74762585e-08, 2.02472563e-08,\n        3.52314004e-07, 6.38601023e-07, 2.49788741e-07, 4.97702857e-08,\n        8.95248853e-09, 3.09092769e-07, 6.68525715e-08, 6.17728847e-06,\n        3.55519774e-07, 4.82555379e-06, 4.56816724e-06, 1.00632030e-06,\n        5.42093858e-06, 2.09320729e-06, 3.83092839e-07, 4.86991212e-06,\n        2.43694722e-05, 2.56681062e-07, 8.63832668e-07, 6.09998722e-07,\n        1.07966014e-06, 3.76456956e-06, 1.74969591e-06, 1.18637536e-06,\n        1.50784388e-07, 1.34870800e-06, 9.96130325e-07, 1.12748012e-06,\n        2.89275135e-07, 1.01199373e-06, 3.10480566e-08, 1.45067531e-07,\n        1.26558336e-06, 3.39497774e-06, 3.42935181e-07, 9.83362384e-07,\n        3.81136658e-07, 1.72278953e-06, 6.00352860e-06, 1.54326791e-08,\n        3.96012638e-07, 1.47352722e-07, 1.79122722e-07, 6.93016375e-07,\n        8.36267500e-05, 2.56906588e-07, 3.51594025e-07, 8.25545783e-08,\n        5.67604445e-07, 1.08268534e-06, 1.89774516e-06, 2.16050283e-03,\n        4.29603785e-01, 1.07624714e-04, 1.10562395e-08, 5.84981933e-07,\n        7.06443331e-08, 2.95814175e-07, 4.58169644e-08, 3.93046840e-08,\n        1.51907518e-08, 2.29506000e-07, 4.63015475e-08, 1.30545716e-08,\n        5.95132610e-09, 3.17670242e-07, 3.65797881e-08, 3.37072038e-06,\n        4.28877343e-08, 1.46307002e-07, 9.32429156e-09, 4.02610453e-07,\n        2.71421470e-07, 2.34199042e-08, 3.92000175e-08, 3.31181809e-07,\n        7.20081275e-08, 9.55431823e-09, 3.60028992e-07, 4.12938306e-07,\n        3.22377531e-07, 2.96478007e-08, 1.23911505e-07, 1.00578827e-05]],\n      dtype=float32)\n\n\n\n# Predictions for most likely classes. \n\nprediction_classes =vgg16.decode_predictions(preds, top=10)\nfor imagenet_id, name, likelihood in prediction_classes[0]:\n    print(\"-{}: {:2f} % likelihood\".format(name, likelihood*100))\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n40960/35363 [==================================] - 0s 0us/step\n-coffee_mug: 50.948304 % likelihood\n-cup: 42.960379 % likelihood\n-pitcher: 1.568271 % likelihood\n-saltshaker: 1.233752 % likelihood\n-coffeepot: 0.845757 % likelihood\n-water_jug: 0.560588 % likelihood\n-teapot: 0.533551 % likelihood\n-espresso_maker: 0.225793 % likelihood\n-espresso: 0.216050 % likelihood\n-whiskey_jug: 0.186379 % likelihood"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_VGG16.html#combine-into-a-function",
    "href": "projects/Past_projects/ImageRecog_VGG16.html#combine-into-a-function",
    "title": "Image Recognition & Classification with VGG16",
    "section": "Combine into a function!",
    "text": "Combine into a function!\nHere we’ll use another image to test how the model performs at prediction and combine all the previous steps into a function.\n\ndef imgClasser(locc):\n    imgName = locc\n    #Re- Load in the sample image we want to predict\n    img =image.load_img(imgName, target_size=(224,224)) #Load in the image and resize\n    \n    #Convert image to array (flatten)\n    x=image.img_to_array(img)\n\n    #The neural network is actually expecting (a list) more than 1 image so we will trick it\n    #Add a 4th dimension to the array\n\n    x= np.expand_dims(x,axis=0)\n\n    #Normalize data to 0-1 instead of 0-255\n    x=vgg16.preprocess_input(x)\n    \n    \n    #Now we run the normalized data through the network and predict\n    predictions =model.predict(x)\n\n    #We will get back a predictions object with 1000 element array. \n    #Each element reps a probability that the input matches each of the 1000 objects \n    #that the network was trained on.\n\n    #Here we use a function to tell us the names of the objects that \n    #the network predicted. We only want to top 10 so we ask for ony 10\n\n    prediction_classes =vgg16.decode_predictions(predictions, top=10)\n\n    #Print out all the predictions\n    for imagenet_id, name, likelihood in prediction_classes[0]:\n        print(\"-{}: {:2f} % likelihood\".format(name, likelihood*100))\n    #Look at loaded image\n    plt.figure()\n    plt.imshow(img) \n    plt.title('Selected Image')\n    plt.show()  # display it\n\n\nimgClasser('ant.jpeg')\n\n-ant: 83.244991 % likelihood\n-tick: 9.916326 % likelihood\n-ground_beetle: 1.015474 % likelihood\n-barn_spider: 0.821714 % likelihood\n-cockroach: 0.662146 % likelihood\n-long-horned_beetle: 0.640894 % likelihood\n-lacewing: 0.417204 % likelihood\n-scorpion: 0.339317 % likelihood\n-centipede: 0.336908 % likelihood\n-mantis: 0.327309 % likelihood\n\n\n\n\n\n\n\n\n\nThe model was able to classify the image as containing an ant with 83.2% likelihood"
  },
  {
    "objectID": "projects/Past_projects/ImageRecog_VGG16.html#archived",
    "href": "projects/Past_projects/ImageRecog_VGG16.html#archived",
    "title": "Image Recognition & Classification with VGG16",
    "section": "Archived",
    "text": "Archived\nProject Archive Note:\nThis project is archived.  Please note that library and framework versions may be outdated. Last updated:\n\nApril 2025"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\n\n Back to top"
  }
]